 See discussions, st ats, and author pr ofiles f or this public ation at : https://www .researchgate.ne t/public ation/228296045 Testing for Publication Bias in Political Science Article    in  Politic al Analysis  · Dec ember 2008 DOI: 10.1093/ oxfordjournals.p an.a004877 CITATIONS 104READS 435 3 author s, including: Some o f the author s of this public ation ar e also w orking on these r elat ed pr ojects: Studying Hat e Crime with the Int erne t: What Mak es R acists Adv ocate Racial V iolenc e? View pr oject Classr oom V oter R egistr ation Experiments  View pr oject Donald P . Green Columbia Univ ersity 198 PUBLICA TIONS    25,017  CITATIONS     SEE PROFILE David W . Nick erson Temple Univ ersity 41 PUBLICA TIONS    3,075  CITATIONS     SEE PROFILE All c ontent f ollo wing this p age was uplo aded b y Donald P . Green  on 02 June 2014. The user has r equest ed enhanc ement of the do wnlo aded file. Political Analysis ,9:4 Testing for Publication Bias in Political Science Alan S. Gerber, Donald P. Green, and David Nickerson Department of Political Science, Yale University, New Haven, CT 06520-8301 e-mail: alan.gerber@yale.edu If the publication decisions of journals are a function of the statistical signiﬁcance of re- search ﬁndings, the published literature may suffer from “publication bias.” This paperdescribes a method for detecting publication bias. We point out that to achieve statisti-cal signiﬁcance, the effect size must be larger in small samples. If publications tend tobe biased against statistically insigniﬁcant results, we should observe that the effect sizediminishes as sample sizes increase. This proposition is tested and conﬁrmed using theexperimental literature on voter mobilization. 1Introduction THEDEARTHOFinsigniﬁcantﬁndingsinjournalsreﬂectsthebehaviorofbothresearchers andjournaleditors.Editorsandrefereeslookaskanceatpapersthatreportinsigniﬁcantﬁnd-ings(Mahoney1977),andtheirreputationfordoingsocreatesthe“ﬁle-drawerproblem”—researchers elect not to submit their ﬁndings when their research fails to reject the nullhypothesis (Iyengar and Greenhouse 1988; Greenwald 1975). If articles that do not reject the null hypothesis tend to go unpublished, surveys of publishedresearchwillcreateadistortedimpressionabouteffectsize.Toachievestatisticalsigniﬁcance, studies with a small sample size require larger estimated effects than thosewith large samples. Publication bias against statistically insigniﬁcant results is therefore adirectlytestableproposition.Onecandetectthepresenceofpublicationbiasbyplottingthesize of the estimated effect by the sample size (Begg 1985, 1994). For one-tailed tests, thesmaller the sample size, the larger the published effect size (Light and Pillemer 1984). Doespublicationbiasinhabitpoliticalscience?Althoughthephenomenonhasbeenwell documented in other ﬁelds such as psychology (e.g., Coursol and Wagner 1986), medicalsciences (e.g., Simes 1986; Begg and Berlin 1988; Dickersin 1990), and economics (e.g.,DeLong and Lang 1992), the only extended discussion of publication bias in political sci-ence is by Lee Sigelman (1999, p. 206) who argues that small sample size is symptomaticof poor methodology. According to this explanation, what may appear to be bias towardstatistical signiﬁcance may instead be an innocuous process whereby methodologically Authors’ note : We are grateful for the useful comments from the three anonymous referees. We are also grateful to the Smith Richardson Foundation and the Institution for Social and Policy Studies at Yale, which helped fundthis research, but bear no responsibility for its content. Copyright 2001 by the Society for Political Methodology 385 386 Alan S. Gerber, Donald P. Green, and David Nickerson deﬁcient studies are denied publication. Sigelman’s suggestion has an observable implica- tion: published studies based on small samples should be well-executed studies, but thereshould be no tendency for studies based on small samples to show unusually large effects.Thisisanempiricalquestion,andinthispaperweofferatestofpublicationbias.Afterillus-tratinghowintheorypublicationbiasescanalterthebalanceofﬁndingsinaresearchlitera-ture,weexaminetheexperimentalworkonvoterturnout,whichcontainsstudieswithwidelyvarying sample sizes. This literature strongly suggests the presence of publication bias. 2Detecting Publication Bias Beforedelvingintotheempiricalﬁndingsonvotermobilization,letusstepbackandthink more generally about similar research situations. Consider the case where the dependentvariableisbinary(e.g.,voteornot)andthenaturalhypothesistestisaone-tailedtest(e.g.,the alternative hypothesis states that get-out-the-vote reminders increase voting rates). 1 To simplify the presentation, we describe the analysis of data produced by an experimentmeasuringtheeffectofsomepoliticalcontactontheprobabilitythatasubjectvotes.Afterperforming an experiment, the researcher performs a one-sided test of the null hypothesisthat there is no treatment effect. The hypothesis is rejected when RDt ⁄p N ¾°>Zﬁ (1) whereZﬁis the value of the ﬁpercentile level for the standard normal distribution, t⁄D PT¡PC(the treatment effect), PTis the proportion voting in the treatment group, PCis theproportionvotinginthecontrolgroup, Nisthetotalsamplesize, °equalstheproduct of the inverse of the fraction of the total sample in the treatment group times the inverseof the fraction of the total sample in the control group, and ¾is the population standard deviation. 2 Suppose that a journal review process is captured by a function: the probability that the paper is accepted equals f(R), where fis an increasing function. A special case of f(R) occurs when the publication process can be summarized by a “cutoff rule,” whereby publication occurs only if Ris greater than some cutoff level (e.g., the 5% signiﬁcance level). Use of a “cutoff rule” has two important consequences. First, the expected value of the published treatment effect is always greater than the true treatment effect. This follows from the properties of a truncated normal distribution.Suppose that some variable Xis distributed N(„,¾ 2). For any constant A, the expected value ofX, such that X>A,i s E[XjX>A]D„C‚(ﬁ)¾ (2) whereﬁD(A¡„)=¾,and,usingstandardnotationforthenormaldensityandcumulative distribution functions, ‚(ﬁ)D’(ﬁ)=(1¡8(ﬁ)), which is strictly greater than zero. Since usingacutoffruleamountstotruncatinganormalsamplingdistributionfrombelow,Eq.(2)shows that estimates will be biased upward. 1When articles are published based on the results of one-sided tests, smallereffect sizes will go unpublished; whentwo-sidedtestsareused,studieswillgounpublishedwhenthe absolutevalue oftheireffectsizesissmall. 2Equation(1)incorporatestheapproximationthatthesamplingdistributionof t⁄isnormal,withavarianceequal to¾2=N. Testing for Publication Bias in Political Science 387 Table 1 The effect of a “5% signiﬁcance level” rule on reported treatment effectsa Expected value of reported effect (%)True effect ND50 ND100 ND200 ND500 ND1000 ND5000 ND10,000 ND30,000 10% 30.8 22.5 16.7 12.1 10.4 10.0 10.0 10.0 5% 29.9 21.4 15.4 10.2 7.6 5.1 5.0 5.01% 29.3 20.7 14.7 9.4 6.7 3.1 2.2 1.4 aTheexamplesassumethatthesampleisdividedequallyintotreatmentandcontrolgroupsandthatthepopulation turnout probability for the control group is 0.5. The cell values are the expected value of the reported treatmenteffect for each sample size and true effect size. Second,theexpectedvalueofthepublishedtreatmenteffectdecreasesasthesamplesize increases.Ifthepublicationprocessisacutoffrule,thenresultsarereportedintheliteratureonly ifR>Z ﬁ, which implies that reported literature is a truncated portion of the entire samplingdistributionof t⁄.UsingEqs.(1)and(2),theexpectedvalueof t⁄jR>Zﬁequals E[t⁄jR>Zﬁ]DtC‚(x)¾°p N(3) wherexDZﬁ¡(tN1=2=¾°),andtisthetruetreatmenteffect.DifferentiatingEq.(3)with respect to the sample size ( N) yields d(E[t⁄jR>Zﬁ]) d(N)D¡N¡3=2 2[‚(x)]• (‚(x)¡x)tp N ¾°C1‚ (4) Since‚(ﬁ)(‚(ﬁ)¡ﬁ)>0 for anyﬁ,3Eq. (4) is strictly negative for any positive N. This shows that as the sample size increases, the expected value of t⁄decreases. Table1usesEq.(3)toconstructexamplesillustratinghowacutoffrulebiasespublished estimatesoftreatmenteffects.Thetablesreporttheexpectedvalueofpublishedresultsforavarietyoftreatmenteffectsandsamplesizes.Table1illustrateshowpublishedtreatmenteffectsarebiasedwhenthecutoffruleisthe5%signiﬁcancelevel(one-sidedtest).Weseethat, for small samples, the published treatment effects dramatically overestimate the truetreatment effect. Theoverestimationisespeciallypronouncedwhenthetrueeffectisverysmall.Suppose thatthetrueeffectis0.01(thetreatmentcausesa1-percentage-pointincreaseintheproba-bilitythatthesubjectvotes).Whenthesampleis50,theexpectedreportedtreatmenteffectisjustunder0.3,30timesthetrueeffect.Whenthesamplesizeis100,theexpectedpublishedtreatmenteffectis0.207,orabout20timesthetrueeffect.Largersamplesizesreduce,butdonoteliminate,thelargeupwardbiasinreportedresults.Signiﬁcantoverestimationofthetreatmenteffectispresentevenasthesamplesizeapproaches500.Inthatcasetheexpectedpublished treatment effect declines to 0.094, which is still an 800% overestimation of thetrue effect. For a sample size of 10,000, the cutoff rule of 5% signiﬁcance will cause theaveragereportedestimatetobemorethandoublethetrueparameterof0.01.Forverylargesamples, the bias shrinks and becomes relatively unimportant. The pattern is similar fortreatment effects larger than 0.01. When the sample size is small, the published literaturereportseffectsthatareseveraltimeslargerthanthetrueeffect.Asthesamplesizeincreases, 3SeeGreene(1997,pp.951–952)forastatementofthisandotherpropertiesofthetruncatednormaldistribution. 388 Alan S. Gerber, Donald P. Green, and David Nickerson Table 2 The probability that effects obtained by a replication study are smaller than those in the published literaturea Trueeffect ND50ND100ND200 ND500 ND1000 ND5000 ND10,000 ND30,000 10% .93 .89 .83 .68 .55 .50 .50 .50 5% .96 .95 .93 .88 .79 .53 .50 .501% .98 .98 .97 .97 .96 .93 .88 .76 aThe cell entries show the probability that a replication study with a sample size equal to Nand the true treatment effect listed in the ﬁrst column produces a result smaller than the average published ﬁnding undersimilar conditions using the cutoff rule examined in Table 1. the extent of bias declines, but the bias caused by using a cutoff rule for publication is relatively small only when the sample size is in the thousands. Thespecterofpublicationbiascallsintoquestionthediagnosticvalueofsmallstudies. LookingdownthecolumnsinTable1,weobservethestartlingﬁndingthat,inthepresenceof publication bias, when the sample size is small, the magnitude of the actual effects has almostnoeffectontheexpectedvalueofthepublishedeffects .Statedsomewhatdifferently, themagnitudeofpublishedresultsmaysaymoreaboutthepublicationprocessthanaboutthe causal process under investigation. As a practical matter, note that the sample sizesrequired to mitigate publication bias tend to exceed the sample sizes commonly used inpolitical science, particularly in experimental studies. 4 Theﬁnalimplicationconcernsreplicationsofpublishedstudieswhenthereispublication bias.Forthecaseanalyzedhere(whereaone-sidedhypothesistestisused),thesizeofthetreatmenteffectuncoveredinareplicationstudywillbesmallerthantheaveragepublishedeffect, and will be much lower when the typical sample size in the existing literature issmall. Table 2 provides the probability that a single replication produces a result as largeastheaverageofthepublishedliteratureundertheconditionsanalyzedinTable1.Table2suggests that, unless the true effect is large, or sample sizes are large, it is very likely thatreplication studies will ﬁnd smaller effects than those reported in the literature. 3Data Togaugethepresenceofpublicationbiasempirically,weassembledagroupofresearchpub- licationsontheeffectivenessofvotermobilizationcampaigns.Thisliteraturewasselectedbecause it is one of the few in political science to estimate parameters using randomizedexperimentaldesign.Thus,thevagariesofmodelspeciﬁcationthatcloudmeta-analysesofnonexperimentaldataanalysisarenotatissue.Anotherfeatureofthisliteraturealsorecom-mendsit.Thekindsofmanipulationsusedinthesestudiesareallfairlysimilar.Whileeachmobilization campaign contacted voters using somewhat different appeals, together theyform a set of relatively similar interventions. Although the studies span several decades,one ﬁnds telling variations in sample size among studies conducted at the same point intime (cf., Miller et al. 1981; Adams and Smith 1980). Table 3 presents the results of published voter mobilization studies. Recall the two general predictions from our model of publication bias: The published literature will tend 4ThepatternreportedinTable1generalizestomorerelaxedpublicationrules.Forexample,whenthetruetreatment effectis0.01andthesamplesizeis100,publishingonlythosestudieswhichshowpositivecoefﬁcientswillleadto an average reported estimate of 0.084, a 740% overestimation of the true effect. Table 3 Results of voter mobilization experiments N of subjects Effects on Study Date Election Place (including control group) Treatment turnout⁄ Gosnell (1927) 1924 Presidential Chicago 3,969 registered voters Mail C1% Gosnell (1927) 1925 Mayoral Chicago 3,676 registered voters Mail C9% Eldersveld (1956) 1953 Municipal Ann Arbor 41 registered voters Canvass C42% 43 registered voters Mail C26% Eldersveld (1956) 1954 Municipal Ann Arbor 276 registered voters Canvass C20% 268 registered voters Mail C4% 220 registered voters Phone C18% Miller et al. (1981) 1980 Primary Carbondale, IL 79 registered voters Canvass C21% 80 registered voters Mail C19% 81 registered voters Phone C15% Adams and Smith (1980) 1979 Special city council Washington, DC 2,650 registered voters Phone C9% Gerber and Green (2000) 1998 General New Haven, CT 29,380 registered voters Canvass C9% 29,380 registered voters Mail C1% 29,380 registered voters Phone ¡4% ⁄These are the effects reported in the tables of these research reports. They have not been adjusted for contact rates. In Eldersveld’s 1953 experiment, subjects were those who opposed or had no opinion about charter reform. In 1954, subjects were those who hadvoted in national but not local elections. Note that this table includes only studies that use random experimental design (or near-random, in the caseof Gosnell [1927]). It excludes “controlled” experiments such as Blydenburgh (1971). 389 390 Alan S. Gerber, Donald P. Green, and David Nickerson Fig. 1Relationship between sample size and effect size. to produce ﬁndings that overestimate the true effects, and published treatment effects will decrease as the sample size increases. The published literature is consistent with both ofthese predictions. Sending voters mailers prior to the election has immense effects whenthe sample size is small and relatively muted effects when the sample size is large. Forexample,whiletheclassicEldersveld(1956)studyofthe1953electionfoundmailtohavea massive, 26-percentage-point effect based on a sample size of 43, this ﬁgure dropped to4 when the sample size was expanded to 268 the following year. Using the logged samplesizes of the mail experiments as predictors of the logged effects sizes, we ﬁnd a slopeof¡0.46 (SED0.15). Similarly, the effects of phone calls and personal canvassing tend to be much larger in small studies than big ones. The effects of personal canvassing, forexample, is found to be 9 percentage-points in Gerber and Green’s (2000) study of 29,380subjects, compared to 42 percentage-points in Eldersveld’s study of 41 subjects prior tothe 1953 election. Finally, the four phone experiments suggest that effect size declines asonemovesfromthesmallstudiesbyEldersveld(1956)andMilleretal.(1981)tothosebyAdams and Smith (1980) and Gerber and Green (2000). The strong relationship between sample size and effect size is clearly shown in Fig. 1. For all modes of voter contact, studies with larger samples produce smaller estimatedeffects.Consistentwiththeexamplespresentedearlier,whensamplesaresmall,allmodesof treatment appear to work very well. While we cannot observe either the “true effect”of various modes of contact or the publication rule directly, it is nonetheless interesting tocomparethepatterninthepublishedstudiestotheexamplesshowninTable1.Supposethatthetrueeffectsofmail,canvassing,andphonecallsaresimilartothosereportedinthelargeststudyofvotercontact,thatbyGerberandGreen(2000).Forpurposesofcomparisontothecases analyzed in Table 1, let the true effect of canvassing be 10%, and let the true effectof mail and phone equal 1%. 5In the absence of publication bias, we expect no tendency for smaller studies to show larger or smaller estimated effects. However, when the sample 5Setting the true effect of phone calls to 1% is a matter of convenience. In the presence of publication bias, until sample sizes reach into the thousands, the published literature when the true effect is 1% is very similar to thepublished literature when the effect is 0%. Testing for Publication Bias in Political Science 391 size is 50 and the true treatment effect is 10%, Table 1 shows that the cutoff rule produces expectedvaluesofthepublishedliteratureof31%.Whenthetrueeffectisonly1%,Table1showsthatthepredictedeffectsare29%.Nowconsidertheactualstudies.Thetwosmalleststudiesassessbothmailandcanvassing.Theaveragesamplesizeisslightlyover50,andtheaverage treatment effects are 31% for canvassing and 23% for mailings, results consistentwith the case of a fairly strict cutoff rule. A comparison of the published effects of phonecalls reveals a similar pattern. 4Conclusion The pattern of results observed in the voter mobilization literature matches that predicted by the model of publication bias. Smaller studies indeed report larger effect sizes. Whilethe literature on voter mobilization cannot be said to be typical of publications in politicalscienceorsocialsciencemoregenerally,itprovidesinterestingevidenceoftheexistenceofpublicationbias.Althoughnonexperimentalresearchoftenreliesonlargersamples,nonex-perimentalliteraturesmaybeevenmoresusceptibletopublicationbias.Uncertaintiesaboutmodel speciﬁcation make data analysis much more complex and arguably dependent onthediscretionoftheanalyst.Also,nonexperimentaldataaremoreabundantandaccessiblethan experimental data, so it may be easier to locate or gather data that yield signiﬁcanteffect estimates. The method used in this paper could well be applied to nonexperimentalliteratures, gauging the correlation between sample size and effect size. Publicationbiasposesapotentiallyseriousproblemtothosewhoendeavortosynthesize research ﬁndings in political science. Unfortunately, for any given literature it is difﬁcultto know how many other studies were conducted but went unreported. It would be helpfulif researchers automatically published a synopsis of their ﬁndings regardless of the out-come. More realistically, professional associations within political science could create acentralregistryofabstractsforproposedstudies,akintowhatexistsinthemedicalsciences(Chalmersetal.1986;Simes1986;BeggandBerlin1988).Meta-analystscouldthencom-parethepublishedwiththeunpublishedliteraturetogetamoreaccuratesenseofwhatwasleft on the cutting-room ﬂoor. If the discipline is to take seriously its large and growingresearchoutput,itmustfosterinstitutionssuchasregistriesthatallowformeaningfulsynthe-sesofexistingﬁndings.Untilthen,politicalscientistsareprobablywelladvisedtodiscountdeeplythediagnosticweightofpublishedstudiesbasedonsmallnumbersofobservations. References Adams,WilliamC.,andDennisJ.Smith.1980.“EffectsofTelephoneCanvassingonTurnoutandPreferences:A Field Experiment.” Public Opinion Quarterly 44 (Autumn):389–395. Begg, C. B. 1985. “A Measure to Aid in the Interpretation of Published Clinical Trials.” Statistics in Medicine 4:1–9. Begg,C.B.1994.“PublicationBias.”In TheHandbookofResearchSynthesis ,eds.H.CooperandL.V.Hedges. New York: Russell Sage Foundation. Begg, C. B., and J. A. Berlin. 1988. “Publication Bias: A Problem in Interpreting Medical Data.” Journal of the Royal Statistical Society Series B 151:419–463. Berlin,J.A.,C.B.Begg,andT.A.Louis.1989.“AnAssessmentofPublicationBiasUsingaSampleofPublished Clinical Trials.” Journal of the American Statistical Association 84:381–392. Chalmers, I., J. Hetherington, M. Newdick, L. Mutch, A. Grant, E. Enkin, and K. Dickersin. 1986. “The Oxford Database of Perinatal Trials: Developing a Register of Published Reports of Controlled Trials.” Controlled Clinical Trials 7:306–324. Coursol,A.,andE.Wagner.1986.“EffectofPositiveFindingsonSubmissionandAcceptanceRates:ANoteon Meta-Analysis Bias.” Professional Psychology 17:136–137. 392 Alan S. Gerber, Donald P. Green, and David Nickerson DeLong,J.B.,andK.Lang.1992.“AreAllEconomicHypothesesFalse?” JournalofPoliticalEconomy 100:1257– 1272. Dickersin, K. 1990. “The Existence of Publication Bias and Risk Factors for Its Occurrence.” Journal of the American Medical Association 263:1385–1389. Eldersveld, Samuel J. 1956. “Experimental Propaganda Techniques and Voting Behavior.” American Political Science Review 50 (Mar.):154–165. Gerber,AlanS.,andDonaldP.Green.2000.“TheEffectsofCanvassing,DirectMail,andTelephoneContacton Voter Turnout: A Field Experiment.” American Political Science Review 94 (Sept.):653–663. Greene, William H. 1997. Econometric Analysis , 3rd ed. Upper Saddle River, NJ: Simon and Schuster. Greenwald, A. G. 1975. “Consequences of Prejudice Against the Null Hypothesis.” Psychological Bulletin 82:1–12. Iyengar, S., and J. B. Greenhouse. 1988. “Selection Models and the File Drawer Problem.” Statistical Science 3:9–135. Light,R.J.,andD.B.Pillemer.1984. SummingUp:TheScienceofReviewingResearch .Cambridge,MA:Harvard University Press. Mahoney, M. J. 1977. “Publication Prejudices: An Experimental Study of Conﬁrmatory Bias in the Peer Review System.” Cognitive Therapy Research 1:161–175. Miller, Roy E., David A. Bositis, and Denise L. Baer. 1981. “Stimulating Voter Turnout in a Primary: Field Experiment with a Precinct Committeeman.” International Political Science Review 2(4):445–460. Sigelman, Lee. 1999. “Publication Bias Reconsidered.” Political Analysis 8:201–210. Simes,R.J.1986.“PublicationBias:TheCaseforanInternationalRegistryofClinicalTrials.” JournalofClinical Oncology 4:1529–1541. View publication stats