 The Thirty-Fourth AAAI Conference on Artiï¬cial Intelligence (AAAI-20) Reasoning about Political Bias in Content Moderation Shan Jiang, Ronald E. Robertson, Christo Wilson Northeastern University, USA {sjiang, rer, cbw }@ccs.neu.edu Abstract Content moderation, the AI-human hybrid process of remov- ing (toxic) content from social media to promote community health, has attracted increasing attention from lawmakers due to allegations of political bias. Hitherto, this allegation has been made based on anecdotes rather than logical reasoning and empirical evidence, which motivates us to audit its validity. In this paper, we ï¬rst introduce two formal criteria to measure bias (i.e., independence and separation) and their contextual meanings in content moderation, and then use YouTube as a lens to investigate if the political leaning of a video plays a role in the moderation decision for its associated comments. Our results show that when justiï¬able target variables (e.g., hate speech and extremeness) are controlled with propensity scoring, the likelihood of comment moderation is equal across left- and right-leaning videos. Bad Content Moderation, Bad! Social media has long played host to problematic content such as partisan propaganda (Allcott and Gentzkow 2017),misinformation (Jiang and Wilson 2018), and violent hate speech (Olteanu et al .2018). In an attempt to police this con- tent and improve the health of their user community, social media platforms publish sets of community guidelines that explain the types of content they prohibit, and remove or hide this content from their platforms. This practice is commonly referred to as content moderation . Content moderation is typically implemented as an AI- human hybrid process. To scale with the large amount of toxic content generated online, an AI ï¬ltering layer ï¬rst ï¬nds potential candidates for moderation (Gibbs 2017; Sloane2018), and then sends them to human reviewers for a ï¬nal determination (Levin 2017; Gershgorn and Murphy 2017). This content moderation process, however, has been criti- cized for potential bias: biased AI systems have been docu- mented (Barocas, Hardt, and Narayanan 2019; Hutchinsonand Mitchell 2019), and human moderators can bring theirown biases into the moderation process (Diakopoulos and Naaman 2011). As a result, content moderation faces a back- lash from ideological conservatives, who allege that socialmedia platforms are biased against them and are censoring their views (Kamisar 2018; Usher 2018), e.g., Figure 1. These Copyright c/circlecopyrt2020, Association for the Advancement of Artiï¬cial Intelligence (www.aaai.org). All rights reserved. ? Figure 1: Allegations of political bias in content moderation based on anecdotes, not hard evidence. allegations have even spurred lawmakers to action, e.g., in June 2019, the â€œEnding Support for Internet Censorship Actâ€ was introduced into the US Senate to limit immunity granted by Section 230 of the Communications Decency Act to â€œen- courage providers of interactive computer services to provide content moderation that is politically neutralâ€ (Hawley 2019). These allegations of political bias, however, are based on anecdotes, and there is little support from logical reasoning and empirical evidence (Jiang, Robertson, and Wilson 2019; Shen et al .2018; Shen and Rose 2019). In this paper, we conduct an audit on the validity of these allegations driven by a high-level research question: â€¢Research question: is content moderation biased? To approach this question, we ï¬rst introduce two formal criteria to measure bias and how they apply in the context of content moderation, and formulate two null hypotheses Hind 0 (for independence) and Hsep 0(for separation) under these criteria. Then, we use YouTube comment moderation as a case study to investigate a more concrete question: â€¢Case study: does the political leaning of a video play a role in the moderation decision for its comments? Our results show: Hind 0is rejected, i.e., comments are more likely to be moderated under right-leaning videos; Hsep 0holds, i.e., with propensity scored justiï¬able target variables (e.g., hate speech and extremeness), there is no signiï¬cant differ- ence in moderation likelihood across the political spectrum. 13669 How to Measure Bias, Really? Recent advances in fairness research provide many criteria to measure bias, each aiming to formalize different desider- ata (Barocas, Hardt, and Narayanan 2019). Most of these criteria characterize the joint or conditional probability be- tween involved variables (e.g., decision, sensitive features), and can be approximately classiï¬ed to two categories: inde- pendence and separation (Hutchinson and Mitchell 2019). Independence Independence, also referred to as demographic parity ,i s a fairness criterion that requires the decision variable andthe sensitive feature to be statistically independent. In the context of political bias and content moderation, an item on social media (e.g., post, comment) can be associated with its political leaning P={left,right}and moderation decision M={moderated ,alive}. This criterion requires these two variables to satisfy MâŠ¥âŠ¥P, which, given that Pis a binary variable, is equivalent to: P{M|P=left}=P{M|P=right}. (1) The graphic model of independence criterion is shown in Figure 2a. To allege political bias under this criterion, then, requires empirical evidence to reject (1) as the null hypothesis Hind 0with statistical conï¬dence. Although this criterion is intuitive and has been applied in many studies (Robertson et al .2018; Hu et al .2019), its desirability is context-dependent: e.g., moderation decisions are intended to be made based on the toxicity of content, and if toxicity is unevenly distributed across the political spectrum, the pursuit for independence may be unachievable and even undesirable. Separation Separation, also referred to as equalized odds , is a type of conditional independence that allows dependence between the decision variable and the sensitive feature, but only to the extend that can be justiï¬ed by target variables. For contentmoderation, such target variables can include hate speech, extreme videos, etc. Denoting a universe of justiï¬able target variables as J, this criterion requires MâŠ¥âŠ¥P|J, which, given that P is a binary variable, is equivalent to âˆ€J: P{M|P=left,J}=P{M|P=right,J}. (2) This criterion is also widely adopted in previous studies, especially when the correlation between sensitive featuresand target variables is inherent (Thoemmes and Kim 2011; Lanza, Moore, and Butera 2013; Austin 2008). A practical limitation of this criterion is that stable estima- tors of (2) requires matched observational pairs conditional onJ. Therefore, as Jcontains more variables, matching be- comes more difï¬cult. An alternative method is to summarize all of the target variables into one scalar, i.e., f:R|J|â†’R. A particular example of fispropensity scoring deï¬ned as: ps(J): =P{P=left (or right) |J}(Rosenbaum and Rubin 1983). It is proven that if (2) holds and P{P|J}âˆˆ(0,1), thenâˆ€ps(J),P{P|ps(J)}âˆˆ(0,1)and: P{M|P=left,ps(J)}=P{M|P=right,ps(J)}.(3)0RGHUDWLRQ GHFLVLRQ3ROLWLFDO OHDQLQJ (a) Independence. 1st null hypothesis Hind 0:MâŠ¥âŠ¥P.0RGHUDWLRQ GHFLVLRQ3ROLWLFDO OHDQLQJ-XVWLÃ€DEOH WDUJHW YDULDEOHV  KDWHVSHHFK H[WUHPHYLGHR HWF SV -  (b) Separation. Propensity scoring function ps(J)is used to summarize Jto a scala, hence 2nd null hypothesis Hsep 0:MâŠ¥âŠ¥P|ps(J). Figure 2: Graph models of fairness criteria. These criteria characterize the joint or conditional distribution of political leaning P(sensitive feature), moderation decision M(deci- sion variable) and justiï¬able target variables J. The graphic model of propensity scored separation criterion is shown in Figure 2b. To allege political bias under this criterion, then, requires empirical evidence to reject (3) as the null hypothesis Hsep 0with statistical conï¬dence. Is Y ouTube Biased, for Example? Y ouTube is one of the major social media platforms that faces allegations of politically biased content moderation, and it practices moderation at different content levels, e.g., videos, channels, users, and comments (Y ouTube 2018a). Here, we use YouTube as a lens to investigate if the political leaning of a video plays a role in the moderation decision for its associated comments.1 Data The Y ouTube data we use contain 84,068 comments posted on 258 political videos,2labeled with involved variables. The moderation decision for each comment is labeled by comparing two snapshots of the dataset: the ï¬rst collected in January 2018 (Jiang and Wilson 2018), and the second in June 2018 (Jiang, Robertson, and Wilson 2019). Disappeared comments within this time range are labeled as moderated , and the others are labeled as alive . The political leaning of the video under which the com- ment was posted is labeled from another dataset (Robertson et al .2018), where all political entities on the web are as- signed an ideological score iâˆˆ[âˆ’1,1](left to right). We link a videoâ€™s publisher to its political entity, and use the sign of the ideological score as its political leaning leftorright . Linguistic signals in comments are used as the ï¬rst set of target variables, as the text content of comments is theprimary focus of the moderation system (YouTube 2018c). We use an existing lexicon Comlex to map the text content to 8 binary variables: swear (including hate speech, e.g., the n-word), laugh (e.g., â€œhahaâ€), emoji ,fake (e.g., â€œlieâ€), ad- 1These results, although under a different frame, are also re- ported in (Jiang, Robertson, and Wilson 2019). 2Available at: https://moderation.shanjiang.me 13670 Ê³Ëš^0 PRGHUDWHGà²­3 OHIW`Ê³Ëš^0 PRGHUDWHGà²­3 ULJKW` ss (a) Hind 0is rejected. There is signiï¬cant difference between com- ment moderation probability under left- and right- leaning videos. Ê³Ëš^0 PRGHUDWHGà²­3 OHIW SV - `Ê³Ëš^0 PRGHUDWHGà²­3 ULJKW SV - ` ss (b)Hsep 0holds. There is no signiï¬cant difference between comment moderation probability under left- and right- leaning videos with propensity scored justiï¬able variables. Figure 3: Estimated moderation probability. The (conditional) moderation probability for comments with corresponding conï¬dence intervals are estimated to test the independence and separation hypotheses. ministration (e.g., â€œmayorâ€), American (e.g., â€œnycâ€, â€œtexasâ€), nation (e.g., â€œmexicoâ€), and personal (e.g., â€œyourâ€). The social engagement of a video is also considered as target variables, e.g., a video with a high dislike rate attracts more ï¬‚aggers and more attention from moderators. This in- cludes three variables: views ,likes , and dislikes of the video. We also consider the extremeness of a video, as extreme videos are more likely to call for violence or spread conspir- acy theories (Y ouTube 2018b). This is labeled using the same dataset as the political leaning variable. We label a video extreme if|i|>0.5and center otherwise. Misinformation related features are another set of tar- get variables we control. Misinformation might play a rolein content moderation as social media companies have re-cently established collaborations with fact-checkers (e.g., Snopes, PolitiFact) (Glaser 2018). This contains two binary variables: if a video contains misinformation or not (as judged by Snopes or PolitiFact), and if a comment is posted before or after the corresponding fact-check. Results Independence Hind 0measures only moderation decision and political leaning variables. We estimate the empirical proba- bility of the two variable and conï¬dence intervals for bi-nomial proportions. As shown in Figure 3a, there is sig- niï¬cant difference between Ë†P{M=moderated |P=left,J} andË†P{M=moderated |P=right,J}, therefore the inde- pendence hypothesis Hind 0 is rejected. However, as we discuss above, the independence criterion has a fatal limitation in this context because there are strong correlations between political leaning and justiï¬able target variables, e.g., comments under right-leaning videos contain signiï¬cantly more swear words (Pearson Ï‡2= 671.2âˆ—âˆ—âˆ—),3 indicating increased likelihood of hateful content, which is the primary trigger for content moderation. Right-leaning videos also have signiï¬cantly more dislikes (Mann-Whitney 3âˆ—p< 0.05;âˆ—âˆ—p< 0.01;âˆ—âˆ—âˆ—p< 0.001. 6HOIPRGHUDWLRQUDWH+VHS  (a) Self-moderation. The effect of self-moderation is minimal.//55 %LDVRIIDFWFKHFNHUV+VHS  (b) Biased fact-checkers. Slight bias does not change results.  7KUHVKROGIRUH[WUHPHFHQWHU+VHS  (c) Thresholding extremeness. V arying thresholds for extreme- ness has mostly minimal effect. 7KUHVKROGIRUULJKWOHIW+VHS  (d) Thresholding political lean- ing. Results ï¬‚uctuate on both the left and right ends. Figure 4: Robustness of Hsep 0. Potential scenarios are simu- lated to check the robustness of our results. The results are mostly stable except a few cases where the results ï¬‚uctuate on both left and right. U=4.08Ã—108âˆ—âˆ—âˆ—) than left-leaning ones, providing an alternative explanation that the higher dislike rate may result in more ï¬‚agged comments, thus increased moderation. Therefore, our main focus is on investigating if the dif- ference in moderation probability can be justiï¬ed by tar-get variables, i.e., Hsep 0. We estimate propensity scores ps(J)by logistic regressions, use them to match two- nearest neighbors from our observations, and then com-pute the conditional probability given matched propensityscores. As shown in Figure 3b, there is no signiï¬cant dif- ference between Ë†P{M=moderated |P=left,ps(J)}and Ë†P{M=moderated |P=right,ps(J)}, therefore no evi- dence to reject the second hypothesis, i.e., Hsep 0 holds. Overall, our results show that although comments are more likely to be moderated under right-leaning videos, this differ- ence is well-justiï¬ed, i.e., once our measured target variables are balanced, there is no signiï¬cant difference in moderation likelihood across the political spectrum. Robustness Conclusions made from observational data are often ques- tionable due to alternative explanations. Therefore, we con- duct additional experiments to check the robustness of Hsep0, including self-moderation instead of moderation by the plat- form (Figure 4a), bias in the ratings provided by fact-checkers (Figure 4b), varying the threshold to label extremeness (Fig- ure 4c), and labeling political leaning only when |i|exceed- ing a certain threshold (Figure 4d). Due to space limits, we omit details on the implementation and discussion of these experiments. Interested readers can refer to our original pa- per (Jiang, Robertson, and Wilson 2019). In short, these experiments show that Hsep 0in general holds when the simulated scenario is moderate, but can be rejected under extreme cases. However, under these scenarios, the results ï¬‚uctuate on both the left and right ends, i.e., bias against both left and right political leanings. Therefore, the allegation of bias in YouTube comment moderation is still not supported. 13671 Itâ€™s Complicated By using Y ouTube content moderation as a lens, our results show that the allegation of biased content moderation is sup- ported by the intuitive (yet out-of-context) independence cri- terion, however, it is not supported by the separation criterion, where we justify moderation decisions with other target vari- ables. Interestingly, research on alleged political bias often reaches similar conclusions: (Bakshy, Messing, and Adamic 2015) show that the allegation of biased newsfeed on Face- book was due more to homophily than algorithmic curation; (Robertson et al .2018) show that the allegation of biased search results from Google was dependent largely on the input query instead of the self-reported ideology of the user. The goal of this research is twofold. First, we call for transparency in content moderation practices. The opaque nature of this process can breed conspiracy theories such as the one we investigated in this paper. Further, these allega- tions are challenging to validate, as neither researchers, nor critics, can access removed data that underpin moderation decisions. Therefore, we recommend that moderated content be preserved and protected. Second, our work ï¬ts in a broader scope of under- standing fairness, discrimination, neutrality, and bias inalgorithm-mediated systems (Baeza-Yates 2016; Sandviget al .2014). We introduce recently proposed theories of fairness measures (Barocas, Hardt, and Narayanan 2019;Hutchinson and Mitchell 2019) to an existing body of em-pirical work on auditing political bias on the web (Jiang,Robertson, and Wilson 2019; Robertson et al .2019; Hu et al.2019; Ali et al .2019; Jiang, Martin, and Wilson 2019; Robertson et al .2018). We hope this work can foster a health- ier, contextual, and dialectical discussion of political bias and social media at large. Acknowledgments This research was supported in part by NSF grant IIS- 1553088. Any opinions, ï¬ndings, and conclusions or rec- ommendations expressed in this material are those of the authors and do not necessarily reï¬‚ect the views of the NSF. References Ali, M.; Sapiezynski, P .; Bogen, M.; Korolova, A.; Mislove, A.; and Rieke, A. 2019. Discrimination through optimization: How facebookâ€™s ad delivery can lead to biased outcomes. PACM on HCI 4(CSCW). Allcott, H., and Gentzkow, M. 2017. Social media and fake news in the 2016 election. Journal of Economic Perspectives 31(2). Austin, P . C. 2008. A critical appraisal of propensity-score matching in the medical literature between 1996 and 2003. Statistics in Medicine 27(12). Baeza-Yates, R. 2016. Data and algorithmic bias in the web. In Proc. of WebSci . Bakshy, E.; Messing, S.; and Adamic, L. A. 2015. Exposure to ideologically diverse news and opinion on facebook. Science 348. Barocas, S.; Hardt, M.; and Narayanan, A. 2019. Fairness and Machine Learning . fairmlbook.org. http://www.fairmlbook.org. Diakopoulos, N., and Naaman, M. 2011. Towards quality discourse in online news comments. In Proc. of CSCW .Gershgorn, D., and Murphy, M. 2017. Facebook is hiring more people to moderate content than twitter has at its entire company. Quartz. Gibbs, S. 2017. Google says ai better than humans at scrubbing extremist youtube content. The Guardian. Glaser, A. 2018. Y outube is adding fact-check links for videos on topics that inspire conspiracy theories. Slate. Hawley, J. 2019. Ending support for internet censorship act. Hu, D.; Jiang, S.; Robertson, R. E.; and Wilson, C. 2019. Auditing the partisanship of google search snippets. In Proc. of WWW . Hutchinson, B., and Mitchell, M. 2019. 50 years of test (un)fairness: Lessons for machine learning. In Proc. of F AT* . Jiang, S., and Wilson, C. 2018. Linguistic signals under misinfor- mation and fact-checking: Evidence from user comments on social media. PACM on HCI 2(CSCW). Jiang, S.; Martin, J.; and Wilson, C. 2019. Whoâ€™s the guinea pig?: Investigating online a/b/n tests in-the-wild. In Proc. of F AT* . Jiang, S.; Robertson, R. E.; and Wilson, C. 2019. Bias misperceived: The role of partisanship and misinformation in youtube comment moderation. In Proc. of ICWSM . Kamisar, B. 2018. Conservatives cry foul over controversial groupâ€™s role in youtube moderation. The Hill. Lanza, S. T.; Moore, J. E.; and Butera, N. M. 2013. Drawing causal inferences using propensity scores: A practical guide for community psychologists. American journal of community psychology 52(3-4). Levin, S. 2017. Google to hire thousands of moderators after outcry over youtube abuse videos. The Guardian. Olteanu, A.; Castillo, C.; Boy, J.; and V arshney, K. R. 2018. The effect of extremist violence on hateful speech online. In Proc. of ICWSM . Robertson, R. E.; Jiang, S.; Joseph, K.; Friedland, L.; Lazer, D.; and Wilson, C. 2018. Auditing partisan audience bias within google search. PACM on HCI 2(CSCW). Robertson, R. E.; Jiang, S.; Lazer, D.; and Wilson, C. 2019. Au- diting autocomplete: Suggestion networks and recursive algorithm interrogation. In Proc. of WebSci . Rosenbaum, P . R., and Rubin, D. B. 1983. The central role of the propensity score in observational studies for causal effects. Biometrika 70(1). Sandvig, C.; Hamilton, K.; Karahalios, K.; and Langbort, C. 2014. Auditing algorithms: Research methods for detecting discrimination on internet platforms. Data and Discrimination . Shen, Q., and Rose, C. 2019. The discourse of online content moderation: Investigating polarized user responses to changes in reddit?s quarantine policy. In Proc. of ALW3 ACL . Shen, Q.; Yoder, M.; Jo, Y .; and Rose, C. 2018. Perceptions of censorship and moderation bias in political debate forums. In Proc. of ICWSM . Sloane, G. 2018. Facebook pursues ai in bid to id harmful content. AdAge. Thoemmes, F. J., and Kim, E. S. 2011. A systematic review of propensity score methods in the social sciences. Multivariate be- havioral research 46(1). Usher, N. 2018. How republicans trick facebook and twitter with claims of bias. The Washington Post. Y ouTube. 2018a. Community guidelines. Y ouTube. 2018b. Harassment and cyberbullying policy. Y ouTube. 2018c. Hate speech policy. 13672