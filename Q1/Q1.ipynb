{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3238a00-3e9e-420e-a189-2e6eac36804f",
   "metadata": {},
   "source": [
    "Import libraries and get the stop words list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9fe1ef85-196b-47ec-841c-9dfac8c33903",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\stast\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\stast\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348c74fb-e141-4c34-ad7e-86658c6a38e9",
   "metadata": {},
   "source": [
    "Define functions that we will use for all the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dcc8b292-8e03-4acb-b2e2-2c9f09284928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all separators using the separators.txt file\n",
    "def preprocess(text):\n",
    "  text = text.replace('-', '')\n",
    "  with open('separators.txt', 'r', encoding='utf8') as f:\n",
    "    separators = f.read().splitlines()\n",
    "  for s in separators:\n",
    "    text = text.replace(s, ' ')\n",
    "  return text\n",
    "\n",
    "# create dataframe where columns are docs and rows are words and cells are term frequency in the corresponding document\n",
    "def create_term_frequency_df(doc_dict):\n",
    "    term_frequencies = {}\n",
    "    for filename in doc_dict:\n",
    "        word_count = {}\n",
    "        for word in doc_dict[filename]:\n",
    "            if word in word_count:\n",
    "                word_count[word] += 1\n",
    "            else:\n",
    "                word_count[word] = 1\n",
    "        term_frequencies[filename] = word_count\n",
    "    df = pd.DataFrame(term_frequencies).fillna(0).astype(int)\n",
    "    return df\n",
    "\n",
    "# create a language model from the dataframe. We use Laplace smoothing\n",
    "def language_model(data):\n",
    "    lengths = data.sum(axis=0)\n",
    "    for col in data.columns:\n",
    "        data[col] += 1\n",
    "        data[col] /= lengths[col] + data.shape[0]\n",
    "    return data\n",
    "\n",
    "# for each column we save 20 most popular words(with the hifhest probability) and save them to specified file\n",
    "def model_to_txt(model, name):\n",
    "    words_dict = {}\n",
    "    for col in model.columns:\n",
    "        file_col = model[col]\n",
    "        file_col = file_col.sort_values(ascending=False)\n",
    "        file_col = file_col.head(20)\n",
    "\n",
    "        popular_words = []\n",
    "        for word in file_col.index:\n",
    "            popular_words.append(word)\n",
    "        words_dict[col] = popular_words\n",
    "\n",
    "    with open(f'{name}.txt', 'w', encoding='utf8') as f:\n",
    "        for key, value in words_dict.items():\n",
    "            f.write(f'{key}:{value}\\n')\n",
    "\n",
    "# functions that gets all the documents as dictionary and creates a text file with the specified name that contains 20 most popular word for \n",
    "# each document in the collection\n",
    "def lm_result(docs, filename):\n",
    "    term_freq = create_term_frequency_df(docs)\n",
    "    lm = language_model(term_freq)\n",
    "    model_to_txt(lm, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e5ddd5-2711-44dc-8b9f-27f4870914e6",
   "metadata": {},
   "source": [
    "Preprocess the documents and analize the language model result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "18783975-14f2-4a11-94f0-74ac9b0b5231",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = {}\n",
    "for filename in os.listdir('text'):\n",
    "  file_path = os.path.join('text', filename)\n",
    "  with open(file_path, encoding='utf8') as file:\n",
    "    text = file.read()\n",
    "    text = preprocess(text)\n",
    "    text = word_tokenize(text)\n",
    "    documents[filename] = text\n",
    "lm_result(documents, 'collection_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791b9dbd-7ec1-4808-94f6-b34ac2ae9c03",
   "metadata": {},
   "source": [
    "Now remove stop words and make the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3b0812ff-3035-4fea-967c-bf043b4e07bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_stop_words_docs = documents\n",
    "for filename in no_stop_words_docs:\n",
    "    no_stop_words_docs[filename] = [word for word in no_stop_words_docs[filename] if word not in stop_words]\n",
    "lm_result(no_stop_words_docs, 'no_stop_words_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf041b7-37ce-4813-acaa-7384491681bc",
   "metadata": {},
   "source": [
    "Perform Case Folding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "77d387c3-9064-461d-8b7e-d464188391d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "case_fold_docs = no_stop_words_docs\n",
    "for filename in case_fold_docs:\n",
    "    case_fold_docs[filename] = [word.casefold() for word in case_fold_docs[filename] if word.casefold() not in stop_words]\n",
    "lm_result(case_fold_docs, 'case_folding_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80634b01-8dc4-4dc5-abbd-e10e0c1bfac4",
   "metadata": {},
   "source": [
    "Use Porter Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8f8c7665-ea67-4601-974b-e225472c0a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "stemmed_docs = case_fold_docs\n",
    "for filename in stemmed_docs:\n",
    "    stemmed_docs[filename] = [ps.stem(word) for word in stemmed_docs[filename]]\n",
    "lm_result(stemmed_docs, 'stemmed_model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
