Counterfactually Measuring and Eliminating Social Bias in
Vision-Language Pre-training Models

Yi Zhang”

School of Computer and Information
Technology & Beijing Key Lab of
Traffic Data Analysis and Mining,
Beijing Jiaotong University, China

Junyang Wang
School of Computer and Information
Technology & Beijing Key Lab of
Traffic Data Analysis and Mining,
Beijing Jiaotong University, China

Jitao Sang?

School of Computer and Information
Technology & Beijing Key Lab of
Traffic Data Analysis and Mining,
Beijing Jiaotong University, China

yi.zhang@bjtu.edu.cn 21120406 @bjtu.edu.cn *Peng Cheng Lab, Shenzhen 518066,
China
jtsang@bjtu.edu.cn
ABSTRACT 1 INTRODUCTION

Vision-Language Pre-training (VLP) models have achieved state-of-
the-art performance in numerous cross-modal tasks. Since they are
optimized to capture the statistical properties of intra- and inter-
modality, there remains risk to learn social biases presented in the
data as well. In this work, we (1) introduce a counterfactual-based
bias measurement CounterBias to quantify the social bias in VLP
models by comparing the [MASK]ed prediction probabilities of
factual and counterfactual samples; (2) construct a novel VL-Bias
dataset including 24K image-text pairs for measuring gender bias in
VLP models, from which we observed that significant gender bias is
prevalent in VLP models; and (3) propose a VLP debiasing method
FairVLP to minimize the difference in the [MASK]ed prediction
probabilities between factual and counterfactual image-text pairs
for VLP debiasing. Although CounterBias and FairVLP focus on
social bias, they are generalizable to serve as tools and provide new
insights to probe and regularize more knowledge in VLP models.

CCS CONCEPTS

+ Social and professional topics — Computing / technology
policy; « Applied computing — Law, social and behavioral
sciences; « Computing methodologies — Machine learning.

KEYWORDS
Fairness in Machine Learning, Responsible Artificial Intelligence

ACM Reference Format:

Yi Zhang, Junyang Wang, and Jitao Sang. 2022. Counterfactually Measuring
and Eliminating Social Bias in Vision-Language Pre-training Models. In
Proceedings of the 30th ACM International Conference on Multimedia (MM
°22), October 10-14, 2022, Lisboa, Portugal. ACM, New York, NY, USA, 9 pages.
https://doi.org/10.1145/3503161.3548396

*This work was done when the author interned at Peng Cheng Lab.
tCo rresponding authors

 

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.

MM 22, October 10-14, 2022, Lisboa, Portugal

© 2022 Association for Computing Machinery.

ACM ISBN 978-1-4503-9203-7/22/10...$15.00
https://doi.org/10.1145/3503161.3548396

4996

factual bias concept: counterfactual bias concept:

  

adversarial
image

original
image

“the woman is [MASK]” “the man is [MASK] ”

~~ nea

VLP (Masked Language Modeling}

oa Go
P([MASK]="shopping") P([MASK]="shopping")

=0.96 =0.05

Figure 1: We propose to measure social bias in VLP model by
comparing the [MASK]ed prediction probabilities of factual
and counterfactual samples.

The past few years have witnessed the rapid development of
Vision-Language Pre-training (VLP) models [2, 4, 17, 38], and task-
specific finetune on VLP models has become a new and state-of-
the-art paradigm in many multimedia tasks [20, 21, 32]. Beyond
accuracy, fairness which concerns about the discrimination towards
socially protected or sensitive groups plays a critical role in trust-
worthy deployment of VLP models in downstream tasks. This is
highly endorsed by many VLP model designers [16]. Yet, very lim-
ited studies have begun to examine the fairness problem in VLP
models, which typically apply the bias measurement designed for
the unimodal pre-training (e.g., text-based language models [1]).
The characteristics of VLP models regarding the interaction be-
tween visual modality and textual modality are largely ignored.
This work is thus devoted to measuring and eliminating the social
bias in VLP models by emphasizing/addressing its multimodal char-
acteristics, with the goal of helping understand the existed social
bias and inspiring the development of fair VLP models.

Measuring the social bias in pre-training model essentially in-
volves the correlation between bias and target concepts. The cross-
modal interaction of VLP models poses two main challenges for bias
measurement. The first challenge is on modeling the bias and target
concepts. In VLP models, the joint modeling of visual modality
delivers different embeddings w.r.t. image contexts, making direct
MM °22, October 10-14, 2022, Lisboa, Portugal

calculating the distance in the embedding space as in [26] not
applicable. Inspired by the Masked Language Modeling (MLM) pre-
training task commonly used in VLP models, we employ prompt-
based query to probe the modeling of bias and target concepts in
VLP models. As illustrated in Figure 1, the concepts to be modeled
are masked before issuing into VLP models. The resultant MLM
prediction probability of [MASK]ed token P([MASK]="shopping”)
manifests VLP model’s modeling of the concept given the input
image-text pair.

The second challenge is on quantifying the bias-target associ-
ation. With MLM prediction probability for concept probing, a
natural idea is to compare the prediction probabilities of the target
concept in the observed image-text pairs of different bias concepts
(e.g., the difference of P([MASK]=“shopping”) between male and
female inputs. However, relying on the observed data suffers from
two problems: (1) There exists no adequate number of image pairs
with only different bias concepts. For example, when measuring the
social bias of “shopping” over gender, it is difficult to collect pairs of
identical shopping images except for different genders. Other infor-
mation beyond gender like the different background concepts can
largely affect the MLM prediction probability, as detailed discussed
in Section 3 for the example in Figure 2. (2) The bias measurement
result is highly sensitive to the observed dataset. Examining image-
text datasets with different distributions will lead to inconsistent
observations. To address this, we propose a counterfactual-based
bias measure method (CounterBias) to quantify the social bias in
VLP models exempted from the limitation in observed data pairs.
Specifically, we design a model-specific adversarial attack scheme
to generate counterfactual images by altering the bias concept from
factual images (e.g., attacking woman to man as illustrated in Fig-
ure 1). This successfully alters the bias concept without changing
other information. The resultant MLM prediction difference be-
tween factual and counterfactual image-text pairs thus manifests
the social bias of the examined VLP models.

Since there exists no off-the-shelf dataset specially designed
to analyze the social bias in VLP models, we propose VL-Bias, a
dataset to facilitate the study on understanding model bias in VLP
models. VL-Bias contains 52 activities and 13 occupations with
a total of 24k image-text pairs. Using CounterBias, we examined
two typical VLP architectures, dual-stream and single-stream, on
VL-Bias dataset. Several key observations include: (1) Social bias
is prevalent in the examined three VLP models, and exists in both
visual and language modalities; (2) The gender bias contained in
VLP models is basically consistent with human gender stereotypes;
(3) The examined single-stream ViLT and dual-stream ALBEF/TCL
show different cross-modal conformities regarding the observed
unimodal social bias; (4) The examined VLP models contain stronger
gender bias than unimodal pre-training model BERT.

Inheriting the idea of measuring social bias, we propose a simple
counterfactual-based method to eliminate the social bias in VLP
models. Specifically, we first generate counterfactual samples of bias
concepts in visual and language modalities, respectively. Then, the
difference in the predicted [MASK]ed probability between factual
and counterfactual image-text pairs is minimized, so that preventing
the model from learning the association between bias and target
concepts. Experimental results demonstrate the effectiveness in
benchmark datasets.

4997

Yi Zhang, Junyang Wang, & Jitao Sang

   

“the man is [MASK] ”

Figure 2: Example “shopping” images with their predicted
factual target probability from ALBEF. The top and second
row correspond to images with female and male bias concepts
respectively.

Our contributions can be summarized as follows:

e We propose to employ model-specific adversarial attacks
on bias concepts to generate counterfactual samples, and
counterfactually measure social bias at instance-level by
comparing the [MASK]ed prediction probabilities of factual
and counterfactual samples.

We introduce VL-Bias, a dataset for studying gender bias in
VLP models. Observations are derived regarding typical VLP
architectures, the consistency with human stereotypes, and
the comparison with unimodal pre-training models.

We propose a counterfactual bias elimination method, which
eliminates bias by minimizing [MASK]ed probability discrep-
ancy between factual and counterfactual samples.

2 BACKGROUND AND RELATED WORKS

2.1 Vision-Language Pre-training(VLP)

Benefited from the cross-modal interactions performed by the trans-
former [33], Vision-Language Pre-training (VLP) has improved per-
formance on various joint vision-and-language(V+L) downstream
tasks [8, 12, 20, 25, 29]. There are two mainstream architectures
for bridging the cross-modal semantic gap: dual-stream architec-
ture and single-stream architecture. The former first encodes visual
and language modalities separately and then fuses the two repre-
sentations through a transformer network; the latter collectively
operates on a concatenation of image and text inputs. In the work,
we consider ALBEF [16] and TCL [36] as example to examine the
former, and ViLT [13] as example to examine the latter.

2.2 Bias Measurement and Elimination in VLP

Much research work [1, 41] has been done on evaluating bias in
unimodal language models(LM). [1] proposed cosine-based word
embedding bias measure, and showed that word embeddings like
GloVe [24] encode gender biases. Furthermore, [27] investigates
two unimodal biases in VL-BERT [28]. However, there exists very
limited work for such study in multimodal bias of VLP due to the
cross-modal interaction. As far as we know, only [26] applied the
cosine-based word embedding bias measure to multimodal bias
of VLP models. However, [15] proved this method is not suitable
for the contextual setting, as direct word will be affected by the
context of image and text, resulting in inconsistent measure result.
Counterfactually Measuring and Eliminating Social Bias in Vision-Language Pre-training Models

MM °22, October 10-14, 2022, Lisboa, Portugal

 

 

 

Bias Factual target probability CounterFactual target probability Bias
Source P(T|V-Ly) P(T|V-Leg) Measure
Multimodal The man is [mask]
Vie | PCH) PCV)
ision The woman is [mask] P(B|V-Lop) - P(BIV-Lp)
Language The woman is [mask] The man is [mask]

 

 

Vision: E] Factual image [1 CF-adversarial

Language : AFactual word A CF-replaced

Figure 3: The proposed CounterBias to counterfactually measure social bias between target concept T and bias concept B.

This motivates us to design bias measure scheme to consider the
multimodal nature of the input in VLP models. For bias elimina-
tion, there has been works [30, 35] focused on eliminating bias in
unimodal language models. However, eliminating the social bias in
VLP models remain unexplored till now.

3 COUNTERFACTUALLY MEASURING SOCIAL
BIAS IN VLP

The pre-training task of Masked Language Modeling (MLM) has
been recognized as confidentially manifesting VLP model behavior
by maximizing the mutual information(MI) between [MASK]ed
token and the image-text input [16], and [15, 27] utilized [MASK]ed
token to probe the model’s response of its context to measure
unimodal contextual bias. These provide theorectical basis and
inspire us to use the prediction of [MASK]ed token to probe VLP
model’s understanding of the bias and target concepts. Specifically,
when we construct the input to VLP model with image and text
template masking off certain target concept T (e.g., template of “the
woman is [MASK]” masking off the activity concept “shopping” in
Figure 1), the output prediction probability can indicate how VLP
model probes the concept from the given factual image-text pair.
We denote the calculation of factual target probability as follows:

(1)

Social bias essentially refers to how the change of bias concept
in the input influences the model’s inference on the target concept.
Note that ideally only the involved bias concept is changed but
fixing all the other information, so as to accurately examine the
inherent influence from changed bias concept to target concept
inference. However, as discussed in Section 1, it is almost impos-
sible to perfectly control and keep the other variables unchanged.
Figure 2 shows some “shopping” images with their predicted fac-
tual target probability. We can see among images with the same
bias concept (in the same row), the diversity of other variables like
environment and background objects imposes obvious influence
on the predicted target probability, even more significant than the
bias concepts (between the two rows). This indicates relying on
the observed data for bias measurement is unreliable. To avoid the
negative effects of image diversity as a confounder, we propose
CounterBias to counterfactually change the bias concepts from ob-
served image-text pairs without changing other information, and
quantify the social bias by examining the difference between the

P(T | V-Lf) = P([MASK] = T|Vp, Lf)

4998

predicted target probability of observed factual data P(T | V-Ly)
and generated counterfactual data P(T | V-Le,).

Regarding how to counterfactually change the bias concepts in
the visual modality, a direct idea is to use GANs to edit images.
However, It is generally accepted that GANs-based editing can un-
intentionally change other information(e.g., image background)[3,
7, 10, 11, 23] , which defeats the purpose of controlling other infor-
mation unchanged.

Model-specific adversarial attacks. To avoid changing informa-
tion other than bias concept, we propose model-specific adversarial
attacks to counterfactually change the bias concepts in image. In
addition to affecting other information as little as possible, model-
specific adversarial attacks ensure that all bias features that the VLP
model relies on are counterfactually altered. Specifically, we create
a bias prompt template with masking the factual bias concept, “a
[MASK] is in the picture”, as text input to guide the VLP model
to classify bias concept B (e.g., woman) in the image input. Then
we use the prediction probability P(B | Vr) of the [MASK]ed bias
concept word to represent the bias concept that the model extracts
from the factual image Vy:

(2)

where B is the factual bias concept of input image. Similar to tra-
ditional adversarial attacks [6, 14, 22], using cross-entropy loss
between P(B | Vg) and counterfactual goal B’, and search the ad-
versarial perturbation 6 based on gradient of VLP model so that
bias concept of Vp + 6 is close to B’:

P(B | Vp) = P([MASK]=B|Vy, bias prompt))

d=argmin£ (P([MASK] [Vp + 6, bias prompt), B’| (3)

S| se
where B’ represents counterfactual goals, if B is woman, B’ is man.
And Vy + 6 is generated counterfactual image V.f.

Regarding how to counterfactually change bias concepts in lan-
guage modality, we manually replace the bias concept word in
factual text Ly, e.g. gendered word woman is replaced by man. The
generated counterfactual text is denoted as Ler.

Note that the bias concept is continuous in images rather than
discrete, e.g., the intensity of gender features in images of woman is
continuous, so the change of P(B|V-L) also needs to be considered
in the measure of social bias. The bias concepts in factual and
counterfactual images, P(B | Vr) and P(B | Vp), are formulated
as the predicted probability of the [MASK]ed biased concept word,
MM °22, October 10-14, 2022, Lisboa, Portugal

similar to Equation 2. Considering that the bias concept in text is
binarized, P(B | Ly) can thus be directly defined as 1, and P(B | Lr)
is 0. The bias concept P(B | V,L) in the image-text pairs can be
formulated as the average of the P(B | V) and P(B | L).

Once factual probability and counterfactual probability in target
concept and bias concept are calculated, we can measure how the
counterfactual change of bias concept influences the model’s infer-
ence on the target concept, as summarized in Figure 3. Specifically,
the involved social bias between bias concept B and target concept
T can be measured as:

P(T | V-Lep) - P(T | V-Lp)

OY * FE VLep) = PBL VL)

(4)

Considering the multi-modal characteristic of VLP, we are in-
terested in measuring social bias both in multimodality, and also
in visual and language unimodality. The difference between mea-
suring multimodal and unimodal biases lies in the counterfactual
samples used, i.e., the difference in P(T | V-Lep) and P(B | V-Lef)
calculations. The following two subsections described measuring
multimodal bias and unimodal bias in detail.

3.1 Measuring Multimodal Bias

Multimodal Bias refers to how the change of bias concept in both
the visual and language modalities influences the model’s infer-
ence on the target concept. Given counterfactual image-text pair
(Ver, Lef) as input, we denote the calculation of counterfactual
target probability as follows:

PCT | Vofs Lef) = P([MASK] = T|Vef; lef) (5)
The multimodal bias biasy 7 (T, B) can be formulated as:
P(T | Vep, Leg) — PCT | V-L
biasy_1(T,B) = (T | Vep, Leg) — P(T | V-Lp) 6)

P(B | Vops Leg) — P(B | Vp. Lp)

biasy ,(T, B) is an instance-level bias measure for model bias. Posi-
tive values indicate target concept T biased towards B, and negative
values indicate that the bias direction of T is opposite to B. Further,
we also measure dataset-level social bias, by applying indicator
function to align the bias directions represented by positive values,
and calculate the average bias over a set of instances Sy which
contain the specific target concept T:

biasy 1 (T) = Es, [1 pap, biasy-_(T, B) - L pap, biasy-(T, B)] (7)

where By and B, are pre-defined, and the sign of biasy_,(T) indi-
cates the bias direction - positive for Bp, and negative for B,. If Bo
is male, positive values indicate T biased towards male.

3.2 Measuring Unimodal Bias

Unimodal bias refers to how the change of bias concept in uni-
modality influences the model’s inference on the target concept.
We use counterfactual images and counterfactual texts to discover
visual unimodal bias and language unimodal bias, respectively.

For visual unimodal bias, we calculate the counterfactual target
probability from given counterfactual visual V.¢ input and factual
language input L¢ as follows:

P(T | Vp, Leg) = P([MASK] = T|Ver, Lp) (8)

4999

Yi Zhang, Junyang Wang, & Jitao Sang

Activity 13K
Working
Female Male

Occupation 11K

Engineer Judge
Female Male Female

Crying
Female

Male Male

    

Figure 4: Example images in the collected VL-Bias dataset.

Table 1: Templates for caption generation.

 

| Template

The {gender} is [MASK]

The {gender} in the photo is [MASK]
A {gender} who is [MASK]

A photo of a {gender} who is [MASK]

Type

Sentence w/o specification
Sentence w/ specification
Phrase w/o specification
Phrase w/ specification

 

 

Then, visual bias biasy (T, B) can be calculated as follows:
PCL | VopsLp) - PCT | V-Lp)

Mav 2) = SBT Vg) PBL Wp

(9)

For the language unimodal bias, we calculate the counterfactual
target probability from given counterfactual visual Vp input and
factual language input L¢¢ as follows:

P(T | Vp; Lop) = P([MASK] = T|Vy, Lef) (10)
Then, language bias bias; (T, B) can be calculated as follows:
P(T | Vp,£ —P(T|V-L
bias, (T, B) = PO Wop hep) POF) (11)

P(B| Lop) — L(B | Vp)

4 MEASURING RESULTS
4.1 VL-Bias Dataset

Fairness research in CV mainly focuses on human face analy-
sis [9, 34, 39], since existing face datasets such as CelebA [19] and
UTKface [40] have detailed annotation of both bias and target con-
cepts of faces. For fairness in NLP models, researchers can measure
the social bias of NLP models by means of manually construct-
ing language templates [31]. However, in vision-language research
bridging CV and NLP, few studies focus on fairness problems due
to the lack of both bias and target concepts annotation.

To address this issue, we construct VL-Bias to facilitate the study
on analyzing social bias in VLP models. Using gender as the bias
concept example, we selected 52 activities and 13 occupations re-
lated to humans!.

For image collection, in addition to downloading human-related
images from the Internet, we also collected human-related images
from the existing image datasets, such as MS-COCO [18], Flickr [37].

'The VL-Bias dataset, as well as the complete activity and occupation concept list, is
available at https://github.com/VL-Bias/VL-Bias
Counterfactually Measuring and Eliminating Social Bias in Vision-Language Pre-training Models

MM °22, October 10-14, 2022, Lisboa, Portugal

 

Human stat sewing shopping fishing skating climbing
stereotype Activities ALBEF .____1____1__o1 «> -99 @90-'@8 @@-@
ee -0.5 0 0.5
. climbing drinking driving i i aby spe
Male fishing jumping lifting TcL sewing shopping cow. ow climbing — fishing
ase working skating biking 0.5 om 05
shopping cooking | skating shopping fishing jumping
Female- | washing sewing baking ViLT e—e e—e- 008 eho o--— e e
biased | picking serving crying 05 0 0.5
sweeping cleaning — towards female towards male ———>

Figure 5: The human gender stereotypes (Left) and the observed multimodal bias (B@V-L) of three examined VLP models on

different activities (Right).

P(female|Vp,L 7) =0.98 P(female|Vop.L cf) =0.04

   

   

Gender
Counterfactual

[MASK] ” “the m

P([MASK]="sewing"|V;,L,) = 0.84

_—

“the woman is

anis [MASK] ’
P([MASK]="sewing"|Vcg. Log) =0.57
P(sewing|LgLo¢)-P(sewing|VpL¢ )

P(female|Lplice)-P(female|VLe)
= 0.287

bias(sewing, female) =

Ol Factual image [J CF-adversarial A Factual word A CF-replaced
Figure 6: Example of the calculated social bias in VLP model

ALBEF.

Then, we annotated gender concepts, activity concepts, and occu-
pation concepts of the images and removed the images that did
not explicitly contain these concepts. We illustrate some annotated
examples in Figure 4.

We also generate corresponding captions for each image accord-
ing to the annotation. Consider different languages and grammars
have different effects on the prompt of [MASK], we use four tem-
plates * described in Table 1 to generate captions. Finally, for each
template, we have collected 24k image-text pairs, including 13K for
the 52 activities and 11K for the 13 occupations.

4.2 The Observed Social Bias in VLP

Current VLP models can be divided into dual- and single- stream
models according to architectures [13]. In dual-stream VLP models,
the visual and textual inputs are first processed by two uni-modal
encoders respectively, then the resulting representations are fed
into cross-modal Transformer layers. For single-stream, VLP models
use the concatenation of the image-text pair directly as the input
of cross-modal Transformer layers. To deeply analyze the social
bias in VLP models, we selected the representative VLP models of
these two architectures respectively. For dual-stream, we consider
ALBEF and TCL, and for single-stream, we consider ViLT. Taking
gender bias as an example, we evaluate each VLP model on our
VL-Bias dataset.

2We considered sentence and phrase, and whether to use "A photo of” in the text to
specify that the text is about the image [25].

5000

Table 2: The social bias (in %) in VLP models in terms of multi-
modal bias(B@V-L), visual modal bias(B@V), and language
modal bias(B@L). For each bias, we used four text templates
to measure bias and calculate the average(Avg.).

 

 

 

 

 

 

Activity 13K Occupation 11K

B@V-L B@V B@L|B@V-L B@V B@L

Sw/o 10.29 9.44 3.98 13.31 9.62 2.38

a Sw/ 11.33 6.54 7.15 16.00 10.46 6.46
& Pwo 10.75 8.21 4.27 11.85 7.85 1.69
= Pw 9.85 7.54 3.31 11.31 8.23 2.08
Avg. 10.56 7.93 4.68 13.12 9.04 3.15
Sw/o 7.62 6.96 3.67 12.00 10.85 3.92

a Sw/ 8.94 6.15 5.50 16.46 12.01 6.00
z Pwo 7.52 5.90 2.65 10.15 6.85 4.77
Pw 6.88 5.38 2.37 8.38 5.46 2.85
Avg. 7.74 6.10 3.55 11.75 8.79 4.84
Sw/o 8.62 6.52 2.37 16.15 17.23, 3.62

HH Sw/ 9.48 8.71 2.35 14.92 17.35 4.62
Pwo 14.65 14.38 2.27 12.92 17.61 3.38
Pw 11.19 11.85 1.69 13.23 15.15 4.46
Avg. 10.99 10.37 2.17 14.31 16.84 4.02

 

Using the multimodal bias measurement as example, given tar-
get concept T and bias concept B, instance-level social bias can
be calculated according to Equation 6. One example is illustrated
in Figure 6. We can see the instance-level measurement provides
intuitive understanding of the involved social bias. By aggregating
instances with the same target concept, we can derive the final
dataset-level social bias according to Equation 7. The following
elaborates some of the key observations.

Social bias is prevalent in the examined VLP models. We
report the gender bias of the 52 activities and 13 occupations in
Table 2, where each element represents the average absolute value
of the measured bias results. Main observations include: (1) all
three VLP models involve with significant gender bias. Taking
multimodal bias (B@V-L) of ALBEF in activities as an example,
10.56% means that by altering the gender information of the input,
the model’s predicted probability of [MASK]ed activity will change
by an average of 10.56%. (2) Gender bias exists in both modalities
of VLP model and is more serious in visual modality (B@V) than in
language modality(B@L). We conjecture that this is because images
have richer information than text, which leads the model to capture
MM °22, October 10-14, 2022, Lisboa, Portugal

Language bias

0.2:
towards male

skating

(0.30, 0. oy

climbing

Yi Zhang, Junyang Wang, & Jitao Sang

Language bias
0.2

 

4
e e (0.48, o.02)\, a
<
towards female % xt eo @ © towards mate@ 5
“0.4 0.3 02m 01 "ole o1 0.2 0.3 0.4 oz
ee o* a
® ®
sewing Od
(0.25, -0.13) ~“®
a @male @ female
(0.22, 018)" “@ towards female
.2.
(a) ALBEF

towards male
skating fishing

(0.24, 0.05} 01 at (0.21, 0.10}

jumping
Yy YW (0.22, 0.05} s
@ Je@ ° z
towards female e o towards male. 5
-0.4 -0.3 02 -O1 °e or 0.2 0.3 04 o
shopping —x ra
£0.23,-0.06 © a

@male @female
-9.2 Ltowards female
(b) ViLT

Figure 7: The visual modal bias (B@V) and language modal bias (B@L) in dual-stream ALBEF and single-stream ViLT.

more statistical properties within the visual modality. This further
leads to stronger social bias in visual modality than in language
modality. (3) Bias measurement is sensitive to the choice of text
input template, and using the average(Avg. in Table 2) of multiple
templates as the measurement result is more generalized.

The gender bias contained in VLP models is basically con-
sistent to human gender stereotypes. To investigate whether
the social bias involved in VLP models matches that from human
stereotypes, we recruited 100 Amazon Mechanical Turk workers for
labeling the stereotypes for the 52 activities and retained 19 activi-
ties with agreed gendered stereotyped label 3(male or female-biased,
illustrated in Figure 5 (Left)).

In Figure 5 (Right), we plot the multimodal bias (B@V-L) of
examined VLP models with each activity, marked with human
stereotypes using colored points(red for male-biased, green for
female-biased, gray for neutral). The sign of B@V-L indicates the
direction of gender bias, positive for “male,” and negative for “fe-
male”. We observed that the social bias B@V-L manifests is basically
consistent with human gender stereotypes, which indicates that
VLP models inherit human stereotypes from the training data. In
the three examined VLP models, compared to ALBEF and TCL,
ViLT slightly deviates from human stereotypes in some of the ac-
tivities, such as "skating". This may be related to the inconsistency
in the bias direction between visual and language modalities in the
following analysis.

The examined dual-stream ALBEEF/TCL and single-stream
ViLT show different cross-modal conformities regarding the
observed unimodal social bias. Unlike unimodal pre-training
models, there are complex vision-language cross-modal interac-
tions in VLP models. The relationship between the social bias in
two modalities may be of interest. Taking dual-stream ALBEF and
single-stream ViLT as examples, in Figure 7, we use scatter plots
to compare the difference between visual modal bias (x-axis) and
language modal bias (y-axis). Each point corresponds to an activ-
ity, with three colors representing different human stereotypes.
For ALBEF (Figure 7(a)), it is easy to find the strong conformity

3 For each activity, we asked 100 distinct AMT workers to label it as male-
biased, female-biased or neutral. An activity obtains gender-biased stereo-
types label only when at least 90% works offer the same label. The rest 33
activities are associated with neutral label in the resultant human stereotype.

5001

Table 3: The social bias in language modality of three VLP
models and language model BERT.

 

Target concept | ALBEF TCL ViLT BERT

 

52 Activities
13 Occupations

0.53
0.69

0.59
0.71

0.39
0.48

0.15
0.20

 

between visual modal bias and language modal bias: all activities
are basically located in the first and third quadrants of the coor-
dinate system, which means that the discovered gender biases in
two modalities follow the same direction. Moreover, the bias of two
modalities in 84% of activities is consistent with human stereotypes.
However, the bias in ViLT is different from ALBEF, as shown in
Figure 7(b), visual modal bias and language modal bias have large
differences in direction, whit activities widely spreading across the
four quadrants. Bias in unimodality is also inconsistent with human
stereotypes in some activities, such as visual bias in skating is also
inconsistent with humans.

The conformity between visual modal bias and language modal
bias in dual-stream ALBEF is possibly attributed to the explicit
align constraints between the two unimodal representations (i.e.,
Image-Text Contrastive Learning pre-training task). By aligning
the two modalities in dual-stream, ALBEF forces the two modalities
to learn only shared information between visual and textual inputs,
resulting in more consistent social biases contained in the two
modalities, and biases in both two modalities are also consistent
with human stereotypes. Moreover, the inconsistency between the
learned information of the two modalities in ViLT again explains
the reason for the slight deviation of multimodal bias(B@V-L) and
human stereotypes.

The examined VLP models contain stronger gender bias than
unimodal pre-training model BERT. It is common practice
for VLP models to use BERT [5] as backbone. We investigate the
difference in language modal bias between BERT-based VLP models
and original BERT.

For a fair comparison, we remove visual input when predicting
[MASK]ed target concepts, which ensures that both VLP models
and BERT rely only on language input:

PCT | Lf) = P([MASK] = T|Vnone. Lf)

P(T | Lep) = P([MASK] = T\Vnone, Lep) 0)
Counterfactually Measuring and Eliminating Social Bias in Vision-Language Pre-training Models

P([MASK]]V;.L,) Du_. P([MASK]|Vop.L og)
t t
VLP Shared VLP
t Ot
Vel¢ => VopLot
i DA Mnckedinken generate ~~
[= "hersence CD srecsing CE)ooirteracul
NUL examples 7

Figure 8: The overall framework of our proposed FairVLP.

Then, considering that the absence of visual input may result in
a lower value of P(T | Lrjep), we use log probability for stable
numerical computations:

log P(T | Ler) — log Py (T | Lp)
PBI Lep) — PCB Ep)

For both VLP models and BERT, we use Equation 13 to calculate
language modal bias. We report the language modal bias of the
52 activities and 13 occupations in Table 3. The results show that
the examined three VLP models contain stronger gender bias than
unimodal pre-training model BERT. This suggests that learning of
cross-modal representations generally exacerbates the dependence
of social biases in language modality.

bias, (T, B) = (13)

5 COUNTERFACTUALLY ELIMINATING
SOCIAL BIAS IN VLP

5.1 Method

Section 4 observes that social bias is prevalent in the examined
VLP models, which can lead to unintended social consequences
in downstream tasks. Inheriting the idea of counterfactually mea-
suring social bias, we propose a fair Vision-Language Pre-training
task FairVLP that eliminates social bias by minimizing the differ-
ence in the predicted [MASK]ed probability between factual and
counterfactual image-text pairs.

Following the setup of masked language modeling (MLM) pre-
training tasks commonly used in VLP models, our FairVLP is ap-
plicable to both single- and dual-stream structures without being
restricted to a specific VLP architecture. For the image-text pairs
input at each training step i, we randomly mask out the text input
tokens with probability g and replace them with the special token
[MASK]. The image and [MASK]ed text are denoted as Vi and Le

Counterfactual samples generation. Similar to Section 3, for
factual image Ves we use model-specific adversarial attack to gen-

erate counterfactual image Vie by altering the bias concept of Vp.

And we directly replace the words about bias concept (e.g., man is
replaced to woman) in factual text L’. to generate counterfactual

. f
text Lt f"
Minimize the difference in the prediction of [MASK]ed words
between factual and counterfactual samples. Following the
formulation of MLM, we can utilize both the image V and the

5002

MM °22, October 10-14, 2022, Lisboa, Portugal

Table 4: Social bias(in %) in VLP models with different debi-
asing pre-training tasks.

 

Occupation 11K

 

 

 

 

Method Activity 13K
B@V-L B@V B@L | B@V-L B@V B@L
vanilla 13.02 1110 489 | 1484 14.23 3.15
1121 9.25 «4.03 | 12.47 12.35 2.98
W417 9.54 «4.39 | 13.52 12.19 3.08
CyeleGAN 10.92 9.23 432 | 12.07 11.35 3.01
FairVLP-V | 841 7.23 3.91 | 911 7.98 2.86
FairVLP-L | 853 7.46 382 | 1123 961 1.77
FairVLP | 697 6.76 3.29| 7.74 7.15 1.72

 

text L to predict [MASK]ed words P([MASK] | V,Z). As shown
in Figure 8, we use factual and counteract samples to predict
P([MASK] | VivL Le) and P([MASK] | V’ cf):

At current 1 aining step, our FairVIP ie eliminates social
bias in VLP models by minimizing the Kullback-Leibler(KL) di-
vergence in [MASK]ed word prediction distributions between the

factual and counterfactual image-text pairs:
Lx = Di {P([MASK] | VevL Ly)I/P([MASK] LV, a) (14)

With the basic cross-entropy loss #f for the factual and counterfac-
tual samples in MLM task:

Lmim =
+H (M?, P([MASK] | v?

H(M?, P([MASK] | Ven a)
Lip)

where M! is the [MASK]ed word, and the final training objective is
to minimize Lrgirv Lp:

(15)

(16)

Lerairvip = (1 - @L£Lmtm + + L£KL

where a is the coefficient weight to control Lxz.

By iteratively generating counterfactual samples and minimizing
L£Fairvip, The generated counterfactual samples Vip and ui
guaranteed to have counterfactual properties for the VLP model
at training step i. Minimizing the difference in prediction between
factual and counterfactual samples, as a result, the VLP model’s
inference on target concept is independent of the bias concept at
training step i. Throughout pre-training process, FairVLP consis-
tently prevents the VLP from learning social bias.

5.2 Experimetnal Results

Experiment Setup. We follow dual-stream ALBEF architecture
and common MLM and ITM (image-text matching) pre-training
tasks to pretrain debiasing VLP model on the MS-COCO dataset.
To evaluate the debiasing performance of FairVLP, we consider
two common and effective debiasing methods in language models
as baselines: Gender Swapping(GS) [42] and Dropout Regulariza-
tion(DR) [35]. GS swaps gendered words in text input for mitigating
the correlation between gender and other concepts. DR eliminates
model over-fitting to gender concepts by increasing dropout rate.
We migrated the idea of GS to a visual modality and edited the
concept of face gender in the images using CycleGAN [43]. We
MM °22, October 10-14, 2022, Lisboa, Portugal

Yi Zhang, Junyang Wang, & Jitao Sang

Table 5: Fine-tuned image-text retrieval results on Flickr30K and COCO datasets.

 

Method Flickr30K (1K test set)

MSCOCO (5K test set)

 

 

TR IR TR IR
R@1 R@5 R@10 R@1 R@S R@1IO|R@1 R@S5 R@1IO R@1 R@S RE@1O
Vanilla 90.8 98.6 99.3 79.4 94.9 97.2 70.5 89.9 94.6 53.5 79.3 87.2
FairVLP | 90.8 98.8 98.3 78.9 94.8 97.3 70.3 90.3 94.9 53.6 79.3 87.1
also consider two variants of our FairVLP for ablation study, elimi- P(horse) P(cow) P(horse) P(cow)

nating bias in visual modality by using only visual counterfactual
samples(FairVLP-V) and eliminating bias in language modality by
only using language counterfactual samples(FairVLP-L). For all
methods, the learning rate is 1e-4 with 0.02 weight decay, and the
batch size is 256.

Debiasing performance. Table 4 summarizes the debiasing per-
formance of different methods measured on the VL-Bias dataset.
Main observations include: (1) All the proposed three settings of
FairVLP obtain remarkable debiasing performance than that of
baselines. Among three settings, by simultaneously performing
counterfactual debiasing on both two modalities, FairVLP demon-
strates the best performance in eliminating social bias. (2) SW and
CycleGAN show very limited debiasing results due to the lack of
minimizing the difference in prediction of [MASK]ed word between
the original sample and gender-swapping sample. (3) DR under-
performs SW, we owe this result to the random dropout does not
change the statistical relationship in the training data.

Moreover, We observe that FairVLP-V, which is designed to elim-
inate the bias of visual modality, not only achieves a significant
debiasing effect in the visual modality, but also demonstrates a
remarkable debiasing effect in language modality. And FairVLP-L
also performs debiasing for visual modality. We conjecture that this
is because gender conflict in image-text pairs is an implicit form of
data augmentation, which effectively makes the modeling of other
concepts independent of gender concept.

Downstream task performance. For the downstream tasks
of the pre-training model, we follow the most commonly used
Image-Text Retrieval among V+L tasks for model performance eval-
uation. We evaluate pre-trained models on the Flickr30K and COCO
benchmarks, and fine-tune the pre-trained model using the training
samples from each dataset.

Table 5 report results on fine-tuned image-text retrieval, respec-
tively. Our FairVLP pre-training task achieves comparable if not
better performance with Vanilla. This suggests that FairVLP can
leverage reasonably constructed counterfactual samples to effec-
tively eliminate social bias without affecting the model’s learning
of reliable knowledge. This phenomenon further indicates that the
performance of the downstream task can be compatible with social
bias elimination.

6 DISCUSSION

Visual-Language Pre-training models are designed to address the
lack of downstream task-specific data, so the key to performance
of downstream task is the learning of reliable cross-modal knowl-
edge in pre-trained models. In addition to revealing social bias in

5003

   
   

 

   

0.76

“A [MASK] is on the grass”

 

 

Ground¢——————_ Grass
sseig) <———— punaigy

“A [MASK] is on the ground *
OO Factual image

"A[MASK] is on the grass”
OO CF-adversarial

Figure 9: Illustration of discovering spurious correlation by
CounterBias.

VLP models, we discuss that the proposed CounterBias provides
new insights into the opaque representation of knowledge in VLP
models. In this spirit, we examined the ability of CounterBias to dis-
cover correlations between two arbitrary concepts in VLP models.
Specifically, taking ALBEF as an example, we use CounterBias to
measure the correlation between animal concepts (e.g., horse and
cow) and background concepts (e.g., grass and ground). As shown
in Figure 9, the MLM prediction difference between factual and
counterfactual image-text pairs well reflects the correlation learned
in ALBEF which shows that the model learns the spurious correla-
tion between horse and grass, as well as cow and grass. Accordingly,
with available human annotation for spurious correlations, FairVLP
can also be considered as a method for regularizing the model to
reliably learn knowledge.

7 CONCLUSION

In this work, we propose to measure social bias in VLP models
by comparing how the model of target concept changes factual
and counterfactual samples with different bias concepts. A novel
VL-Bias dataset is introduced for studying social bias in multimodal
models. Taking gender as an example, our bias measure results on
VLP models indicate that gender bias is prevalent in the examined
VLP models. We propose to counterfactually eliminate social bias
via minimizing the modeling differences between factual and coun-
terfactual samples with different bias concepts. In the future, we
are working towards counterfactually probing more knowledge as
well as regulating the learning of VLP models.

ACKNOWLEDGMENTS

This work is supported by the National Key R&D Program of China
(Grant No. 2018AAA0100604).
Counterfactually Measuring and Eliminating Social Bias in Vision-Language Pre-training Models

REFERENCES

(4)

(2

[3]

[4]

[5]

[6]

[7

[8

[9]

10]

f4]

(12]

(13]

(14)

(15]

[16]

[17]

[18]

{19]

(20]

(21]

(22]

Aylin Caliskan, Joanna J Bryson, and Arvind Narayanan. 2017. Semantics derived
automatically from language corpora contain human-like biases. Science 356,
6334 (2017), 183-186.

Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan,
Yu Cheng, and Jingjing Liu. 2020. Uniter: Universal image-text representation
learning. In ECCV.

Kristy Choi, Aditya Grover, Trisha Singh, Rui Shu, and Stefano Ermon. 2020.
Fair generative modeling via weak supervision. In International Conference on
Machine Learning. PMLR, 1887-1898.

Yuhao Cui, Zhou Yu, Chunqi Wang, Zhongzhou Zhao, Ji Zhang, Meng Wang, and
Jun Yu. 2021. ROSITA: Enhancing Vision-and-Language Semantic Alignments
via Cross-and Intra-modal Knowledge Integration. In Proceedings of the 29th ACM
International Conference on Multimedia. 797-806.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. [n.d.].
Bert: Pre-training of deep bidirectional transformers for language understanding.
(n.d), 4171-4186.

Jan J. Goodfellow, Jonathon Shlens, and Christian Szegedy. 2015. Explaining
and harnessing adversarial examples. In 3rd International Conference on Learning
Representations.

Aditya Grover, Jiaming Song, Ashish Kapoor, Kenneth Tran, Alekh Agarwal, Eric J
Horvitz, and Stefano Ermon. 2019. Bias correction of learned generative models
using likelihood-free importance weighting. Advances in neural information
processing systems 32 (2019).

Xiaowei Hu, Xi Yin, Kevin Lin, Lijuan Wang, Lei Zhang, Jianfeng Gao, and
Zicheng Liu. 2020. Vivo: Surpassing human performance in novel object caption-
ing with visual vocabulary pre-training. (2020).

Xiaowen Huang, Jiaming Zhang, Yi Zhang, Xian Zhao, and Jitao Sang. 2021.
Trustworthy Multimedia Analysis. In Proceedings of the 29th ACM International
Conference on Multimedia. 5667-5669.

Niharika Jain. 2020. A Study on Generative Adversarial Networks Exacerbating
Social Data Bias. Ph.D. Dissertation. Arizona State University.

Niharika Jain, Alberto Olmo, Sailik Sengupta, Lydia Manikonda, and Subbarao
Kambhampati. 2022. Imperfect ImaGANation: Implications of GANs exacerbating
biases on facial data augmentation and snapchat face lenses. Artificial Intelligence
304 (2022), 103652.

Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le,
Yun-Hsuan Sung, Zhen Li, and Tom Duerig. 2021. Scaling up visual and vision-
language representation learning with noisy text supervision. In International
Conference on Machine Learning. PMLR, 4904-4916.

Wonjae Kim, Bokyung Son, and Ildoo Kim. 2021. Vilt: Vision-and-language trans-
former without convolution or region supervision. In International Conference on
Machine Learning. PMLR, 5583-5594.

Alexey Kurakin, Ian Goodfellow, and Samy Bengio. 2017. Adversarial examples
in the physical world. 5th International Conference on Learning Representations
(2017).

Keita Kurita, Nidhi Vyas, Ayush Pareek, Alan W Black, and Yulia Tsvetkov. 2019.
Measuring Bias in Contextualized Word Representations. In Proceedings of the
First Workshop on Gender Bias in Natural Language Processing. 166-172.

Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong,
and Steven Chu Hong Hoi. 2021. Align before fuse: Vision and language repre-
sentation learning with momentum distillation. Advances in Neural Information
Processing Systems 34 (2021).

Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Li-
juan Wang, Houdong Hu, Li Dong, Furu Wei, et al. 2020. Oscar: Object-semantics
aligned pre-training for vision-language tasks. In European Conference on Com-
puter Vision. Springer, 121-137.

Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva
Ramanan, Piotr Dollar, and C Lawrence Zitnick. 2014. Microsoft coco: Common
objects in context. In European conference on computer vision. Springer, 740-755.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. 2015. Deep Learning Face
Attributes in the Wild. In Proceedings of International Conference on Computer
Vision (ICCV).

Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. 2019. Vilbert: Pretrain-
ing task-agnostic visiolinguistic representations for vision-and-language tasks.
Advances in neural information processing systems 32 (2019).

Jianjie Luo, Yehao Li, Yingwei Pan, Ting Yao, Hongyang Chao, and Tao Mei. 2021.
CoCo-BERT: Improving Video-Language Pre-training with Contrastive Cross-
modal Matching and Denoising. In Proceedings of the 29th ACM International
Conference on Multimedia. 5600-5608.

Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and
Adrian Vladu. 2017. Towards deep learning models resistant to adversarial attacks.
arXiv preprint arXiv:1706.06083 (2017).

5004

[23]

[24]

[25]

[30]

[33]

[34]

[35]

[36]

[37]

[38]

[39]

[40]

[41]

[42]

[43]

MM °22, October 10-14, 2022, Lisboa, Portugal

Sachit Menon, Alexandru Damian, Shijia Hu, Nikhil Ravi, and Cynthia Rudin.
2020. Pulse: Self-supervised photo upsampling via latent space exploration of
generative models. In Proceedings of the ieee/cvf conference on computer vision

and pattern recognition. 2437-2445.
Jeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. Glove:

Global vectors for word representation. In Proceedings of the 2014 conference on
empirical methods in natural language processing (EMNLP). 1532-1543.

Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
et al. 2021. Learning transferable visual models from natural language supervision.
In International Conference on Machine Learning. PMLR, 8748-8763.

Candace Ross, Boris Katz, and Andrei Barbu. 2020. Measuring Social Biases in
Grounded Vision and Language Embeddings. arXiv preprint arXiv:2002.08911.
Tejas Srinivasan and Yonatan Bisk. 2021. Worst of both worlds: Biases com-
pound in pre-trained vision-and-language models. arXiv preprint arXiv:2104.08666
(2021).

Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai. 2019.
V1-bert: Pre-training of generic visual-linguistic representations. arXiv preprint
arXiv: 1908.08530 (2019).

Alane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang, Huajun Bai, and Yoav Artzi.
2019. A Corpus for Reasoning about Natural Language Grounded in Photographs.
In Proceedings of the 57th Annual Meeting of the Association for Computational
Linguistics. 6418-6428.

Tony Sun, Andrew Gaut, Shirlyn Tang, Yuxin Huang, Mai ElSherief, Jieyu Zhao,
Diba Mirza, Elizabeth Belding, Kai-Wei Chang, and William Yang Wang. 2019.
Mitigating Gender Bias in Natural Language Processing: Literature Review. As-
sociation for Computational Linguistics (ACL 2019) (2019).

Zeyu Sun, Jie M Zhang, Mark Harman, Mike Papadakis, and Lu Zhang. 2020.
Automatic testing and improvement of machine translation. In Proceedings of the
ACM/IEEE 42nd International Conference on Software Engineering. 974-985.

Hao Tan and Mohit Bansal. 2019. LKMERT: Learning Cross-Modality Encoder
Representations from Transformers. In Proceedings of the 2019 Conference on
Empirical Methods in Natural Language Processing and the 9th International Joint
Conference on Natural Language Processing (EMNLP-IFCNLP). 5100-5111.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. Advances in neural information processing systems 30 (2017).

Mei Wang, Weihong Deng, Jiani Hu, Xunqiang Tao, and Yaohai Huang. 2019.
Racial faces in the wild: Reducing racial bias by information maximization adap-
tation network. In Proceedings of the ieee/cvf international conference on computer
vision. 692-702.

Kellie Webster, Xuezhi Wang, Jan Tenney, Alex Beutel, Emily Pitler, Ellie Pavlick,
Jilin Chen, Ed Chi, and Slav Petrov. 2020. Measuring and reducing gendered
correlations in pre-trained models. arXiv preprint arXiv:2010.06032 (2020).
Jinyu Yang, Jiali Duan, Son Tran, Yi Xu, Sampath Chanda, Liqun Chen, Belinda
Zeng, Trishul Chilimbi, and Junzhou Huang. 2022. Vision-Language Pre-Training
with Triple Contrastive Learning. Proceedings of the IEEE conference on computer
vision and pattern recognition (2022).

Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. 2014. From image
descriptions to visual denotations: New similarity metrics for semantic infer-
ence over event descriptions. Transactions of the Association for Computational
Linguistics 2 (2014), 67-78.

Shengyu Zhang, Tan Jiang, Tan Wang, Kun Kuang, Zhou Zhao, Jianke Zhu, Jin
Yu, Hongxia Yang, and Fei Wu. 2020. Devlbert: Learning deconfounded visio-
linguistic representations. In Proceedings of the 28th ACM International Conference
on Multimedia. 4373-4382.

Yi Zhang and Jitao Sang. 2020. Towards accuracy-fairness paradox: Adversarial
example-based data augmentation for visual debiasing. In Proceedings of the 28th
ACM International Conference on Multimedia. 4346-4354.

Zhifei Zhang, Yang Song, and Hairong Qi. 2017. Age progression/regression
by conditional adversarial autoencoder. In Proceedings of the IEEE conference on
computer vision and pattern recognition. 5810-5818.

Jieyu Zhao, Tianlu Wang, Mark Yatskar, Ryan Cotterell, Vicente Ordonez, and
Kai-Wei Chang. 2019. Gender Bias in Contextualized Word Embeddings. In
Proceedings of the 2019 Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies, Vol. 1.

Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, and Kai-Wei Chang.
2018. Gender Bias in Coreference Resolution: Evaluation and Debiasing Meth-
ods. In Proceedings of the 2018 Conference of the North American Chapter of the
Association for Computational Linguistics: Human Language Technologies, Volume
2 (Short Papers). 15-20.

Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. 2017. Unpaired
image-to-image translation using cycle-consistent adversarial networks. In Pro-
ceedings of the IEEE international conference on computer vision. 2223-2232.
