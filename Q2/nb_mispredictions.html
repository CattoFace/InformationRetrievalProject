<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>label</th>
      <th>text</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>30</th>
      <td>poli</td>
      <td>Political Bias and Factualness in News Sharing across more than 100,000 Online Communities Galen Weld,1Maria Glenski,2Tim Althoff1 1Paul G. Allen School of Computer Science and Engineering, University of Washington 2National Security Directorate, Paciﬁc Northwest National Laboratory {gweld, althoff}@cs.washington.edu, maria.glenski@pnnl.gov Abstract As civil discourse increasingly takes place online, misinfor- mation and the polarization of news shared in online com-munities have become ever more relevant concerns with realworld harms across our society. Studying online news shar-ing at scale is challenging due to the massive volume of con-tent which is shared by millions of users across thousandsof communities. Therefore, existing research has largely fo-cused on speciﬁc communities or speciﬁc interventions, suchas bans. However, understanding the prevalence and spread ofmisinformation and polarization more broadly, across thou-sands of online communities, is critical for the developmentof governance strategies, interventions, and community de-sign. Here, we conduct the largest study of news sharing onreddit to date, analyzing more than 550 million links spanning4 years. We use non-partisan news source ratings from MediaBias/Fact Check to annotate links to news sources with theirpolitical bias and factualness. We ﬁnd that, compared to left-leaning communities, right-leaning communities have 105%more variance in the political bias of their news sources, andmore links to relatively-more biased sources, on average. Weobserve that reddit users’ voting and re-sharing behaviorsgenerally decrease the visibility of extremely biased and lowfactual content, which receives 20% fewer upvotes and 30%fewer exposures from crossposts than more neutral or morefactual content. This suggests that reddit is more resilient tolow factual content than Twitter. We show that extremely bi-ased and low factual content is very concentrated, with 99%of such content being shared in only 0.5% of communities,giving credence to the recent strategy of community-widebans and quarantines. 1 Introduction Biased and inaccurate news shared online are major con- cerns that have risen to the forefront of public discourse re-garding social media in recent years. Two thirds of Amer-icans get at least some of their news content from socialmedia, but less than half expect this content to be accu-rate (Shearer and Matsa 2018). Globally, only 22% of sur-vey respondents trust the news in social media “most of thetime” (Newman 2020). Internet platforms such as Twitter,Facebook, and reddit account for an ever-increasing share ofthe dissemination and discussion of news (Geiger 2019). Copyright © 2021, Association for the Advancement of Artiﬁcial Intelligence (www.aaai.org). All rights reserved.Harms caused by biased and false news have substantial impact across our society. Polarized content on Twitter and Facebook has been shown to play a role in the outcome ofelections (Recuero, Soares, and Gruzd 2020; Kharratzadehand¨Ustebay 2017); and misinformation related to COVID- 19 has been found to have a negative impact on public healthresponses to the pandemic (Tasnim, Hossain, and Mazumder2020; Kouzy et al. 2020). Developing methods for reduc-ing these harms requires a broad understanding of the polit-ical bias and factualness of news content shared online, butstudying news sharing is challenging for three reasons: (1)the scale is immense, with billions of news links shared an-nually, (2) it is difﬁcult to automatically quantify bias andfactualness at scales where human labeling is often infeasi-ble (Rajadesingan, Resnick, and Budak 2020), and (3) thedistribution of links is complex, with these links shared bymany millions of users and thousands of communities. While previous research has led to important insights on speciﬁc aspects of news sharing, such as user engagement(Risch and Krestel 2020), fact checking (Vosoughi, Roy,and Aral 2018; Choi et al. 2020), speciﬁc communities (Ra-jadesingan, Resnick, and Budak 2020), and speciﬁc rumors(Vosoughi, Mohsenvand, and Roy 2017; Qazvinian et al.2011), large scale studies of news sharing are critical to un-derstanding polarization and misinformation more broadly,and can inform community design, governance, and moder-ation interventions. In this work, we present the largest study to date of news sharing behavior on reddit, one of the most popular socialmedia websites. We analyze all 559 million links submit-ted to reddit from 2015-2019 1, including 35 million news links submitted by 1.3 million users to 135 thousand com-munities. We rate the bias and factualness of linked-to newssources using Media Bias/Fact Check (MBFC), 2which con- siders how news sources favor different sides of the left-rightpolitical spectrum (bias), and the veracity of claims made inspeciﬁc news stories (factualness) (§3). In our analyses, we examine: the diversity of news within 1August 2019 was the most recent month of data available at the time of this study. 2While bias and factualness may vary from story to story, news source-level ratings maximize the number of links that can be rated, and are commonly used in research (Bozarth, Saraf, and Budak2020). ProceedingsoftheFifteenth InternationalAAAIConferenceonWebandSocial Media(ICWSM2021) 796 communities (§4), and how this diversity is composed of both the differences between community members and in- dividual members’ diversity of submissions; the impact of current curation and ampliﬁcation behaviors on news’ vis- ibility and spread (§5); and the concentration of extremely biased and low factual content (§6), examining the distribu- tion of links from the perspectives of who submitted themand what community they were submitted to. We show that communities on reddit exist across the left-right political spectrum, as measured by MBFC, but74% are ideologically center left. We ﬁnd that the diver-sity of left-leaning communities’ membership is similar tothat of equivalently right-leaning communities, but right-leaning communities have 105% more politically variednews sources, as their members individually post more var-ied links. This variance comes from the presence of linksthat are different from the community average, and in right-leaning communities, 74% of such links are to relatively-more biased news sources, 35% more than in left-leaningcommunities (§4). We demonstrate that, regardless of the political leaning of the community, community members’ voting and crosspost-ing (re-sharing) behavior reduces the impact of extremelybiased and low factual news sources. Links to these newssources receive 20% fewer upvotes (§5.2) and 30% fewerexposures from crossposts compared to more neutral andhigher factual content (§5.3). Furthermore, we ﬁnd that userswho submit such content leave reddit 68% more quicklythan others (§5.1). These ﬁndings suggest that low factualcontent spreads more slowly and is ampliﬁed less on redditthan has been reported for Twitter (Vosoughi, Roy, and Aral2018; Bovet and Makse 2019), although we do not directlycompare behavior across the two platforms. Differences be-tween reddit and Twitter may stem from reddit’s explicit di-vision into communities, or users’ ability to downvote con-tent, both of which help control content exposure. Extremely biased and low factual content can be challeng- ing to manage, as it is spread through many users, newssources, and communities. We ﬁnd that extremely biasedand low factual content is spread by an even broader set ofusers and communities relative to news content as a whole,exacerbating this challenge (§6). However, we ﬁnd that 99%of extremely biased or low factual content is still concen-trated in 0.5% of communities, lending credence to recentinterventions at the community level (Chandrasekharan et al.2017, 2020; Saleem and Ruths 2018; Ribeiro et al. 2020). Our work demonstrates that additional research on news sharing online is especially needed on the topics of whyusers depart platforms and where they go, why false newsappears to spread more quickly on Twitter than on reddit,and how curation and ampliﬁcation practices can manageinﬂuxes of extremely biased and low factual content. Finally, we make all of our data and analyses publicly available 3to encourage future work on this important topic. 3https://behavioral-data.github.io/news labeling reddit/2 Related Work Misinformation and Deceptive News. Social news plat- forms have seen a continued increase in use and a simultane-ous increase in concern regarding biased news and misinfor-mation (Mitchell et al. 2019; Marwick and Lewis 2017). Re-cent studies have used network spread (Vosoughi, Roy, andAral 2018; Ferrara 2017; Bovet and Makse 2019), contentconsumer (Allen et al. 2020), and content producer (Linvilland Warren 2020) approaches to assess the spread of mis-information. In this work, we examine news sharing behav-ior from news sources who publish content with varied de-grees of bias or factualness, building on related work that hasanalyzed social news based on the characteristics of a newsource’s audience (Samory, Abnousi, and Mitra 2020a) orthe type of content posted (Glenski, Weninger, and Volkova2018). Polarization and Political Bias. Many papers have recently been published on detecting political bias of online content either automatically (Baly et al. 2020; Demszky et al. 2019)or manually (Ganguly et al. 2020; Bozarth, Saraf, and Budak2020). Others have examined bias in moderation of content,as opposed to biased content or news sources themselves(Jiang, Robertson, and Wilson 2019, 2020). Echo cham-bers are a major consideration in understanding polariza-tion, with papers focusing on their development (Allison andBussey 2020) and the role of news sources in echo chambers(Horne, Nørregaard, and Adali 2019). Others have examinedwho shares what content with what political bias, but didso using implicit community structure (Samory, Abnousi,and Mitra 2020b). In this work, we examine thousands ofexplicit communities on reddit, characterizing their polar-ization by examining the political diversity of news sourcesshared within, and the diversity of the community memberswho contribute. Moderation and Governance. A large body of work has examined the role of moderation interventions such explana- tions (Jhaver, Bruckman, and Gilbert 2019), content removal(Chandrasekharan et al. 2018), community bans (Chan-drasekharan et al. 2017, 2020; Saleem and Ruths 2018) onoutcomes such as migration (Ribeiro et al. 2020), harass-ment (Matias 2019a) and harmful language use (Waddenet al. 2021). Others have focused on moderators themselves(Matias 2019b; Dosono and Semaan 2019), and technolog-ical tools to assist them (Jhaver et al. 2019; Zhang, Hugh,and Bernstein 2020; Chandrasekharan et al. 2019), as wellas self-moderation through voting (Glenski, Pennycuff, andWeninger 2017; Risch and Krestel 2020) and communitynorms (Fiesler et al. 2018). In contrast, our work informs theviability of different moderation strategies, speciﬁcally byexamining the sharing and visibility of news content acrossthousands of communities. 3 Dataset &amp; Validation We analyze all reddit submissions to extract links, and anno-tate links to news sources with their political bias and factu-alness using ratings from Media Bias/Fact Check. 797 Figure 1: The percentage of links that can be annotated us- ing the MBFC labels is very consistent (± 3.3%) over time, suggesting that comparisons over time are not signiﬁcantlyimpacted by changes in annotation coverage. 3.1 Reddit Content reddit is the sixth most visited website in the world, and iswidely studied due to its size, diversity of communities, andthe public availability of its content (Medvedev, Lambiotte,and Delvenne 2018). Users can submit links or text (knownas “selfposts”) to speciﬁc communities, known as “subred-dits.” Users may view submissions for a single community,or create a “front page” which aggregates submissions fromall communities the user “subscribes” to. Here, we focus onsubmissions over comments, as submissions are the primarymechanism for sharing content on reddit, and users spendmost of their time engaging with submissions (Glenski, Pen-nycuff, and Weninger 2017). To create our dataset, we downloaded all public red- dit submissions from Pushshift (Baumgartner et al. 2020)posted between January 2015 and August 2019 4, inclusive, for a total of 56 months of content (580 million submissions,35 million unique authors, 3.4 million unique subreddits).For each submission, we extract the URLs of each linked-to website, which resulted in 559 million links 5. Additional summary statistics are included in the Appendix. Ethical Considerations. We value and respect the privacy and agency of all people potentially impacted by this work. All reddit content analyzed in this study is publicly accessi-ble, and Pushshift, from which we source our reddit content,permits any user to request removal of their submissions atany time. We take speciﬁc steps to protect the privacy ofpeople included in our study (Fiesler and Proferes 2018):we do not identify speciﬁc users, and we exclusively ana-lyze data and report our results in aggregate. All analysis ofdata in this study was conducted in accordance with the In-stitutional Review Board at the University of Washingtonunder identiﬁcation number STUDY00011457. 3.2 Annotation of Links’ News Sources To identify and annotate links to news sources, we make useof Media Bias/Fact Check (hereafter MBFC), an indepen-dently run news source rating service. Bozarth, Saraf, andBudak (2020) ﬁnd that “the choice of traditional news lists[for fact checking] seems to not matter,” when comparing 5 4August 2019 was the most recent month available at the time of this study. 5While link submissions by deﬁnition contain exactly one link, text submissions (selfposts) can include 0 or more links.different news lists including MBFC. Therefore, we selectedMBFC as it offers the largest set of labels of any news sourcerating service (Bozarth, Saraf, and Budak 2020). MBFC pro-vides ratings of the political bias (left to right) and factual-ness (low to high) of news outlets around the world, alongwith additional details and justiﬁcations for ratings, using arigorous public methodology 6. MBFC is widely used for la- belling bias and factualness of news sources for downstreamanalysis (Heydari et al. 2019; Main 2018; Starbird 2017;Darwish, Magdy, and Zanouda 2017; Nelimarkka, Laakso-nen, and Semaan 2018) and as ground truth for predictiontasks (Dinkov et al. 2019; Stefanov et al. 2020). From MBFC’s public reports on each news source, we extract the name of the news source, its website, and the po-litical bias and factualness ratings. Bias is measured on a7-point scale of ‘extreme left,’ ‘left,’ ‘center left,’ ‘center,’‘center right,’ ‘right,’ and ‘extreme right,’ and is reported for2,440 news sources. Factualness is measured on a 6-pointscale of ‘very low factual,’ ‘low factual,’ ‘mixed factual,’‘mostly factual,’ ‘high factual,’ and ‘very high factual,’ andis reported for 2,676 news sources (as of April 2020). Forbrevity, in the following analyses, we occasionally use theterm ‘left leaning’ to indicate a news source with a biasrating of ‘extreme left,’ ‘left,’ or ‘center left,’ and the term‘right leaning’ to indicate a news source with a bias ratingof ‘center right,’ ‘right,’ or ‘extreme right.’ We then annotate the links extracted from reddit sub- missions with the MBFC ratings using regular expres-sions to match the URL of the link with the domain ofthe corresponding news source. For example, a link towww.rt.com/news/covid/ would be matched with the rt.comdomain of RT, the Russian-funded television network, andannotated with a bias of ‘center right’ and a factualness of‘very low.’ Links to URL shorteners such as bit.ly were ex-cluded from labeling. We ﬁnd that links to center left andhigh factual news sources are most common, accounting for53% and 64% of all news links, respectively. Extreme leftnews source links are much less common, with 22.2 extremeright links for every 1 extreme left link (Fig. 2). Validation of MBFC Annotations. The use of fact check- ing sources such as MBFC is common practice for large scale studies, and MBFC in particular is widelyused (Dinkov et al. 2019; Stefanov et al. 2020; Heydariet al. 2019; Main 2018; Starbird 2017; Darwish, Magdy,and Zanouda 2017; Nelimarkka, Laaksonen, and Semaan2018). Additional conﬁdence in MFBC annotations comesfrom the results of Bozarth, Saraf, and Budak (2020), whoﬁnd that (1) MBFC offers the largest set of biased and lowfactual news sources when compared among 5 fact check-ing datasets, and (2) the selection of a speciﬁc fact checkingsource has little impact on the evaluation of online content.Furthermore, we ﬁnd that the coverage (the percentage oflinks that can be annotated using MBFC, excluding links toobvious non-news sources such as links to elsewhere on red-dit, to shopping sites, etc.) is very consistent ( ±3.3%) over the 4 year span of our dataset (Fig. 1). Additionally, Bozarth,Saraf, and Budak (2020) ﬁnd that it is very rare for a news 6https://mediabiasfactcheck.com/methodology/ 798 Figure 2: Distributions of mean bias and factualness are quite similar for both the user and community units of anal-ysis. Grey bars show the normalized total counts of links ofeach type across all of reddit. source’s bias or factualness to change over time, suggesting that the potential ‘drift’ of ratings over time should not affectour results. Robustness Checks with Different Set of Annotations. Lastly, we use an additional fact checking dataset fromVolkova et al. (2017), consisting of 251 ‘veriﬁed’ newssources and 210 ‘suspicious’ news sources, as an additionalpoint of comparison for validation. While the exact classesin the Volkova et al. dataset are not directly comparable to MBFC, we can create a comparable class by comparinglinks with a MBFC factualness rating of ‘mostly factual’or higher with Volkova et al.’s ‘veriﬁed’ news sources. In this case, when considering links that can be annotated us-ing both datasets, MBFC and Volkova et al. have a Cohen’s kappa coefﬁcient of 0.82, indicating “almost perfect” inter-rater reliability (Landis and Koch 1977). We examined ifthese differences could have an impact of downstream anal-ysis and found this to be unlikely. For example, results com-puted separately using MBFC and Volkova et al. agree with one another with a Pearson’s correlation of 0.98 on the taskof identifying the number of ‘mostly factual’ or higher linksposted to a community. Computing Mean Bias/Factualness. As described above, MBFC labels for bias and factualness are ordinal, yet for many analyses, it is useful to have numeric labels (e.g. com- puting the variance of links in a community). To convertfrom MBFC’s categorical labels to a numeric scale, we usea mapping of (-3, -2, -1, 0, 1, 2, 3) to assign ‘extreme left’links a numeric bias value of -3, ‘left’ links a value of -2,‘center left’ links a value of -1, ‘center’ links a value of 0,and positive values to map to the equivalent categories onthe right. While this choice is somewhat arbitrary, it is con-sistent with the linear spacing between bias levels given byMBFC. Furthermore, we explored different mappings, in-cluding nonlinear ones, and found that our results are robustto different mappings. As such, we use the mapping givenabove as it is easiest to interpret. We use a similar mappingof (0, 1, 2, 3, 4, 5) to assign ‘very low factual’ links a nu-meric value of 0, ‘low factual’ links a value of 1, etc., with ‘very high factualness’ links assigned a value of 5. These numeric values are used to compute users’ and communities’ mean bias andmean factualness, central con- structs in our analyses. To do so, we simply take the averageof the numeric bias and factualness values of the links byeach user or in each community. For many of our analyses,we group users by rounding their mean bias/factualness tothe nearest integer. Thus, when we describe a user as havinga ‘left center bias,’ we are indicating that the mean bias ofthe links they submitted is between -1.5 and -0.5. The distributions of means are very similar for users and communities, with both closely following the overall distri-bution of news links on reddit, shown with grey bars (Fig. 2).74% of communities and 73% of users have a mean bias ofapproximately center left, and 65% of communities and 62%of users have a mean factualness of ‘high factual’ (amongusers/communities with more than 10 links). Similarly, we deﬁne user variance of bias as the variance of the bias values of the links submitted by a user, and sim-ilarly community variance of bias is deﬁned as the variance of of the bias values of links submitted to a community. Aswith mean bias, we ﬁnd that the distributions of user andcommunity variance of bias are very similar to one another.The median user has a variance of 0.85, approximately thevariance of a user with center bias who submits 62% centerlinks, 22% center-left or center-right links, and 16% left orright links. The median community has a variance of 0.91,approximately that of a community where 62% of the con-tent submitted has center bias, 20% of the content has center-left or center-right bias, and 18% of the content has left orright bias. Of course, a substantial amount of a community’svariance comes from the variance of its userbase. We exploresources of this variance in §4. 3.3 Estimating Potential Exposures to Content Links on reddit do not have equal impact; some links areviewed by far more people than others. To understand theimpact of certain types of content, we would like to under-stand how broadly that content is viewed. As view countsare not publicly available, we use the number of subscribersto the community that a link was posted to as an estimatefor the number of potential exposures to community mem- bers that this content may have had. While some users, es-pecially those without accounts, view content from commu-nities they are not subscribed to, subscription counts cap-ture both active contributors and passive consumers withinthe community, which motivated our use of this proxy overother alternatives, such as the number of votes. As communities are constantly growing, we deﬁne the number of potential exposures to a link as the number ofsubscribers to the community the link was posted to at the time it was posted. To estimate historic subscriber counts, wemake use of archived Wayback machine snapshots of sub-reddit about pages, which provide the number of subscribersat the time of the snapshot. These snapshots are availablefor the ⇠3,500 largest subreddits. In addition, we collected the current (as of Dec. 29, 2020) subscriber count for the 799 25,000 largest subreddits, as well as the date the subreddit was created (at which point it had 0 subscribers). We usethe present subscriber count, archived subscriber counts (ifavailable), and the creation date, and linearly interpolate be-tween these data points to create a historical estimate of thesubscriber counts over time for each of the 25,000 largest (bynumber of posts) subreddits in our dataset. The resulting setof subscriber count data, when joined with our set of redditcontent, provides potential exposure estimates for 93.8% ofsubmissions. For the remaining 6.2% of submissions, we in-tentionally, conservatively overestimate the potential expo- sures by using the ﬁrst percentile value (4 subscribers) fromour subscriber count data. The effect of this imputation onour results is very minor as these only occur in communitieswith extremely little activity. 4 Diversity of News within Communities In this section, we examine the factors that contribute toa community’s variance of bias. This variance can comefrom a combination of two sources: (1) community mem-bers who are individually ideologically diverse (user diver-sity), and (2) a diverse group of users with different meanbiases (group diversity). High user diversity corresponds toa community whose members have high user variance (e.g.users who are ideologically diverse individually), and highgroup diversity corresponds to a community with high vari-ance of its members’ mean bias (e.g. a diverse group of users who may be ideologically consistent individually). Ofcourse, these sources of variance are not mutually exclusive;overall community variance is maximized when both user diversity andgroup diversity are large. Method. This intuition can be formalized using the Law of Total Variance, which states that total community varianceis exactly the sum of User Diversity (within-user variance)and Group Diversity (between-user variance): Var(B c) = E[Var( Bc|U)] + Var(E[B c|U]) where Bcis a random variable representing the bias of a link submitted to community c, and Uis a random variable rep- resenting the user who submitted the link. We compute user diversity and group diversity for each community. User diversity is given by taking the mean ofeach user’s variance of bias, weighted by the number of la-beled bias links that user submitted. Group diversity is givenby taking the variance of each community members’ meanuser bias, again weighted by their number of labeled links.We then sum the user and group diversity values to computethe overall community variance of political bias. To understand how communities vary relative to their mean, we compute the balance of links in the adjacent rel-atively more- and less- biased categories. For example, acommunity with ‘left’ mean bias has two adjacent cate-gories: ‘extreme left’ and ‘center left,’ with ‘extreme left’being the relatively-more biased category, and ‘center left’being the relatively-less biased category. Results. Across all of reddit, we ﬁnd most (82%) commu- nities’ group diversity constitutes a majority of their over- all variance of bias. When binned by their mean bias, we Figure 3: While group diversity is similar between left- and right-leaning communities with a similar degree ofbias (right panel), right-leaning communities have higheruser diversity than equivalently biased communities on theleft (left panel). As a result, right-leaning communitieshave higher overall variance around their community mean.Right-leaning communities also favor relatively-more bi-ased links, when compared to left-leaning communities. ﬁnd that communities with extreme bias have, on average, lower total variance than communities closer to the middleof the spectrum (Fig. 3). A community with mean bias of‘extreme left’ would be expected to have a lower total vari-ance as there are no links with bias further left than ‘extremeleft.’ To control for this dynamic, we only compare symmet-ric labels: ‘extreme left’ to ‘extreme right,’ ‘left’ to ‘right,’and ‘center left’ to ‘center right.’ We ﬁnd that right- and left-leaning communities have similar group diversity (Fig. 3, right), but right-leaning com-munities (red) have 341% more user diversity than equiv-alently left-leaning communities, on average (Fig. 3, left).As a result, the average overall variance is 105% greaterfor right-leaning communities than left-leaning communi-ties. Interestingly, we ﬁnd that a larger share of right-leaningcommunities’ variance is in more biased categories, relativeto the community mean. 74% of right-leaning communities’adjacent links are relatively-more biased, compared to 55%for left-leaning communities, in other words, an increase of 35%⇣ 74% 55%⌘ . Implications. These results suggest that members of com- munities on the left and right have comparable group di- versity, indicating the range of users are equally similarto one another. However, right-leaning communities havehigher user diversity, indicating that the individual usersthemselves tend to submit links to news sources with alarger variety of political leaning. This creates higher over-all variance of political bias in right-leaning communities,however these right-leaning communities also contain morelinks with higher bias, relative to the community mean, asopposed to more relatively-neutral news sources. 800 Figure 4: Users with extreme mean bias stay on reddit less than half as long as users with center mean bias. Users withlow and very low mean factualness also leave more quickly,but expected lifespan decreases as users’ mean factualnessincreases past ‘mixed factual’. Across all ﬁgures, error barscorrespond to bootstrapped 95% conﬁdence intervals (andmay be too small to be visible). 5 Impact of Current Curation and Ampliﬁcation Behaviors The impact of content on reddit is affected by users’ behav-ior: how long they stay on the platform, how they vote, andhow they amplify. In this section, we examine user longevityand turnover, community acceptance of biased and low fac-tual content, and ampliﬁcation through crossposting. 5.1 User Lifespan Do users who post extremely biased or low factual contentstay on reddit as long as other users? Method. We compute each user’s lifespan on the platform by measuring how long they stay active on the platform after their ﬁrst submission. We deﬁne “active” as posting at leastonce every 30 days, as in Waller and Anderson (2019). Wegroup users by their mean bias and factualness, and for eachgroup, compute the expected lifespan of the group members. Results. We ﬁnd that expected lifespan is longer for users who typically submit less politically biased content, with users whose mean bias is near center remaining on reddit forapproximately twice as long as users with extreme or mod-erate mean bias, on average (Fig. 4, top). This result holdsregardless of whether users are left- or right-leaning. Userswith a mean factualness close to ‘mixed factual’ or lowerleave reddit 68% faster than users whose mean factualnessis near ‘mostly factual’ (Fig. 4, bottom). However, we alsoﬁnd that users’ expected lifespan decreases dramatically astheir mean factualness increases to ‘high’ or ‘very high’ lev-els of factualness. Implications. These results suggest that users who mostly post links to extremely biased or low factual news sources leave reddit more quickly than other users. We can onlyspeculate as to the causes of this faster turnover, but we notethat users who stay on reddit the longest tend to post links tothe types of news sources that are most prevalent (grey barsin Fig. 2 show overall prevalence of each type of link). The faster turnover suggests that users sharing this type of content leave relatively early, limiting their impact on theircommunities. However, faster turnover also may make user-level interventions such as bans less effective, as these sanc-tions have shorter-lived impact when the users they are madeagainst leave the site more quickly. Future research couldexamine why users leave, whether they rejoin with new ac-counts in violation of reddit policy, and the efﬁcacy of re-strictions of new accounts. 5.2 Acceptance of Biased or Low Factual Content How do communities respond to politically biased or lowfactual content? Method. On reddit, community members curate content in their communities by voting submissions up or down, which affects its position on the community feed (Glenski, Penny-cuff, and Weninger 2017). A submission’s ‘score’ is deﬁnedby reddit as approximately the number of upvotes minus thenumber of downvotes that post receives. The score has beenused in previous work as a proxy for a link’s reception bya community (Waller and Anderson 2019; Datta and Adar2019). Links submitted to larger communities are seen bymore users and therefore receive more votes. Therefore, wenormalize each link’s score by dividing by the mean scoreof all submissions in that community; links with a normal-ized score over 1are more accepted than average, and links with a score under 1are less accepted than average. In ac- cordance with reddit’s ranking algorithm, submissions withhigher normalized score appear higher in the feed viewedby community members, and stay in this position for longer(Medvedev, Lambiotte, and Delvenne 2018). To compute the community acceptance of links of a given bias or factualness, we average the normalized score of alllinks of that type in that community. We then take the me-dian community acceptance across all left-leaning, right-leaning, and neutral communities. Here we use the medianas it is more resilient to outliers than the mean. Results. We ﬁnd that, regardless of the community’s po- litical leaning, median expected community acceptance is 18% lower for extremely biased content than other con-tent (Fig. 5). For left-leaning and neutral communities, com-munity acceptance decreases monotonically as factualnessdrops below ‘high.’ However, we observe that right leaningcommunities are 167% (p =0.0002) more accepting of ex- treme right biased and 85% (p =0.004) more accepting of very low factual content than left-leaning and neutral com-munities (Mann–Whitney Usigniﬁcance tests). Implications. This suggests that across reddit, communities are sensitive to extremely biased and low factual content,and users’ voting behavior is fairly effective at reducing theacceptance of this content. However, curation does not seemto result in better-than-average acceptance for any content—no median acceptance values are signiﬁcantly (p&lt; 0.05) 801 Figure 5: Regardless of the political leaning of the community, extremely biased content is less accepted by communities than content closer to center. Similarly, low and very low factual content is less accepted than higher factual content. Points perturbedon the x-axis to aid readability. above 1, as non-news content tends to receive higher com- munity acceptance than news content. Previous research has found that on Twitter, news that failed fact-checking spread more quickly and was seen morewidely than news that passed a fact-check (Vosoughi, Roy,and Aral 2018). Interestingly, we ﬁnd evidence that behav-ior on reddit is somewhat different, with median left-leaning,right-leaning, and neutral communities all being less accept-ing of low and very low factual content. Importantly, ourmethodology differs from Vosoughi, Roy, and Aral (2018)in that we use bias and factualness evaluations that were ap-plied to entire news sources, as opposed to the fact check-ing of speciﬁc news articles, limiting direct comparisons.Furthermore, we do not analyze the time between an initialpost and its subsequent ampliﬁcation, and so cannot directlycomment on the ‘speed’ of ampliﬁcation. We do ﬁnd evi-dence, however, that highly biased content on reddit is lessupvoted than more neutral content. These difference may in part be explained by differences between reddit’s and Twitter’s mechanisms for impactingthe visibility of content. Whereas Twitter users are only ableto increase visibility by retweeting, liking, replying to, orquoting content, on reddit, users may downvote to decreasevisibility of content they object to. We speculate that thismay partially explain the differences in acceptance that weﬁnd between reddit and Twitter. 5.3 Selective Ampliﬁcation of News Content How does ampliﬁcation of content affect exposure to biasedand low factual content? On reddit, users are not only ableto submit links to external content (such as news sites), but users are also able to submit links to internal content else- where on reddit, effectively re-sharing and therefore ampli- fying content by increasing its visibility on the site. This is commonly known as ‘crossposting,’ and often occurs whena user submits a post from one subreddit to another subred-dit, although such re-sharing of internal content can happenwithin a single community as well. Here, we seek to under-stand the role that ampliﬁcation through crossposts has onreddit user’s exposure to various kinds of content.Method. To identify the political bias and factualness of crossposted content, we identify all crossposted links tonews sources, and propagate the label of the crosspostedlink. Then, we compute the fraction of total potential ex-posures from crossposts for each bias/factualness category. Results. We ﬁnd that ampliﬁcation via crossposting has an overall small effect on the potential exposures of news con- tent. While 10% of all news links are crossposts, only 1% ofpotential exposures to news links are due to crossposts. Thissuggests that the majority of crossposts are content posted inrelatively larger communities re-shared to relatively smallercommunities with relatively fewer subscribers, diminishingthe impact of ampliﬁcation via crossposting. As such, direct links to news sites have a far greater bearing on reddit users’exposure to news content than crossposts. However, the role of crossposts in exposing users to new content is still important, as crossposts account for morethan 750 billion potential exposures. We ﬁnd that extremelybiased and low factual content is ampliﬁed less than othercontent, as shown in Fig. 6, which illustrates the percent-age of total potential exposures that come from crosspostsfor each bias/factualness category. reddit users exposed tocenter left biased, center biased, or center right biased con-tent are 53% more likely to be exposed to this content viaampliﬁcation than reddit users exposed to extremely biasedcontent. Similarly, reddit users exposed to ‘mostly factual’or higher factualness content are 217% more likely to beexposed to such content via ampliﬁcation than reddit usersexposed to very low factual content. Implications. Given that only 1% of potential exposures are from ampliﬁcations, understanding the way that direct links to external content are shared is critical to understanding the sharing of news content on reddit more broadly. The relative lower ampliﬁcation of extremely biased and very low factual content suggests users’ sensitivity to thebias and factualness of the content they are re-sharing. Asin §5.2, this suggests differences between reddit and Twit-ter, where content that failed a fact-check has been found tospread more quickly than fact-checked content (Vosoughi,Roy, and Aral 2018). We speculate that this may be due to 802 Figure 6: Extremely biased and low factual content is ampli- ﬁed by crossposts relatively less than other content. Regard-less of the bias or factualness of the content, while cross-posts are responsible for more than 750 billion potential ex-posures, they make up only 1% of total potential exposures,suggesting that direct links to news sources play an espe-cially important role in content distribution. structural differences between the two platforms. On red- dit, users primarily consume content through subscriptionsto communities, not other users. This may explain the dimin-ished impact of re-sharing on reddit compared to Twitter. 6 Concentrations of Extremely Biased or Low Factual News Content It is critical to understand where different news content isconcentrated in order to best inform strategies for monitor-ing and managing its spread online. In this section, we ex-amine how extremely biased and low factual content is dis-tributed across users, communities, and news sources. Wealso compare the concentration of extremely biased and lowfactual content to all content. Method. We consider three types of content: (1) news con- tent with extreme bias or low factualness, (2) all news con- tent, and (3) all content (including non-news). We groupeach of these types of content by three perspectives: the userwho posted the content, the community it was posted to, andthe news source (or domain, in the case of all content) linkedto. We then take the cumulative sum of potential exposuresacross the users, communities, and news sources, to computethe fraction of potential exposures contributed by the top n% of users, communities, and news sources. We repeat this pro-cess, replacing the number of potential exposures with thetotal number of links, to consider the concentration of linksbeing submitted, regardless of visibility. Results. We ﬁnd that overall, extremely biased and low fac- tual content is highly concentrated across all three perspec- tives, but is especially concentrated in a small number ofcommunities, where 99% of potential exposures stem from amere 109 (0.5%) communities (Gini coefﬁcient=0.997) (Fig. Figure 7: When compared to all content on reddit (dotted line), extremely biased or low factual content (solid line)is more broadly distributed, making it harder to detect, re-gardless of the community, user, or news source perspective.However, 99% of potential exposures to extremely biased orlow factual content are restricted to only 0.5% of communi-ties. Here, a curve closer to the lower-right corner indicatesa more extreme concentration. Note that axis limits do notextend from 0 to 100%. 7a). No matter the perspective, exposures to extremely bi- ased or low factual content (solid line) are less concentratedthan all content (dotted line) (Fig. 7abc). Under the community and news source perspectives, ex- posures (Fig. 7ac) are more concentrated than links (Fig.7df). While links are already concentrated in a small shareof communities, some communities are especially large, andtherefore content from these communities receives a dispro-portionate share of potential exposures. This is not the casefor users, as the distributions of exposures (Fig. 7b) are lessconcentrated than the distributions of links (Fig. 7e). Thisindicates that while some users submit a disproportionateshare of links, these are not the users whose links receive thelargest potential exposure, as potential exposure is primarilya function of submitting links to large communities. Implications. The extreme concentration of extremely bi- ased or low factual content amongst a tiny fraction of com- munities supports reddit’s recent and high proﬁle decision totake sanctions against entire communities, not just speciﬁcusers (Isaac and Conger 2021). These decisions have beenextensively studied (Chandrasekharan et al. 2017, 2020;Thomas et al. 2021; Saleem and Ruths 2018; Ribeiro et al.2020). While this content is relatively less concentratedamongst users, in absolute terms, this content is still fairlyconcentrated, with 10% of users contributing 84% of po-tential exposures. As such, moderation sanctions againstusers can still be effective (Matias 2019b). We note thatthe concentration of extremely biased or low factual contentamongst a small fraction of users is similar to what has been 803 found on Twitter (Grinberg et al. 2019), although method- ological differences preclude a direct comparison. 7 Discussion Summary &amp; Implications. In this work, we analyze all 580 million submissions to reddit from 2015-2019, and annotate35 million links to news sources with their political bias andfactualness using Media Bias/Fact Check. We ﬁnd: • Right-leaning communities’ links to news sources have 105% greater variance in their political bias than left- leaning communities. When right-leaning communitieslink to news sources that are different than the commu-nity average, they link to relatively-more biased sources35% more often than left-leaning communities (§4). • Existing curation and ampliﬁcation behaviors moderately reduce the impact of highly biased and low factual con-tent. This suggests that reddit differs somewhat from Twit-ter, perhaps due to its explicit community structure, or theability for users to downvote content (§5). • Highly biased and low factual content tends to be shared by a broader set of users and in a broader set of commu-nities than news content as a whole. Furthermore, the dis-tribution of this content is more concentrated in a smallnumber of communities than a small number of users, as99% of exposures to extremely biased or low factual con-tent stem from only 0.5% or 109 communities (§6). Thislends credence to recent reddit interventions at the com-munity level, including bans and quarantines. Limitations. One limitation of our analyses is the use of a single news source rating service, MBFC. However, the se-lection of news source rating annotation sets has been foundto have a minimal impact on research results (Bozarth, Saraf,and Budak 2020). MBFC is the largest (that we know of)dataset of news sources’ bias and factualness, and is widelyused (Dinkov et al. 2019; Stefanov et al. 2020; Heydari et al.2019; Starbird 2017; Darwish, Magdy, and Zanouda 2017).More robust approaches could combine annotations frommultiple sources, and we ﬁnd that MBFC annotations agreewith the Volkova et al. (2017) dataset with a Pearson Corre-lation of 0.96 on an example downstream task (§3.2). Our focus is on the bias and factualness of news sources shared online. We do not consider factors such as the contentof links (e.g. shared images, speciﬁc details of news stories), or the context in which links are shared (e.g. sentiment of a submission’s comments). These factors are important areasfor future work, and are outside the scope of this paper. While MBFC (and by extension, our annotations) in- cludes news sources from around the world, our analyses,especially the left-right political spectrum and associatedcolors, takes a US-centric approach. Polarization and misin-formation are challenges across the globe (Newman 2020),and more work is needed on other cultural contexts. Our paper explores the impact of curation and ampliﬁca- tion practices, but not the impact of community moderatorswho are a critical component of reddit’s moderation pipeline(Matias 2019b). Future work could examine news contentremoved by moderators.Bias # of Links # of News Sources Extreme Left 15,157 51 Left 3,023,382 364 Center Left 17,648,711 544 Center 4,494,687 442 Center Right 4,254,705 263 Right 3,226,828 352 Extreme Right 997,703 423 Unlabeled 525,443,378 Table 1: Numbers of links and unique news sources in ourdataset, by the political bias of the link. Finally, we are limited by the unavailability of data on which users view what content. While we use subreddits’subscriber counts to estimate exposures to content, moregranular data would enable us to better understand the im-pact of shared news articles, for example, the percentage ofusers who are exposed to extremely biased or low factualcontent (Grinberg et al. 2019). 8 Conclusion Biased and inaccurate news shared online are signiﬁcantproblems, with real harms across our society. Large-scalestudies of news sharing online are critical for understand-ing the scale and dynamics of these problems. We presentedthe largest study to date of news sharing behavior on red-dit, and found that right-leaning communities have more po-litically varied and relatively-more biased links than left-leaning communities, current voting and re-sharing behav-iors are moderately effective at reducing the impact of ex-tremely biased and low factual content, and that such con-tent is extremely concentrated in a small number of commu-nities. We make our dataset of news sharing on reddit public,in order to support further research 7. Acknowledgements This research was supported by the Laboratory Directed Re-search and Development Program at Paciﬁc Northwest Na-tional Laboratory, a multiprogram national laboratory oper-ated by Battelle for the U.S. Department of Energy. Thisresearch was supported by the Ofﬁce for Naval Research,NSF grant IIS-1901386, the Bill &amp; Melinda Gates Foun-dation (INV-004841), and a Microsoft AI for Accessibilitygrant. Appendix: Dataset Summary Our dataset was created from all public reddit submissionsposted between January 2016 and August 2019, the mostrecent data available at the time of this study. These submis-sions were downloaded using the Pushshift archives (Baum-gartner et al. 2020), and consist of 580 million submissions,35 million unique authors, and 3.4 million unique subred-dits. As each submission may consist of 0 or more links, thedataset includes a total of 559 million links. These links are 7https://behavioral-data.github.io/news labeling reddit/ 804 Factualness # of Links # of News Sources Very Low 609,229 72 Low 749,202 369 Mixed 7,116,130 677 Mostly 2,217,719 110 High 22,055,943 1,313 Very High 2,263,604 134 Unlabeled 524,092,724 Table 2: Numbers of links and unique news sources in our dataset, by the factualness of the link. to 5.1 million unique domains, of which we are able to label 2,801 unique domains with annotations from MBFC. Table 1 shows the number of links in the dataset, as well as the number of unique news sources, for each bias category. Table 2 shows the number of links in the dataset, as well as the number of unique news sources, for each factualnesscategory. The dataset may be downloaded from our website at https://behavioral-data.github.io/news labeling reddit/ References Allen, J.; Howland, B.; Mobius, M.; Rothschild, D.; andWatts, D. J. 2020. Evaluating the fake news problem at thescale of the information ecosystem. Science Advances 6(14). Allison, K.; and Bussey, K. 2020. Communal Quirks andCirclejerks: A Taxonomy of Processes Contributing to Insu-larity in Online Communities. In ICWSM. Baly, R.; Da San Martino, G.; Glass, J.; and Nakov, P. 2020.We Can Detect Your Bias: Predicting the Political Ideologyof News Articles. In EMNLP, 4982–4991. ACL. doi:10. 18653/v1/2020.emnlp-main.404. URL https://www.aclweb.org/anthology/2020.emnlp-main.404. Baumgartner, J.; Zannettou, S.; Keegan, B.; Squire, M.; and Blackburn, J. 2020. The Pushshift Reddit Dataset. InICWSM. Bovet, A.; and Makse, H. 2019. Inﬂuence of fake news in Twitter during the 2016 US presidential election. Nature Communications 10. Bozarth, L.; Saraf, A.; and Budak, C. 2020. Higher Ground?How Groundtruth Labeling Impacts Our Understanding ofFake News about the 2016 U.S. Presidential Nominees. InICWSM. Chandrasekharan, E.; Gandhi, C.; Mustelier, M. W.; and Gilbert, E. 2019. Crossmod: A Cross-Community Learning-based System to Assist Reddit Moderators. CHI 3: 1 – 30. Chandrasekharan, E.; Jhaver, S.; Bruckman, A.; andGilbert, E. 2020. Quarantined! Examining the Effectsof a Community-Wide Moderation Intervention on Reddit.ArXiv abs/2009.11483. Chandrasekharan, E.; Pavalanathan, U.; Srinivasan, A.;Glynn, A.; Eisenstein, J.; and Gilbert, E. 2017. You Can’tStay Here: The Efﬁcacy of Reddit’s 2015 Ban ExaminedThrough Hate Speech. CHI 1: 31:1–31:22.Chandrasekharan, E.; Samory, M.; Jhaver, S.; Charvat, H.;Bruckman, A.; Lampe, C.; Eisenstein, J.; and Gilbert, E.2018. The Internet’s Hidden Rules. CHI 2: 1 – 25. Choi, D.; Chun, S.; Oh, H.; Han, J.; et al. 2020. Rumorpropagation is ampliﬁed by echo chambers in social media.Scientiﬁc Reports 10(1): 1–10. Darwish, K.; Magdy, W.; and Zanouda, T. 2017. Trump vs.Hillary: What Went Viral During the 2016 US PresidentialElection. In SocInfo. Datta, S.; and Adar, E. 2019. Extracting Inter-communityConﬂicts in Reddit. In ICWSM. Demszky, D.; Garg, N.; Voigt, R.; Zou, J.; Gentzkow, M.;Shapiro, J.; and Jurafsky, D. 2019. Analyzing Polarizationin Social Media: Method and Application to Tweets on 21Mass Shootings. In NAACL-HLT. Dinkov, Y.; Ali, A.; Koychev, I.; and Nakov, P. 2019. Pre-dicting the Leading Political Ideology of YouTube ChannelsUsing Acoustic, Textual, and Metadata Information. In IN- TERSPEECH. Dosono, B.; and Semaan, B. C. 2019. Moderation Practices as Emotional Labor in Sustaining Online Communities: TheCase of AAPI Identity Work on Reddit. CHI . Ferrara, E. 2017. Contagion dynamics of extremist propa-ganda in social networks. Information Sciences 418: 1–12. Fiesler, C.; Jiang, J. A.; McCann, J.; Frye, K.; and Brubaker,J. R. 2018. Reddit Rules! Characterizing an Ecosystem ofGovernance. In ICWSM. Fiesler, C.; and Proferes, N. 2018. “Participant” Perceptionsof Twitter Research Ethics. Social Media + Society 4. Ganguly, S.; Kulshrestha, J.; An, J.; and Kwak, H. 2020.Empirical Evaluation of Three Common Assumptions inBuilding Political Media Bias Datasets. In ICWSM. Geiger, A. 2019. Key ﬁndings about the online newslandscape in America. https://www.pewresearch.org/fact-tank/2019/09/11/key-ﬁndings-about-the-online-news-landscape-in-america/. Accessed: 2021-04-09. Glenski, M.; Pennycuff, C.; and Weninger, T. 2017. Con- sumers and Curators: Browsing and Voting Patterns on Red-dit. IEEE TCSS 4(4): 196–206. doi:10.1109/TCSS.2017. 2742242. Glenski, M.; Weninger, T.; and Volkova, S. 2018. Propa- gation from deceptive news sources who shares, how much,how evenly, and how quickly? IEEE TCSS 5(4): 1071–1082. Grinberg, N.; Joseph, K.; Friedland, L.; Swire-Thompson,B.; and Lazer, D. 2019. Fake news on Twitter during the2016 U.S. presidential election. Science 363: 374 – 378. Heydari, A.; Zhang, J.; Appel, S.; Wu, X.; and Ranade, G.2019. YouTube Chatter: Understanding Online CommentsDiscourse on Misinformative and Political YouTube Videos. Horne, B.; Nørregaard, J.; and Adali, S. 2019. Different Spi- rals of Sameness: A Study of Content Sharing in Mainstreamand Alternative Media. In ICWSM. 805 Isaac, M.; and Conger, K. 2021. Reddit bans forum dedicated to supporting Trump. https://www.nytimes.com/2021/01/08/us/politics/reddit-bans-forum-dedicated-to-supporting-trump-and-twitter-permanently-suspends-his-allies-who-spread-conspiracy-theories.html. Accessed:2021-04-09. Jhaver, S.; Birman, I.; Gilbert, E.; and Bruckman, A. 2019. Human-Machine Collaboration for Content Regula-tion. TOCHI 26: 1 – 35. Jhaver, S.; Bruckman, A.; and Gilbert, E. 2019. Does Trans-parency in Moderation Really Matter? CHI 3: 1 – 27. Jiang, S.; Robertson, R. E.; and Wilson, C. 2019. Bias Mis-perceived: The Role of Partisanship and Misinformation inYouTube Comment Moderation. In ICWSM. Jiang, S.; Robertson, R. E.; and Wilson, C. 2020. Reasoningabout Political Bias in Content Moderation. In AAAI. Kharratzadeh, M.; and ¨Ustebay, D. 2017. US Presidential Election: What Engaged People on Facebook. In ICWSM. Kouzy, R.; Jaoude, J. A.; Kraitem, A.; Alam, M. B. E.;Karam, B.; Adib, E.; Zarka, J.; Traboulsi, C.; Akl, E. A.; andBaddour, K. 2020. Coronavirus Goes Viral: Quantifying theCOVID-19 Misinformation Epidemic on Twitter. Cureus 12. Landis, J.; and Koch, G. 1977. The measurement of observeragreement for categorical data. Biometrics 33 1: 159–74. Linvill, D. L.; and Warren, P. L. 2020. Troll factories: Man-ufacturing specialized disinformation on Twitter. Political Communication 1–21. Main, T. J. 2018. The Rise of the Alt-Right. Brookings Insti- tution Press. Marwick, A.; and Lewis, R. 2017. Media manipulation and disinformation online. Data &amp; Society https://datasociety. net/library/media-manipulation-and-disinfo-online/. Matias, J. 2019a. Preventing harassment and increasing group participation through social norms in 2,190 online sci-ence discussions. PNAS 116: 9785 – 9789. Matias, J. N. 2019b. The Civic Labor of Volun-teer Moderators Online. Social Media + Society 5(2): 2056305119836778. doi:10.1177/2056305119836778.URL https://doi.org/10.1177/2056305119836778. Medvedev, A.; Lambiotte, R.; and Delvenne, J. 2018. The anatomy of Reddit: An overview of academic research.ArXiv abs/1810.10881. Mitchell, A.; Gottfried, J.; Srocking, G.; Walker, M.;and Fedeli, S. 2019. Many Americans Say Made-Up News Is a Critical Problem That Needs To BeFixed. Pew Research Center Science and Journalism https://www.journalism.org/2019/06/05/many-americans-say-made-up-news-is-a-critical-problem-that-needs-to-be-ﬁxed/. Nelimarkka, M.; Laaksonen, S.-M.; and Semaan, B. 2018. Social Media Is Polarized, Social Media Is Polarized: To-wards a New Design Agenda for Mitigating Polarization.InACM DIS, 957–970. United States: ACM. doi:10.1145/3196709.3196764. ACM conference on Designing Interac-tive Systems, DIS ; Conference date: 09-06-2018 Through13-06-2018. Newman, N. 2020. Digital News Report. Reuters Institute. Qazvinian, V.; Rosengren, E.; Radev, D.; and Mei, Q. 2011. Rumor has it: Identifying misinformation in microblogs. InEMNLP, 1589–1599. Rajadesingan, A.; Resnick, P.; and Budak, C. 2020. Quick, Community-Speciﬁc Learning: How Distinctive ToxicityNorms Are Maintained in Political Subreddits. In ICWSM. Recuero, R.; Soares, F. B.; and Gruzd, A. A. 2020. Hy-perpartisanship, Disinformation and Political Conversationson Twitter: The Brazilian Presidential Election of 2018. InICWSM. Ribeiro, M. H.; Jhaver, S.; Zannettou, S.; Blackburn, J.; Cristofaro, E. D.; Stringhini, G.; and West, R. 2020.Does Platform Migration Compromise Content Modera-tion? Evidence from r/The Donald and r/Incels. ArXiv abs/2010.10397. Risch, J.; and Krestel, R. 2020. Top Comment or Flop Com- ment? Predicting and Explaining User Engagement in On-line News Discussions. In ICWSM. Saleem, H. M.; and Ruths, D. 2018. The Aftermathof Disbanding an Online Hateful Community. ArXiv abs/1804.07354. Samory, M.; Abnousi, V. K.; and Mitra, T. 2020a. Char- acterizing the Social Media News Sphere through User Co-Sharing Practices. In ICWSM, volume 14, 602–613. Samory, M.; Abnousi, V. K.; and Mitra, T. 2020b. Char-acterizing the Social Media News Sphere through User Co-Sharing Practices. In ICWSM. Shearer, E.; and Matsa, K. E. 2018. News Use Across SocialMedia Platforms 2018. https://www.journalism.org/2018/09/10/news-use-across-social-media-platforms-2018/. Ac-cessed: 2021-04-09. Starbird, K. 2017. Examining the Alternative Media Ecosys- tem Through the Production of Alternative Narratives ofMass Shooting Events on Twitter. In ICWSM. Stefanov, P.; Darwish, K.; Atanasov, A.; and Nakov, P. 2020.Predicting the Topical Stance and Political Leaning of Me-dia using Tweets. In ACL, 527–537. Online: ACL. doi: 10.18653/v1/2020.acl-main.50. URL https://www.aclweb.org/anthology/2020.acl-main.50. Tasnim, S.; Hossain, M.; and Mazumder, H. 2020. Impact of Rumors and Misinformation on COVID-19 in Social Media.Journal of Preventive Medicine and Public Health 53: 171 – 174. Thomas, P. B.; Riehm, D.; Glenski, M.; and Weninger, T. 2021. Behavior Change in Response to Subreddit Bans andExternal Events. IEEE Transactions on Computational So- cial Systems 1–10. doi:10.1109/TCSS.2021.3061957. Volkova, S.; Shaffer, K.; Jang, J. Y.; and Hodas, N. 2017.Separating Facts from Fiction: Linguistic Models to Classify 806 Suspicious and Trusted News Posts on Twitter. In ACL, 647– 653. Vancouver, Canada: ACL. doi:10.18653/v1/P17-2102. URL https://www.aclweb.org/anthology/P17-2102. Vosoughi, S.; Mohsenvand, M. N.; and Roy, D. 2017. Rumor gauge: Predicting the veracity of rumors on Twitter. KDD 11(4): 1–36. Vosoughi, S.; Roy, D.; and Aral, S. 2018. The spread of true and false news online. Science 359(6380): 1146–1151. ISSN 0036-8075. doi:10.1126/science.aap9559. URL https://science.sciencemag.org/content/359/6380/1146. Wadden, D.; August, T.; Li, Q.; and Althoff, T. 2021. The Effect of Moderation on Online Mental Health Conversa-tions. In ICWSM. Waller, I.; and Anderson, A. 2019. Generalists and Special-ists: Using Community Embeddings to Quantify ActivityDiversity in Online Platforms. In The World Wide Web Con- ference, WWW ’19, 1954–1964. New York, NY, USA: As-sociation for Computing Machinery. ISBN 9781450366748.doi:10.1145/3308558.3313729. URL https://doi.org/10.1145/3308558.3313729. Zhang, A. X.; Hugh, G.; and Bernstein, M. S. 2020. Poli- cyKit: Building Governance in Online Communities. UIST URL https://doi.org/10.1145/3379337.3415858. 807</td>
    </tr>
    <tr>
      <th>29</th>
      <td>poli</td>
      <td>Political Bias Analysis Arkajyoti Misra Target Corporation Stanford University arkajyot@stanford.eduSanjib Basak Digital River Inc. Stanford University sbasak@stanford.edu Abstract The two major political parties in US are polarized between either the liberal and conservative point of view on a multitude of socio-economical and environmental issues. An algorithmic approach towards detection of such bias is both intellectu- ally challenging and useful in areas like election prediction. There exists very few studies in the literature where modern deep learning techniques are applied to de- tect the personal opinion or bias of an individual. In this work, we have developed an LSTM network that achieved an F1 score of 0.718 on a data set consisting of statements made in the recent past by US election candidates. 1 Introduction The political atmosphere in the US is deeply polarized between two predominant ideologies: liberal and conservative. The twp major parties maintain differences in opinion in different issues like global warming, gay rights, abortions, foreign policies and immigration, to name a few. While the liberals encourage active role for government in society and believe in environmental regulations, conservatives like to have a limited role for government in the society and argue against imposing environmental regulations. In the age of big data, with the help modern technology, this information is being captured in both structured and unstructured form at an unprecedented scale. Particularly, the year of 2016 being a US presidential election year, there is no shortage of active and engaging political discussions in the media fed by the plethora of public debates, interviews and speeches. The sudden eruption of activity in the area of opinion mining and sentiment analysis, which deals with the computational treatment of opinion, sentiment, and subjectivity in text, has generated increased interest in building new applications that deal directly with opinion [1]. The agenda of the political parties are obviously biased over the different issues, but the bias in the different forms of public communication like news media, journals, news channels are at times quite difﬁcult to comprehend from a cursory look. In many cases, it is certainly challenging, if not impossible, to manually decipher every bit of such information available in the public domain that can be quite useful in predicting the outcome of an election, for example. Our hypothesis is that a sophisticated deep learning algorithm can be trained to detect such bias auto- matically. Natural Language Processing (NLP) and Information Retrieval are becoming increasingly popular in detecting private states such as opinions, sentiment, and beliefs [2], [3] from text. How- ever, the task of detecting political bias is quite non trivial and provided a unique challenge to the NLP community over years. The goal of our project is to build an analyzer capable of detecting political bias from a given body of text. Previous work on bias or ideology detection from a textual document is relatively rare in literature. Most prior work on the topic are based on a bag-of-words representations of the document together with a multitude of hand crafted rule based on deep understanding of the structure of the language. For example, Gerrish and Blei [4] predicted the voting patterns of Congress members based on 1 bag-of-words representations of bills and inferred political leanings of those members; whereas Gentzkow and Shapiro [5] derived a slant index to rate the ideological leaning of newspapers. With the recent success of deep neural network models in the ﬁeld of natural language processing, researchers have started to apply the state of the art models in the ﬁeld of abstract theme or opinion detection [6] and achieved higher accuracy (an F1 score of close to 70.) Models like Recurrent neural network deploy deep architecture with multiple hidden layers and perform well for sequential prediction of tasks. Within the NLP ﬁeld, sentences are viewed as constructed sequence of tokens that need to carry forward the meaning from the beginning of the sentences to the end. With this view, those models have been successfully applied to tasks such as language modeling [7], and spoken language understanding [8] Recursive neural networks is another class of deep neural net architecture within that has been applied to parsing [9], sentence-level sentiment analysis [10], and paraphrase detection [11]. The major inspiration of the present work comes from the recent work by Iyyer et al [12] where the authors applied a Recursive Neural Network (RvNN) based on the Stanford tree parser and achieved a 69% accuracy on a binomial classiﬁcation of the two prominent political ideologies in the US. 2 Data The data set we used for the project is called the Ideological Books Corpus (IBC), which was com- piled by a group of researchers from the University of Maryland [12]. The data set was based on publicly available US congressional ﬂoor debate transcripts from 2005 [13]. The ﬁnal data set [14] was a compilation of sentences hand picked to be most expressive of political sentiment and man- ually labeled by a majority voting scheme. We are using the entire data set for this project that consists of 2025 liberal and 1701 conservative biased sentences. A typical example of a conservative biased sentence is ”Even he would have been in complete shock over Al Gore ’s ability to scare billions of people into believing humans are killing themselves and suffocating all life on the planet by driving their carbon-emitting cars back and forth to work instead of riding bicycles .” The example highlights the difﬁculty of the task at hand. It would be easy for a real human to categorize the sentence as anti-liberal based on prior knowledge of Al Gore’s political position or that carbon emitting cars are bad for the environment is a predominantly progressive point of view. However, any classiﬁer that needs to effectively identify the political bias of an example like this must have (a) the ability to capture long distance correlation between words and (b) knowledge of proper context. A typical liberal biased example - ”Though the practice can be defended as generating revenue used to improve the college ’s academic program , the effect is to favor middle-class and less needy applicants .” - also supports the fact that political bias may not be explicitly expressed through one or many words and knowledge of the general perception about the middle class of the major US political institutions is of paramount importance in classifying the sentence correctly. We also collected data from ontheissues [15] (OTI) that collects political speeches, dialogues and debate transcripts, from candidates of both the major parties, on a multitude of issues. A majority of the content of the data set being statements from the political candidates on an election campaign, the texts consist of generally smaller sentences and are more expressive of the bias of the speaker compared to the previous data set. For each of the issues we created two documents - one with liberal views and another with conservative views. We did not assign a label to a text based on the political afﬁliation of the speaker; instead we carefully labeled them based on the actual bias expressed in the text. This is a departure from the IBC data set where the text was labeled based on the political afﬁliation of the speaker. In the OTI data set, a typical conservative biased example on the issue of gun control is: ’Restrictive gun laws don’t stop culture of violence’; while a liberal biased statement on environment is: ’Adopt the Clean Cars Act to ﬁght global warming’. 2 Figure 1: A rolled out RNN network used in binary classiﬁcation. 3 Model Recurrent neural networks (RNN) have completely revolutionized the world of Natural Language Processing (NLP) in the last few years. In extremely difﬁcult NLP challenges like machine trans- lation, question answering and text generation to name a few, RNN have recently outperformed the conventional techniques based on a deep understanding of the structure of the language. Conven- tional RNNs are generative model where the network is built to always predict the next word in a sequence, thereby learning the semantic structure of the language. For the task at hand, we are using the network as a classiﬁer where the network is tasked with predicting the label at the end of a sequence, at shown in Fig. 1. The task at hand demands that the model capture long range correlation between words in the text. That is because the data loss does not get contribution from every word in the network and is calculated only at the very last time step and the gradient of the loss has to backpropagate to the beginning of the sentence. However, conventional RNNs stuggle to back propagate the gradient over many time steps because of the well known problems of vanishing or exploding gradients [16]. More recently, RNN variants like GRU and LSTM have solved the problem and therefore, an LSTM network was chosen for the task at hand. An LSTM unit is described by the set of activations in Eqs. (1), where htis the hidden state at time stept;i,fandostand for the input, forget and the output gate respectively. The network learns the variousWandUmatrices and ctcontrols how much of the past information has to be retained in the network. The symbol denotes the Hadamard product. i=(xtUi+ht 1Wi) f=(xtUf+ht 1Wf) o=(xtUo+ht 1Wo) g= tanh(xtUg+ht 1Wg) o=(xtUo+ht 1Wo) ct=ct 1f+gi st= tanh(ct)o (1) The binary cross entropy loss is calculated at the last time step ( tend) of the network as follows: lossCE(y;^y) = X iyilog( ^yi); whereyis the true label and the predicted label is calculated using a sigmoid function applied on the hidden state at tend: ^y=(htendU+b) (2) 4 Results and Discussion In a binary classiﬁcation framework, both the ROC-AUC number and the F1-score are widely ac- cepted to be robust measures. The AUC is the Area Under the Curve of a plot of the true positive rate (TPR) vs the false positive rate (FPR). The metric varies between 0.5 for a completely random prediction and 1 for a perfect prediction. For data sets having highly imbalanced classes present in 3 it, the AUC is a good measure as it properly weights the correct prediction of the minority class too. The F1-score, on the other hand, is deﬁned as the geometric mean of the precision and recall [17] F1 =2precision recall precision +recall; (3) where precision is the fraction of retrieved instances that are relevant, while recall is the fraction of relevant instances that are retrieved [18]. The F1 score varies between 0 and 1 and the standard goal in a classiﬁcation project is to maximize the score. Figure 2: Performance of the Naive Bayes model measured by both F1 and AUC metrics on both the data sets used in the study. In our experiment we shall focus on the F1 score as it is most common in the literature but will also track the AUC score as a guide against the model predicting completely random outcomes. Although the F1 score is more frequently reported in classiﬁcation tasks similar to the present one, the AUC actually looks at all possible prediction thresholds instead of the best possible one. The IBC data set being so challenging, we felt it is important to track both metrics for higher conﬁdence in the prediction. The previous work [12] on the IBC data set consistently used accuracy as the measure of their model prediction. However, we have experimented with another data set (OTI) that tends to have a larger imbalance between the labels on some topics. So we decided to focus on the F1 and AUC metrics instead. A baseline was established for both sets of data using a Naive Bayes bag of words model that works on the frequency of the unigram and bigrams present in the text. Fig. 2 shows the performance of both the IBC and the OTI data set in terms of F1 and AUC. The basic model performed very poorly with the F1 metric on the IBC data set. Both the metrics perform in a similar fashion for the OTI data set. The F1 score on the IBC data sets a very low baseline clearly illustrating the challenges with the content of the data set. We used a single layer LSTM model for our classiﬁcation task, where each hidden unit in ﬁg. 1 is replaced by an LSTM unit governed by the set of activations described in eqn. 1. The binary cross entropy loss described in eqn. 2 was minimized by ’Adam’ optimizer. We let the optimizer run for an unlimited number of epochs and used a 10 step early stopping scheme to terminate each individual model run. We kept our batch size ﬁxed at 32 for all our experiments. We stuck to a ﬁve fold cross validation scheme for all our experiments, where in each fold the model was trained on 80% of the data and a validation score was calculated on the remaining 20%. Exploratory runs with the IBC data revealed that for learning rates larger than 0.001 the model ﬁts nicely to the training data, while the validation loss kept increasing slowly over hundreds of epochs. We believe the reason for the model’s failure to decrease the validation loss lies in the very nature 4 Figure 3: Model performance on the IBC Dataset: Variation of F1 and AUC measures as a function of the dropout rate for a range of hidden layer sizes. of the data set as in every run the training data looks quite different from that of the validation data. The poor F1 score of the bag of words model shown in Fig. 2 supports this claim. Fig. 3 shows the performance of the model on the IBC data set. We were careful not to overﬁt the training data and hence kept the size of the hidden layer at a maximum of 80. The dropout rate had to be pushed to quite extreme limits for the model to start ﬁtting. Again, the extreme low values of dropout, namely 0.1, where the model performs best on both the F1 and AUC metrics suggests the extreme risk of overﬁtting as discussed before. However, such a large amount of regularization leads to a poor model that cannot even ﬁt the training data well. The best F1 score of 0.568 was obtained by a model with a hidden size of 40 and a dropout rate of 0.05. However poor the F1 scores appear for the model trained on the IBC data set, it must be noted that this is not unprecedented. Using a combination of bi-directional RNN and a bidirectional RvNN Isroy et al [19] obtained a best F1 score in the range of 0.5-0.6 on an opinion extraction task. Figure 4: Model performance on the OTI Dataset: Variation of F1 and AUC measures as a function of the dropout rate for a range of hidden layer sizes. Nonetheless, the results presented in ﬁg. 3 does not show beyond any reasonable doubts, that a state of the art LSTM network is up to the task of capturing implicit meaning of a text. As mentioned before, we strongly believe it is not a shortcoming of the model, rather a combination of complexity and lack of volume of the data led to the results. It should be noted that Iyyer et al [12] extracted better performance from the data set because it was manually labeled to the node level of the parse tree associated with every sentence that was used in building their RvNN model. We argue that this method is not scalable to newer data sets and we wanted to test further on our hypothesis that an LSTM network should be able to extract hidden meaning of a text with just an overall label for the text. To test out hypothesis, we collected and manually cleaned data publicly available on the web to create the OTI dataset. The advantage of the OTI data set over the IBC data set was that almost all the sentences showed a bias that could be detected by a normal human, thereby reducing the 5 possibility of ’confusing’ the model. Moreover, the average length of a sentence was 11 in the OTI data set, compared to 37 for the IBC data set, which made the training the model a lot easier. The ﬁrst indication of learning by the model became obvious when we did not have to resort un- reasonably strong regularization (very low dropout number). Fig. 4 shows a fundamental change in behavior of the inﬂuence of dropout on the F1 score. The best performing models had a dropout of 0.9 beyond which overﬁtting started and the score dropped rather dramatically. There was not a big inﬂuence coming from the hidden layer size of the network, another clear sign that the model was truly learning from the data without a need of becoming too complex. The AUC score, on the other hand, showed a nearly opposite trend. The model could obtain the best AUC value of 0.642 at a dropout of 0.3. It must be reiterated that the two metrics do not capture the same sensitivity from a model and we kept the AUC as an extra metric to look at. Figure 5: Effect of learning rate of the Adam optimizer as a function of dropout rate for the minimum and maximum hidden layer sizes used in the study. We also looked at the sensitivity of our result as a function of the learning rate in the model. Fig. 5 shows that the peak performance does not suffer from doubling the learning rate from 0.0002 to 0.0004, but a smaller hidden layer size with a faster learning rate lead to poor performance. The RNN and its variants can also be used in a bi-directional network. Meaning of a particular word in context does not always depend on the preceding words, sometimes the meaning of a word gets fully expressed from trailing words. Bi-directional networks try to capture this effect. We experimented with a bi-directional network where we concatenated the the hidden layers from the forward and backward direction. Fig. 6 shows that the network did not learn any extra information from the text. The bi-directional model performed nearly at par with its regular counterpart and the best bi-directional model also had a dropout rate of 0.9 and a hidden layer size of 80. 5 Conclusions In this work we have showed how a state of the art model like LSTM can be used to predict the implicit political bias present in a text even if there are no speciﬁc words present in the text that obviously relates to one of the two major political ideologies. We have explored two data sets with very different content - one is extracted from speeches of US congressional ﬂoor debates while the other is a collection of statements on multiple socio-political issues by US presidential candidates in most recent history. The model performed poorly on the former data set because of two main reasons: There is very little overlap of contents in the data that led to severe training challenges but more importantly a good portion of the data did not actually show any clear political bias that can be detected by a normal human being. We argue that the work by Iyyer et al [12] on the same data set performed better, although it is not a straight comparison because we used the F1 score instead of accuracy, with a RvNN because it was manually labeled at the node level of the parse tree, none of which we used in our model. Our goal was to build a model that is robust enough so that it can be applied to a variety of different contexts without the need of any detailed manual labeling, which is always difﬁcult to obtain. 6 Figure 6: A bi-directional LSTM network performs very similarly as that of a uni-directional net- work on the OTI data set. To test our hypothesis, we created a curated data set based on publicly available statements made by political candidates. We were able to obtain an F1 score of 0.718 with a single layer uni-directional LSTM network. This is at par with the best results on bias or opinion detection available in the literature [19]. We want to emphasize the fact that a model performs well only when there exists useful information present in the data. We would also like to point out that the method we developed is general in nature in the sense that the same model can be applied to extract bias or opinion in a different ﬁeld of study without any major modiﬁcation. The LSTM model we developed works only marginally better than a bag of words based approach on the OTI data set. It is already extensively reported in the literature that deep learning based models need substantially larger amount of data to outperform the traditional methods of the domain. We, therefore, strongly believe the model will outperform the bag of words model substantially if we have more training data at our disposal. References [1] URL:http://www.cs.cornell.edu/home/llee/omsa/omsa.pdf . [2] J. Wiebe T. Wilson and P. Hoffmann. “Recognizing contextual polarity in phrase-level senti- ment analysis”. In: (2005). [3] B. Pang and L. Lee. “Opinion mining and sentiment analysis. Foundations and trends in information retrieval”. In: (2008). [4] Sean Gerrish and David Blei. “ Predicting legislative roll calls from text”. In: 28th Interna- tional Conference on Machine Learning ().URL:http://www.cs.columbia.edu/ ˜blei/papers/GerrishBlei2011.pdf . [5] Matthew Gentzkow and Jesse M Shapiro. “What drives media slant? evidence from us daily newspapers”. In: Econometrica 78(1) (2010), pp. 35–71. [6] URL:http://www.cs.cornell.edu/ ˜oirsoy/files/emnlp14drnt.pdf . [7] Tomas Mikolov et al. “Extensions of recurrent neural network language model. In Acoustics, Speech and Signal Processing (ICASSP)”. In: 2011 IEEE International Conference (2011), 55285531. [8] Yoshua Bengio Deng. “Investigation of recurrent- neural-network architectures and learning methods for spoken language understanding”. In: Interspeech (2013). [9] Cliff C and Lin, Andrew Ng, and Chris Manning. “Parsing natural scenes and natural language with recursive neural networks”. In: Proceedings of the 28th International Conference on Machine Learning (ICML-11) (2011), 129136. [10] Richard Socher et al. “Semi-supervised recursive autoencoders for predicting sentiment dis- tributions”. In: In Proceedings of the Conference on Empirical Methods in Natural Language Processing (2011), 151161. 7 [11] Richard Socher et al. “Dynamic pooling and unfolding recursive autoencoders for paraphrase detection. In Advances in Neural Information Processing Systems”. In: Proceedings of the Conference on Empirical Methods in Natural Language Processing (2011), 801809. [12] Mohit Iyyer et al. “Political Ideology Detection Using Recursive Neural Networks”. In: Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (2014), pp. 1113–1122. [13] URL:http://www.cs.cornell.edu/home/llee/data/convote.html . [14] URL:http://cs.umd.edu/miyyer/ibc . [15] URL:http://www.ontheissues.org/default.htm . [16] Bengio et all. “Learning Long-Term Dependencies with Gradient Descent is difﬁcult”. In: IEEE Transactions on Neural Network Vol 5 No. 2 (1994). [17] URL:https://en.wikipedia.org/wiki/F1_score . [18] URL:https://en.wikipedia.org/wiki/Precision_and_recall . [19] URL:https://arxiv.org/pdf/1312.0493.pdf . 8</td>
    </tr>
    <tr>
      <th>39</th>
      <td>poli</td>
      <td>The Thirty-Fourth AAAI Conference on Artiﬁcial Intelligence (AAAI-20) Reasoning about Political Bias in Content Moderation Shan Jiang, Ronald E. Robertson, Christo Wilson Northeastern University, USA {sjiang, rer, cbw }@ccs.neu.edu Abstract Content moderation, the AI-human hybrid process of remov- ing (toxic) content from social media to promote community health, has attracted increasing attention from lawmakers due to allegations of political bias. Hitherto, this allegation has been made based on anecdotes rather than logical reasoning and empirical evidence, which motivates us to audit its validity. In this paper, we ﬁrst introduce two formal criteria to measure bias (i.e., independence and separation) and their contextual meanings in content moderation, and then use YouTube as a lens to investigate if the political leaning of a video plays a role in the moderation decision for its associated comments. Our results show that when justiﬁable target variables (e.g., hate speech and extremeness) are controlled with propensity scoring, the likelihood of comment moderation is equal across left- and right-leaning videos. Bad Content Moderation, Bad! Social media has long played host to problematic content such as partisan propaganda (Allcott and Gentzkow 2017),misinformation (Jiang and Wilson 2018), and violent hate speech (Olteanu et al .2018). In an attempt to police this con- tent and improve the health of their user community, social media platforms publish sets of community guidelines that explain the types of content they prohibit, and remove or hide this content from their platforms. This practice is commonly referred to as content moderation . Content moderation is typically implemented as an AI- human hybrid process. To scale with the large amount of toxic content generated online, an AI ﬁltering layer ﬁrst ﬁnds potential candidates for moderation (Gibbs 2017; Sloane2018), and then sends them to human reviewers for a ﬁnal determination (Levin 2017; Gershgorn and Murphy 2017). This content moderation process, however, has been criti- cized for potential bias: biased AI systems have been docu- mented (Barocas, Hardt, and Narayanan 2019; Hutchinsonand Mitchell 2019), and human moderators can bring theirown biases into the moderation process (Diakopoulos and Naaman 2011). As a result, content moderation faces a back- lash from ideological conservatives, who allege that socialmedia platforms are biased against them and are censoring their views (Kamisar 2018; Usher 2018), e.g., Figure 1. These Copyright c/circlecopyrt2020, Association for the Advancement of Artiﬁcial Intelligence (www.aaai.org). All rights reserved. ? Figure 1: Allegations of political bias in content moderation based on anecdotes, not hard evidence. allegations have even spurred lawmakers to action, e.g., in June 2019, the “Ending Support for Internet Censorship Act” was introduced into the US Senate to limit immunity granted by Section 230 of the Communications Decency Act to “en- courage providers of interactive computer services to provide content moderation that is politically neutral” (Hawley 2019). These allegations of political bias, however, are based on anecdotes, and there is little support from logical reasoning and empirical evidence (Jiang, Robertson, and Wilson 2019; Shen et al .2018; Shen and Rose 2019). In this paper, we conduct an audit on the validity of these allegations driven by a high-level research question: •Research question: is content moderation biased? To approach this question, we ﬁrst introduce two formal criteria to measure bias and how they apply in the context of content moderation, and formulate two null hypotheses Hind 0 (for independence) and Hsep 0(for separation) under these criteria. Then, we use YouTube comment moderation as a case study to investigate a more concrete question: •Case study: does the political leaning of a video play a role in the moderation decision for its comments? Our results show: Hind 0is rejected, i.e., comments are more likely to be moderated under right-leaning videos; Hsep 0holds, i.e., with propensity scored justiﬁable target variables (e.g., hate speech and extremeness), there is no signiﬁcant differ- ence in moderation likelihood across the political spectrum. 13669 How to Measure Bias, Really? Recent advances in fairness research provide many criteria to measure bias, each aiming to formalize different desider- ata (Barocas, Hardt, and Narayanan 2019). Most of these criteria characterize the joint or conditional probability be- tween involved variables (e.g., decision, sensitive features), and can be approximately classiﬁed to two categories: inde- pendence and separation (Hutchinson and Mitchell 2019). Independence Independence, also referred to as demographic parity ,i s a fairness criterion that requires the decision variable andthe sensitive feature to be statistically independent. In the context of political bias and content moderation, an item on social media (e.g., post, comment) can be associated with its political leaning P={left,right}and moderation decision M={moderated ,alive}. This criterion requires these two variables to satisfy M⊥⊥P, which, given that Pis a binary variable, is equivalent to: P{M|P=left}=P{M|P=right}. (1) The graphic model of independence criterion is shown in Figure 2a. To allege political bias under this criterion, then, requires empirical evidence to reject (1) as the null hypothesis Hind 0with statistical conﬁdence. Although this criterion is intuitive and has been applied in many studies (Robertson et al .2018; Hu et al .2019), its desirability is context-dependent: e.g., moderation decisions are intended to be made based on the toxicity of content, and if toxicity is unevenly distributed across the political spectrum, the pursuit for independence may be unachievable and even undesirable. Separation Separation, also referred to as equalized odds , is a type of conditional independence that allows dependence between the decision variable and the sensitive feature, but only to the extend that can be justiﬁed by target variables. For contentmoderation, such target variables can include hate speech, extreme videos, etc. Denoting a universe of justiﬁable target variables as J, this criterion requires M⊥⊥P|J, which, given that P is a binary variable, is equivalent to ∀J: P{M|P=left,J}=P{M|P=right,J}. (2) This criterion is also widely adopted in previous studies, especially when the correlation between sensitive featuresand target variables is inherent (Thoemmes and Kim 2011; Lanza, Moore, and Butera 2013; Austin 2008). A practical limitation of this criterion is that stable estima- tors of (2) requires matched observational pairs conditional onJ. Therefore, as Jcontains more variables, matching be- comes more difﬁcult. An alternative method is to summarize all of the target variables into one scalar, i.e., f:R|J|→R. A particular example of fispropensity scoring deﬁned as: ps(J): =P{P=left (or right) |J}(Rosenbaum and Rubin 1983). It is proven that if (2) holds and P{P|J}∈(0,1), then∀ps(J),P{P|ps(J)}∈(0,1)and: P{M|P=left,ps(J)}=P{M|P=right,ps(J)}.(3)0RGHUDWLRQ GHFLVLRQ3ROLWLFDO OHDQLQJ (a) Independence. 1st null hypothesis Hind 0:M⊥⊥P.0RGHUDWLRQ GHFLVLRQ3ROLWLFDO OHDQLQJ-XVWLÀDEOH WDUJHW YDULDEOHV  KDWHVSHHFK H[WUHPHYLGHR HWF SV -  (b) Separation. Propensity scoring function ps(J)is used to summarize Jto a scala, hence 2nd null hypothesis Hsep 0:M⊥⊥P|ps(J). Figure 2: Graph models of fairness criteria. These criteria characterize the joint or conditional distribution of political leaning P(sensitive feature), moderation decision M(deci- sion variable) and justiﬁable target variables J. The graphic model of propensity scored separation criterion is shown in Figure 2b. To allege political bias under this criterion, then, requires empirical evidence to reject (3) as the null hypothesis Hsep 0with statistical conﬁdence. Is Y ouTube Biased, for Example? Y ouTube is one of the major social media platforms that faces allegations of politically biased content moderation, and it practices moderation at different content levels, e.g., videos, channels, users, and comments (Y ouTube 2018a). Here, we use YouTube as a lens to investigate if the political leaning of a video plays a role in the moderation decision for its associated comments.1 Data The Y ouTube data we use contain 84,068 comments posted on 258 political videos,2labeled with involved variables. The moderation decision for each comment is labeled by comparing two snapshots of the dataset: the ﬁrst collected in January 2018 (Jiang and Wilson 2018), and the second in June 2018 (Jiang, Robertson, and Wilson 2019). Disappeared comments within this time range are labeled as moderated , and the others are labeled as alive . The political leaning of the video under which the com- ment was posted is labeled from another dataset (Robertson et al .2018), where all political entities on the web are as- signed an ideological score i∈[−1,1](left to right). We link a video’s publisher to its political entity, and use the sign of the ideological score as its political leaning leftorright . Linguistic signals in comments are used as the ﬁrst set of target variables, as the text content of comments is theprimary focus of the moderation system (YouTube 2018c). We use an existing lexicon Comlex to map the text content to 8 binary variables: swear (including hate speech, e.g., the n-word), laugh (e.g., “haha”), emoji ,fake (e.g., “lie”), ad- 1These results, although under a different frame, are also re- ported in (Jiang, Robertson, and Wilson 2019). 2Available at: https://moderation.shanjiang.me 13670 ʳ˚^0 PRGHUDWHGಭ3 OHIW`ʳ˚^0 PRGHUDWHGಭ3 ULJKW` ss (a) Hind 0is rejected. There is signiﬁcant difference between com- ment moderation probability under left- and right- leaning videos. ʳ˚^0 PRGHUDWHGಭ3 OHIW SV - `ʳ˚^0 PRGHUDWHGಭ3 ULJKW SV - ` ss (b)Hsep 0holds. There is no signiﬁcant difference between comment moderation probability under left- and right- leaning videos with propensity scored justiﬁable variables. Figure 3: Estimated moderation probability. The (conditional) moderation probability for comments with corresponding conﬁdence intervals are estimated to test the independence and separation hypotheses. ministration (e.g., “mayor”), American (e.g., “nyc”, “texas”), nation (e.g., “mexico”), and personal (e.g., “your”). The social engagement of a video is also considered as target variables, e.g., a video with a high dislike rate attracts more ﬂaggers and more attention from moderators. This in- cludes three variables: views ,likes , and dislikes of the video. We also consider the extremeness of a video, as extreme videos are more likely to call for violence or spread conspir- acy theories (Y ouTube 2018b). This is labeled using the same dataset as the political leaning variable. We label a video extreme if|i|&gt;0.5and center otherwise. Misinformation related features are another set of tar- get variables we control. Misinformation might play a rolein content moderation as social media companies have re-cently established collaborations with fact-checkers (e.g., Snopes, PolitiFact) (Glaser 2018). This contains two binary variables: if a video contains misinformation or not (as judged by Snopes or PolitiFact), and if a comment is posted before or after the corresponding fact-check. Results Independence Hind 0measures only moderation decision and political leaning variables. We estimate the empirical proba- bility of the two variable and conﬁdence intervals for bi-nomial proportions. As shown in Figure 3a, there is sig- niﬁcant difference between ˆP{M=moderated |P=left,J} andˆP{M=moderated |P=right,J}, therefore the inde- pendence hypothesis Hind 0 is rejected. However, as we discuss above, the independence criterion has a fatal limitation in this context because there are strong correlations between political leaning and justiﬁable target variables, e.g., comments under right-leaning videos contain signiﬁcantly more swear words (Pearson χ2= 671.2∗∗∗),3 indicating increased likelihood of hateful content, which is the primary trigger for content moderation. Right-leaning videos also have signiﬁcantly more dislikes (Mann-Whitney 3∗p&lt; 0.05;∗∗p&lt; 0.01;∗∗∗p&lt; 0.001. 6HOIPRGHUDWLRQUDWH+VHS  (a) Self-moderation. The effect of self-moderation is minimal.//55 %LDVRIIDFWFKHFNHUV+VHS  (b) Biased fact-checkers. Slight bias does not change results.  7KUHVKROGIRUH[WUHPHFHQWHU+VHS  (c) Thresholding extremeness. V arying thresholds for extreme- ness has mostly minimal effect. 7KUHVKROGIRUULJKWOHIW+VHS  (d) Thresholding political lean- ing. Results ﬂuctuate on both the left and right ends. Figure 4: Robustness of Hsep 0. Potential scenarios are simu- lated to check the robustness of our results. The results are mostly stable except a few cases where the results ﬂuctuate on both left and right. U=4.08×108∗∗∗) than left-leaning ones, providing an alternative explanation that the higher dislike rate may result in more ﬂagged comments, thus increased moderation. Therefore, our main focus is on investigating if the dif- ference in moderation probability can be justiﬁed by tar-get variables, i.e., Hsep 0. We estimate propensity scores ps(J)by logistic regressions, use them to match two- nearest neighbors from our observations, and then com-pute the conditional probability given matched propensityscores. As shown in Figure 3b, there is no signiﬁcant dif- ference between ˆP{M=moderated |P=left,ps(J)}and ˆP{M=moderated |P=right,ps(J)}, therefore no evi- dence to reject the second hypothesis, i.e., Hsep 0 holds. Overall, our results show that although comments are more likely to be moderated under right-leaning videos, this differ- ence is well-justiﬁed, i.e., once our measured target variables are balanced, there is no signiﬁcant difference in moderation likelihood across the political spectrum. Robustness Conclusions made from observational data are often ques- tionable due to alternative explanations. Therefore, we con- duct additional experiments to check the robustness of Hsep0, including self-moderation instead of moderation by the plat- form (Figure 4a), bias in the ratings provided by fact-checkers (Figure 4b), varying the threshold to label extremeness (Fig- ure 4c), and labeling political leaning only when |i|exceed- ing a certain threshold (Figure 4d). Due to space limits, we omit details on the implementation and discussion of these experiments. Interested readers can refer to our original pa- per (Jiang, Robertson, and Wilson 2019). In short, these experiments show that Hsep 0in general holds when the simulated scenario is moderate, but can be rejected under extreme cases. However, under these scenarios, the results ﬂuctuate on both the left and right ends, i.e., bias against both left and right political leanings. Therefore, the allegation of bias in YouTube comment moderation is still not supported. 13671 It’s Complicated By using Y ouTube content moderation as a lens, our results show that the allegation of biased content moderation is sup- ported by the intuitive (yet out-of-context) independence cri- terion, however, it is not supported by the separation criterion, where we justify moderation decisions with other target vari- ables. Interestingly, research on alleged political bias often reaches similar conclusions: (Bakshy, Messing, and Adamic 2015) show that the allegation of biased newsfeed on Face- book was due more to homophily than algorithmic curation; (Robertson et al .2018) show that the allegation of biased search results from Google was dependent largely on the input query instead of the self-reported ideology of the user. The goal of this research is twofold. First, we call for transparency in content moderation practices. The opaque nature of this process can breed conspiracy theories such as the one we investigated in this paper. Further, these allega- tions are challenging to validate, as neither researchers, nor critics, can access removed data that underpin moderation decisions. Therefore, we recommend that moderated content be preserved and protected. Second, our work ﬁts in a broader scope of under- standing fairness, discrimination, neutrality, and bias inalgorithm-mediated systems (Baeza-Yates 2016; Sandviget al .2014). We introduce recently proposed theories of fairness measures (Barocas, Hardt, and Narayanan 2019;Hutchinson and Mitchell 2019) to an existing body of em-pirical work on auditing political bias on the web (Jiang,Robertson, and Wilson 2019; Robertson et al .2019; Hu et al.2019; Ali et al .2019; Jiang, Martin, and Wilson 2019; Robertson et al .2018). We hope this work can foster a health- ier, contextual, and dialectical discussion of political bias and social media at large. Acknowledgments This research was supported in part by NSF grant IIS- 1553088. Any opinions, ﬁndings, and conclusions or rec- ommendations expressed in this material are those of the authors and do not necessarily reﬂect the views of the NSF. References Ali, M.; Sapiezynski, P .; Bogen, M.; Korolova, A.; Mislove, A.; and Rieke, A. 2019. Discrimination through optimization: How facebook’s ad delivery can lead to biased outcomes. PACM on HCI 4(CSCW). Allcott, H., and Gentzkow, M. 2017. Social media and fake news in the 2016 election. Journal of Economic Perspectives 31(2). Austin, P . C. 2008. A critical appraisal of propensity-score matching in the medical literature between 1996 and 2003. Statistics in Medicine 27(12). Baeza-Yates, R. 2016. Data and algorithmic bias in the web. In Proc. of WebSci . Bakshy, E.; Messing, S.; and Adamic, L. A. 2015. Exposure to ideologically diverse news and opinion on facebook. Science 348. Barocas, S.; Hardt, M.; and Narayanan, A. 2019. Fairness and Machine Learning . fairmlbook.org. http://www.fairmlbook.org. Diakopoulos, N., and Naaman, M. 2011. Towards quality discourse in online news comments. In Proc. of CSCW .Gershgorn, D., and Murphy, M. 2017. Facebook is hiring more people to moderate content than twitter has at its entire company. Quartz. Gibbs, S. 2017. Google says ai better than humans at scrubbing extremist youtube content. The Guardian. Glaser, A. 2018. Y outube is adding fact-check links for videos on topics that inspire conspiracy theories. Slate. Hawley, J. 2019. Ending support for internet censorship act. Hu, D.; Jiang, S.; Robertson, R. E.; and Wilson, C. 2019. Auditing the partisanship of google search snippets. In Proc. of WWW . Hutchinson, B., and Mitchell, M. 2019. 50 years of test (un)fairness: Lessons for machine learning. In Proc. of F AT* . Jiang, S., and Wilson, C. 2018. Linguistic signals under misinfor- mation and fact-checking: Evidence from user comments on social media. PACM on HCI 2(CSCW). Jiang, S.; Martin, J.; and Wilson, C. 2019. Who’s the guinea pig?: Investigating online a/b/n tests in-the-wild. In Proc. of F AT* . Jiang, S.; Robertson, R. E.; and Wilson, C. 2019. Bias misperceived: The role of partisanship and misinformation in youtube comment moderation. In Proc. of ICWSM . Kamisar, B. 2018. Conservatives cry foul over controversial group’s role in youtube moderation. The Hill. Lanza, S. T.; Moore, J. E.; and Butera, N. M. 2013. Drawing causal inferences using propensity scores: A practical guide for community psychologists. American journal of community psychology 52(3-4). Levin, S. 2017. Google to hire thousands of moderators after outcry over youtube abuse videos. The Guardian. Olteanu, A.; Castillo, C.; Boy, J.; and V arshney, K. R. 2018. The effect of extremist violence on hateful speech online. In Proc. of ICWSM . Robertson, R. E.; Jiang, S.; Joseph, K.; Friedland, L.; Lazer, D.; and Wilson, C. 2018. Auditing partisan audience bias within google search. PACM on HCI 2(CSCW). Robertson, R. E.; Jiang, S.; Lazer, D.; and Wilson, C. 2019. Au- diting autocomplete: Suggestion networks and recursive algorithm interrogation. In Proc. of WebSci . Rosenbaum, P . R., and Rubin, D. B. 1983. The central role of the propensity score in observational studies for causal effects. Biometrika 70(1). Sandvig, C.; Hamilton, K.; Karahalios, K.; and Langbort, C. 2014. Auditing algorithms: Research methods for detecting discrimination on internet platforms. Data and Discrimination . Shen, Q., and Rose, C. 2019. The discourse of online content moderation: Investigating polarized user responses to changes in reddit?s quarantine policy. In Proc. of ALW3 ACL . Shen, Q.; Yoder, M.; Jo, Y .; and Rose, C. 2018. Perceptions of censorship and moderation bias in political debate forums. In Proc. of ICWSM . Sloane, G. 2018. Facebook pursues ai in bid to id harmful content. AdAge. Thoemmes, F. J., and Kim, E. S. 2011. A systematic review of propensity score methods in the social sciences. Multivariate be- havioral research 46(1). Usher, N. 2018. How republicans trick facebook and twitter with claims of bias. The Washington Post. Y ouTube. 2018a. Community guidelines. Y ouTube. 2018b. Harassment and cyberbullying policy. Y ouTube. 2018c. Hate speech policy. 13672</td>
    </tr>
    <tr>
      <th>93</th>
      <td>news</td>
      <td>doi:10.1111/j.1662-6370.2011.02015.x\n\nThe Fairness of Media Coverage in Question:\nAn Analysis of Referendum Campaigns on\nWelfare State Issues in Switzerland\nLionel Marquis, Hans-Peter Schaub &amp; Marlène Gerber\nUniversity of Lausanne and University of Berne\n\nAbstract: The mass media are assigned an important role in political campaigns on popular votes.\nThis article asks how the press communicates political issues to citizens during referendum campaigns, and whether some minimal criteria for successful public deliberation are met. The press coverage of all 24 ballot votes on welfare state issues from 1995 to 2004 in Switzerland is examined,\ndistinguishing seven criteria to judge how news coverage compares to idealized notions of the media’s role in the democratic process: coverage intensity, time for public deliberation, balance in media\ncoverage, source independence and inclusiveness, substantive coverage, and spatial homogeneity.\nThe results of our quantitative analysis suggest that the press does fulﬁl these normative requirements to a reasonable extent and that fears about biased or deceitful media treatment of ballot\nissues are not well-founded. However, some potential for optimizing the coverage of referendum\ncampaigns by the Swiss press does exist.\n\nKeywords: Referendum campaigns, media fairness, press coverage, welfare state, Switzerland\n\n1. Introduction1\nInstruments of direct democracy are at the heart of the Swiss political system. They are\nwidely used, so much in fact that Switzerland alone accounts for half of all referendums\nheld at the national level all over the world (Kaufmann et al. 2005; DuVivier 2006). Quite\nexpectedly, this unequalled degree of direct democratic practice has triggered research to\nexplore the determinants, conditions, and outcomes of the use of initiatives and referendums, as well as research to ﬁnd out the causes of their success or failure. For example,\nSwiss scholars have directed their attention to the institutional eﬀects of the referendum on\nthe integration of political forces and to its role in shaping the Swiss concordance system\n(e.g., Neidhart 1970; Papadopoulos 1998; Vatter 2002). Another important area of inquiry\nfocused on individual and aggregate citizen behaviour in referendum votes, relating voting\npatterns to the level of elite support or to structural properties of campaign propaganda\n(e.g., Hertig 1982; Trechsel and Sciarini 1998; Bützer and Marquis 2002).\n1\n\nThis research was carried out at the University of Berne in the framework of a project on the ‘‘Political Consequences of Attitudes Toward the Welfare State’’ funded by the Swiss National Science Foundation, whose ﬁnancial\nsupport is gratefully acknowledged (grant #100012–108274). The authors would like to thank Klaus Armingeon\nand Nathalie Giger for their support, as well as Hans Hirter, head of the Anne´e Politique Suisse series, for giving\nus access to its invaluable archive of Swiss press releases. We also thank two anonymous reviewers for their extremely helpful comments on an earlier draft of our manuscript.\n\n 2011 Swiss Political Science Association\n\n16626370, 2011, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/j.1662-6370.2011.02015.x by Cochrane Israel, Wiley Online Library on [09/12/2022]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\nSwiss Political Science Review 17(2): 128–163\n\n129\n\nIn this study, we follow a complementary approach. Speciﬁcally, we ask how political\nissues are communicated to citizens during referendum campaigns, and whether some minimal criteria for successful public deliberation are met. Our analysis is based on the press\ncoverage of twenty-four ballot votes on welfare issues, spanning more than two legislative\nperiods (1995–2004) of highest importance for the development of the Swiss welfare state.\nAccordingly, our aim is to shed light on the journalistic perception of ballot campaigns.\nHowever, the views of political actors directly involved in partisan campaigns (e.g., parties,\nbusiness associations) are also reﬂected in their agenda-building eﬀorts to capture and manage media attention. It is from the interplay between these two perspectives on the news\nprocess — the journalists’ gatekeeping role and the elites’ agenda-building role — that the\nfocal questions of this study arise. What is the degree of media autonomy? How inclusive\nare journalists’ accounts of the political actors involved in campaigns? How biased are their\nportrayals of issues?\nIn the next section of the paper we present some current ideas about public deliberation\nand provide theoretical insights into the structure and functions of referendum campaigns.\nWe thus set a number of criteria for assessing whether media campaigns may be regarded as\nwise guides to sound decision-making. Section 3 then presents the empirical data collected\nand the methods used for measuring the quality of media campaign coverage. Sections 4 to\n7 oﬀer evidence of how journalists report on welfare state issues. We show that the press\ntreatment of these issues does not fall short of expectations as concerns seven distinct criteria\nof ‘‘fair coverage’’. On this overall basis, we conclude in Section 8 that some fears about\nskewed or deceitful media treatment of ballot issues are not justiﬁed, but also that the\ncommunication and framing of issues does not meet all conceivable normative requirements.\n\n2. Theoretical background: Criteria of fair media coverage\nThis article focuses on the ‘‘supply side’’ of referendum campaigns. Speciﬁcally, we attempt\nto deﬁne a number of dimensions on which to judge the fairness and democratic utility of\nthe coverage of referendum issues in the Swiss media. The importance of articulating ‘‘democratic expectations of media performance’’ has been emphasized in several contributions\n(e.g., Gurevitch and Blumler 1990; McLeod et al. 2002). In particular, the ‘‘social responsibility’’ of the media has attracted much attention, leading scholars to deﬁne a number of\ndesirable qualities of media content –– some of which may be properly regarded as ‘‘ethical’’, but others are more directly related to the functioning of democratic systems (e.g.,\nBunton 1998; Christians and Nordenstreng 2004).\nOur questioning in this article takes place in the interplay of several disciplines — including media ethics and the theory of deliberative democracy. These are broad and currently\nexpanding disciplinary areas rather than formalized theories coupled with established methods and ﬁeldwork, as the example of deliberative theory suggests (see Bächtiger et al. 2010).\nThese theoretical accounts do not provide a clear and agreed-upon indication of what\nmedia fairness should be conceived of, let alone how to measure fairness. However, to the\nextent that the media are an important arena where democratic deliberation may occur, one\nmajor conceptual distinction from deliberative theory is helpful for framing our approach\nto media fairness.\nScholars in the deliberative framework usually advance that democratic decision-making\nand deliberation may be appraised (and purportedly justiﬁed) either from an epistemic or\nfrom a procedural standpoint (see Cohen 1986; Estlund 1997; List and Goodin 2001). The\ndistinction is captured in the question of ‘‘whether we want our political outcomes to be\n 2011 Swiss Political Science Association\n\nSwiss Political Science Review (2011) Vol. 17(2): 128–163\n\n16626370, 2011, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/j.1662-6370.2011.02015.x by Cochrane Israel, Wiley Online Library on [09/12/2022]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\nMedia Coverage of Referendum Campaigns\n\nLionel Marquis et al.\n\nright or whether we want them to be fair’’ (List and Goodin 2001: 277). To be sure, democratic processes oriented toward the search for ‘‘truth’’ or toward the observance of procedures may often bring about similar outcomes. It is nevertheless easy to imagine cases\nwhere ‘‘correctness’’ lacks procedural fairness (e.g., one individual ‘‘rightly’’ chooses for all\nothers) and other cases where procedural rigidity is misplaced from procedure-independent\nstandards (e.g., it violates some notion of the ‘‘common good’’).\nThe ﬁeld of media ethics is concerned with the same kind of questions, asking whether\nand how the ‘‘fairness’’ goal of media coverage can put up with the sometimes dissonant\nideal of the ‘‘correctness’’ of the views being presented — beyond the mere question of\ninformation ‘‘accuracy’’. In this article we adopt a procedural stance and examine how media\ncoverage fulﬁls a number of formal rules regarding the form, provenance, and diﬀusion of\ninformation.2 Even though correctness criteria may appear to involve considerations of a\nmore normative nature, it is however obvious that any set of formal-procedural rules is\nitself not ‘‘given’’. As will become clear, some of the procedural criteria described below\nhave been the subject of considerable speculation and debate. Moreover, ‘‘fairness’’, as the\npivotal concept of procedural approaches, is to be taken here in a generic sense, as it\nextends beyond the notions of fairness developed in careful theoretical analyses of ‘‘democratic proceduralism’’ (Cohen 1989; Estlund 1997).3 This enlarged concept of media fairness\nposits that, whatever the media induce citizens to think and choose, they must do so within\ncertain procedural boundaries.\nIn this article we will focus on seven criteria that appear particularly relevant to evaluating the fairness of the media coverage of referendum campaigns in Switzerland. In this and\nthe following sections we identify these criteria at a fairly abstract level, based on the available literature, and we show how each of them provides an appropriate dimension for\nassessing the achievements of campaign coverage from a normative perspective. Our goal in\nthis contribution is primarily comparative, as we aim at showing how fairness varies across\nissue domains, across media outlets, over time, and across geographical units. Therefore,\nwe deliberately avoid setting precise boundaries between ‘‘fair’’ and ‘‘unfair’’ coverage. On\nthe other hand, the measurement of relevant criteria should be as precise and abstracted\nfrom context as possible, in order to allow for valid and meaningful comparisons. Besides,\nin order to avoid conceptual ambiguities, let us make clear that the term ‘‘fairness’’ is used\nhere to denote the ‘‘positive side’’ on each dimension of media coverage. Thus we do not\nwish to imply, for instance, that a short, two-week campaign coverage is deliberately\n‘‘unfair’’ or intended to fool voters, but only that a longer time period is more desirable\naccording to some formal-procedural normative standard.\nBased on this broad deﬁnition, we now delineate seven speciﬁc criteria for appraising the\nfairness of media campaign coverage.\n2\nBy choosing a procedural rather than epistemic perspective, we do not mean to imply any superiority or higher\ndesirability of the former perspective in dealing with our subject.\n3\nNevertheless, the fact that the elaboration and selection of procedural criteria is no less subjective than that of\nepistemic principles does not mean that the fundamental distinction between the two types of considerations is\nblurred. Generally speaking, our approach to media fairness is blind to information which may be of uttermost\nimportance from an epistemic perspective — the speciﬁc opinions and origins of information purveyed in media\ncoverage. We do not say that journalists are not (or should not be) driven by an orientation toward the common\ngood, as when they downplay opinions that are disruptive and threatening for the democratic order — for example,\nBritish and German journalists have totally ignored extreme right parties in their coverage of election campaigns in\nthe 1980s and 1990s (Bornschier 2010: 174-5). We simply say that this reference to the explicit content and implicit\nintent of media coverage is not relevant for our present purposes.\n\n 2011 Swiss Political Science Association\n\nSwiss Political Science Review (2011) Vol. 17(2): 128–163\n\n16626370, 2011, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/j.1662-6370.2011.02015.x by Cochrane Israel, Wiley Online Library on [09/12/2022]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n130\n\n131\n\n1. Suﬃcient media coverage of issues. We ﬁrst consider the information function of the\nmass media and the public knowledge of political issues that is expected to stem from media\ninformation. Learning eﬀects are extremely important from a normative perspective, since\nthey attest to the capacity of the people to understand politics and to exert inﬂuence on\npolicies, as emphasized by classical democratic theory (e.g., Berelson 1952; Kelley 1960;\nKrouse and Marcus 1984; but see Pateman 1970). As a matter of fact, it has long been\nshown that media exposure generates knowledge about issues and candidates, and enhances\nopinion strength and attitude integration (e.g., Berelson et al. 1954: chap. 11; Delli Carpini\nand Keeter 1996: chap. 5; Jerit et al. 2006). These eﬀects are usually not overwhelming, but\nthey are quite robust across a wide range of contexts. Accordingly, learning eﬀects have\nbeen observed in the Swiss direct democratic context as well (Kriesi 1994; Kriesi 2005: chap.\n4; Marquis 2006). Information holding, in turn, matters for political judgments and voting\ndecisions (Bartels 1996; Sturgis 2003). For example, a lack of knowledge causes citizens to\nbe more conservative on some issues and more liberal on others (Althaus 1998; Gilens\n2001), and it usually prompts reliance on heuristic cues that can lead to serious misﬁts\nbetween the people’s actual choices and their own values and interests (e.g., Kuklinski and\nQuirk 2000). Therefore, the overall intensity of media campaign coverage probably matters\nboth for the public knowledge of issues and for the ﬁnal outcome of the ballots. In fact,\nfew would question that a suﬃcient amount of information is necessary for the proper functioning of democratic institutions. However, as argued below, the content of information\nalso deserves attention, especially as concerns value-laden information (e.g., Hofstetter\net al. 1999).\n2. Suﬃcient time for public deliberation. Time is needed for the public to become\ninformed about the issues and to ponder the pros and cons of a ballot proposal. If all campaign coverage takes place in a last-minute avalanche, then it is unlikely to beneﬁt citizens,\nhowever extensive it may be in terms of sheer volume. Quite logically, the inﬂuence of a\ncampaign usually concentrates on citizens who make up their minds during that campaign,\nthat is, for people whose voting decision was neither clear from the outset nor delayed until\nthe very last campaign days (e.g., Chaﬀee and Choe 1980; Fournier et al. 2004; Marquis\n2006). Were campaigns so short-lived as to be virtually nonexistent for the purpose of\ndeliberation, the collective decision-making process would be essentially reduced to the use\nof last-minute shortcuts (e.g., status quo bias) or of long-standing attitudes and prejudices\n(e.g., a personal bias against ‘‘big government’’). The longer the referendum campaigns, the\nmore likely it is that the same issues will be tackled and thus the same arguments will be\nrepeated. In turn, this cumulative exposure may be expected to facilitate information acquisition by citizens (see Marquis and Bergman 2009). In other words, frequent exposure to\ncampaign information enhances the accessibility of the relevant concepts (e.g., Higgins\n1996; Förster and Liberman 2007) and heightens the likelihood that they will permeate the\ncitizens’ voting considerations.\n3. No outright bias in media coverage. A relatively unbiased media coverage of issues may\nbe stressed as a prerequisite for sound collective deliberation. This ‘‘impartiality’’ or ‘‘balance’’ assumption is more or less explicitly present in the writings of political philosophers\nsuch as Arendt, Elster, and Habermas, but also in concrete experiments designed to mimic\nideal deliberative procedures (e.g., Yankelovich 1991: chap. 12; Luskin et al. 2002). Likewise, the norm of balance is deeply enmeshed in Western (primarily Anglo-American) culture and values (Schudson 2001). It looms large in the self-descriptions and perceived\nprofessional standards of journalists (e.g., Tuchman 1972; Patterson 1998) and in academic\nstudies of media coverage unveiling their structural biases (e.g., Gitlin 1980; Hallin 1984;\n 2011 Swiss Political Science Association\n\nSwiss Political Science Review (2011) Vol. 17(2): 128–163\n\n16626370, 2011, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/j.1662-6370.2011.02015.x by Cochrane Israel, Wiley Online Library on [09/12/2022]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\nMedia Coverage of Referendum Campaigns\n\nLionel Marquis et al.\n\nEntman 2004; Bennett et al. 2007).4 In addition, the development of ‘‘media watchdog journalism’’ monitoring media bias is, in part, reﬂective of the increasingly acute perception of\nthe media bias problem in Western societies (Graber 2006; Hayes 2008). For our present\npurposes, such ‘‘issue neutrality’’ may come in several forms: the media may simultaneously\npresent the arguments of both sides on an issue, or they may successively alternate between\npros and cons (and thus act neutrally on an aggregate basis), or else they may restrict themselves to presenting ‘‘facts’’ and avoid matters of opinion. Meanwhile, the requirement that\nthere be no outright bias in media coverage must not be taken to mean that the media\nshould display no partisan bias, or that they should avoid expressing opinions altogether.\nActually, some bias is inevitable and probably inconsequential, provided that it is not systematic and identical across all media outlets and issues. In addition, some mix of factual\ninformation and opinion statements is probably preferable to either one alone, for voters\nmost certainly need –– and look for –– both types of information to make up their minds.\nAs one study has shown, a voter’s general need for orientation toward media information is\ncodetermined by her interest in acquiring knowledge about facts and about journalistic evaluations (Matthes 2005).\n4. Source independence. Similar to balance, the ‘‘source independence’’ norm implies\nthat the media are assigned responsibility for addressing issues in an autonomous manner.\nIn this case, the normative expectation is that journalists should not depend too heavily\non government, political parties and special interests for getting and framing information\non campaign issues. Actually, there is a widespread belief in Western societies, including\nSwitzerland, that journalists have ‘‘surrendered to complexity’’ and mainly serve as emissaries or ‘‘postmen’’, as Wuerth (1999: 373) put it. According to the ‘‘determination\nhypothesis’’ and similar accounts of media-politics relationships, most news originates\nfrom oﬃcial sources, including government and various branches of the administration,\nand especially from governmental public relations activities. Such source dependency, so\nthe critics say, would lead to the formation of a misinformed and quiescent public (e.g.,\nBaerns 1979; King and Schudson 1995; Shoemaker and Reese 1995; Bentele 2003). For\ninstance, Grossenbacher et al. (2006) found that the press conferences held by two Swiss\ncantonal governments were indeed covered extensively by the local media, and that the\npress releases issued on these occasions were often published in original or only slightly\nabridged form, reproducing the local authorities’ positive self-assessments and issue priority. Likewise, more than half of the news stories in the U.S. press persistently emanate\nfrom oﬃcial sources (e.g., Sigal 1973; Graber 2006). However, some scholars have cast\ndoubt on the generalizability of such ﬁndings. For instance, it was found that the parties’\npress releases rarely ﬁnd their way into the German print media and ⁄ or that they are frequently altered and reframed by journalists (e.g., Donsbach and Wenzel 2002; Fröhlich\nand Rüdiger 2006). To some extent, journalists and their political sources are interdependent, both from a historical perspective (Schönhagen 2008) and in terms of role relationships (Merritt 1995). Journalists themselves are ambivalent about the autonomy issue. On\nthe one hand, they increasingly value public relations sources and practitioners in the\n4\n\nBiased coverage may occur in relatively subtle ways, such as when the photographs of candidates in newspapers\nare more or less favourable, depending on the ﬁt between the candidate’s and the paper’s political leanings (Barrett\nand Barrington 2005). However, no consensus can be found in the literature as to whether media coverage is structurally biased, for instance conservative-leaning vs. liberal-leaning, or pro- vs. anti-establishment. Interestingly, the\npublic perception that the media are biased toward liberal views may not be due to ‘‘real-world’’ media coverage\nbias, but to media self-coverage about biased media content (Watts et al. 1999).\n\n 2011 Swiss Political Science Association\n\nSwiss Political Science Review (2011) Vol. 17(2): 128–163\n\n16626370, 2011, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/j.1662-6370.2011.02015.x by Cochrane Israel, Wiley Online Library on [09/12/2022]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n132\n\n133\n\ncourse of their professional experience (Sallot and Johnson 2006), not least because by\ncreating long-term relationships with insiders they are granted routine access to valuable\nﬁrst-hand information. On the other hand, ‘‘autonomy’’ is seen as an important journalistic norm (e.g., Gurevitch and Blumler 1977; McDevitt 2003), admittedly with some important diﬀerences between national contexts (Patterson 1998; Statham 2006).5 In the present\nstudy, we will assess to what extent journalistic accounts of the campaign issues are elaborated ‘‘independently’’ or based instead on ‘‘self-serving’’ sources of information such as\ngovernment, parties, and referendum committees.\n5. Source inclusiveness. The norm of inclusion calls for the coverage of the whole diversity of viewpoints, arguments, and groups engaged in a referendum campaign. This norm\nthus concerns the question of how broad the variety of sources to which journalists refer\n(and which they make available to the public) should be. From another perspective, it\nasks how open or restricted the access to the media should be for diﬀerent actors to\nvoice their standpoints toward a referendum issue. Several democratic theorists have\nstressed the importance of an unrestricted and equal access for all individuals and societal groups to democratic processes in general (e.g., Dahl 1989: 119–131) and to the processes of public deliberation and will-formation in particular. The inclusiveness of the\nlatter is especially prominent in theories of deliberative democracy. Cohen (1989: 21–23,\n30; see also Dryzek 1990; Habermas 1992) regards inclusiveness and open access as a\ncrucial aspect of an ‘‘ideal deliberative procedure’’.6 However, various empirical studies\nhave raised doubts about whether the media in reality live up to these expectations of\ndemocratic theorists (e.g., Page 1996; Gerhards 1997; Graber 2003). Pfetsch (2004) and\nBerkowitz (2009) argue that the degree of inclusiveness heavily varies depending on factors such as the cultural context, the journalists’ social characteristics, or the issue at\nstake. Speciﬁcally for Swiss referendums, Marquis and Bergman (2009) report a quite\nmarked decrease in the variety of actors involved in advertisement campaigns in the\n1990s. One reason for a low variety of sources might be the increasing commodiﬁcation\nand commercialization of the media, which presses them to restrict themselves to sources\ncompatible with the mainstream (Meier and Jarren 2002). Second, an increasing professionalization in the PR departments of the more established political actors might raise\nthe hurdles for less prominent and ﬁnancially disadvantaged groups to ﬁnd their way\ninto the media (Gurevitch and Blumler 1990). Third, while there do exist strategies for\npolitical ‘‘outsiders’’ to get included in a mediated political discourse, established political\nactors have also developed counter-strategies to hold outsiders and their arguments oﬀ\nthe media attention (Kriesi 2003: 221–225). In the following analysis, we will assess the\nvariety of reported viewpoints by the number of diﬀerent groups of actors which journalists draw on as information sources.\n6. Substantive coverage. For many observers, it is not enough that media information\nbe balanced and independent of special interests — in addition, it has to address the\nsubstance of political issues. In fact, an essential part of campaign news coverage is\n5\n\nFurther, journalists may feel committed to autonomy as a result of their own audiences’ support for the norm\nof impartial coverage (Hargreaves and Thomas 2002) and as a result of their commitment being challenged by\n‘‘popular commentators’’ such as bloggers (Singer 2007). In fact, public trust in the media may be undermined by\nexcessive governmental control (Connolly and Hargreaves Heap 2007).\n6\nThe inclusion of the whole diversity of opinions and societal groups is also a core request of the so-called ‘‘diﬀerence democrats’’, who emphasize that it enhances not only the legitimacy of the opinion-building process, but also\nthe stock of social knowledge and information available to the participants in the public discourse, thus facilitating\nwell-informed and rational decisions (Young 1990: 119; 2000: 81–120; see also Connolly 1991).\n\n 2011 Swiss Political Science Association\n\nSwiss Political Science Review (2011) Vol. 17(2): 128–163\n\n16626370, 2011, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/j.1662-6370.2011.02015.x by Cochrane Israel, Wiley Online Library on [09/12/2022]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\nMedia Coverage of Referendum Campaigns\n\nLionel Marquis et al.\n\nframed in terms of non-substantive features of elections, such as their ‘‘game’’ or\n‘‘horserace’’ aspects, and ignores their underlying issues (e.g., Strömbäck and Dimitrova\n2006). A strong case has been made against the pervasiveness and deleterious inﬂuence\nof horserace information in a variety of election campaigns (e.g., Gollin 1980; Bartels\n1988; Patterson 1994). We therefore only brieﬂy discuss the relevance of the issue for\nthe Swiss direct democratic environment. As pointed out by Rothmayr and Hardmeier\n(2002) and Longchamp (1998), the practice of direct democracy in Switzerland has\ninhibited rather than stimulated the development and use of opinion polls by both the\nmedia and governmental agencies. This is because frequent and ‘‘real’’ voting results\nare widely considered to provide a better account of public opinion than ‘‘unrealistic’’,\n‘‘out-of-context’’ opinion polls. However, Swiss journalists may ﬁnd such polls attractive\nto report, in light of the news media’s preference for the ‘‘horserace’’ aspects of campaigns and for warfare and sports narratives. Moreover, they may ﬁnd it convenient to\ndraw on such ready-to-use information, without paying too much attention to its technical validity and practical relevance (see Hardmeier 1999). As a way to gauge the\nextent of horserace coverage, we will compare the amounts of ‘‘campaign-oriented’’ and\nof ‘‘issue-oriented’’ information provided by the media.\n7. Spatial homogeneity. In a multicultural society such as Switzerland, the protection\nof minorities is usually a pressing issue. In addition, there is widespread concern that\ndirect democratic decisions might heighten the risk of some ‘‘tyranny of the majority’’\n(e.g., Donovan and Bowler 1998; Hajnal et al. 2002). In Switzerland, diﬀerences between\nlinguistic regions, particularly as concerns voting behaviour in referendums, constitute\none of the most salient political cleavages (e.g., Kriesi 1998; Linder et al. 2008). This\ncleavage is also one that is clearly identiﬁable and frequently reported on in the news\nmedia, probably because it is conﬂict-laden and thus has a high news value (Kriesi et al.\n1996; Hardmeier 2000: 388). The Swiss media system is itself highly segmented along linguistic lines (Kriesi et al. 1996; Wuerth 1999; Blum 2003; Tresch 2008), and even the\npublic service media ﬁnd it increasingly diﬃcult to promote national cohesion, as this\ntask is incompatible with economic imperatives (Meier and Schanne 1994: 37–39). In\nfact, concerns about majority rule are not substantiated by empirical evidence in general\nterms, as referendum voting patterns are much more homogeneous than heterogeneous\nacross cantonal and linguistic units (Diskin et al. 2007). In some cases, however, these\nconcerns may be justiﬁed, as suggested by parliamentary votes or referenda results on welfare state issues, showing support for welfare policies to be signiﬁcantly higher in the\nFrench- than in the German-speaking region (e.g., Leuthold et al. 2007). To be sure, it is\nperfectly legitimate for journalists from diﬀerent places and cultural backgrounds to pursue their own agendas, for example as they index their treatment of job issues to real or\nperceived unemployment rates in their own area. Likewise, journalists may choose diﬀerent speciﬁc information ‘‘formats’’ (e.g., heavily biased ‘‘opinion’’ articles or more balanced ‘‘points of view’’ articles) depending on their cultural environment and on how\nparochial the interests at stake are (e.g., Robinson 1995: 360–362). Accordingly, there is\nno reason to expect equal issue coverage all over the country. However, the inﬂuence of\nthe media on voters can diﬀer between the linguistic areas (e.g., Kriesi 1994; Marquis\n2006: 623–628). Hence, heterogeneity of media content may reinforce existing cleavages\nand (re)produce undesirable tensions between linguistic communities. In the cases where\nthe majority group prevails, it may fuel the argument of a ‘‘tyranny of the majority’’. In\nsum, small between-region variations in media coverage are arguably preferable to great\nvariations.\n 2011 Swiss Political Science Association\n\nSwiss Political Science Review (2011) Vol. 17(2): 128–163\n\n16626370, 2011, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/j.1662-6370.2011.02015.x by Cochrane Israel, Wiley Online Library on [09/12/2022]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n134\n\n135\n\n3. Methods and measurements\nThis study bears on the 24 ballot votes dealing with welfare state issues which were held in\nSwitzerland between 1995 and 2004 (see Table A1 in Appendix). Our empirical data consists of press articles collected in twenty-eight daily or weekly newspapers during an eightweek period preceding each ballot. The papers together account for a daily circulation ﬁgure of about two million copies and they are quite representative of the various Swiss\nregions.7 However, they were selected mainly for availability reasons, as they are consulted\non a daily basis by the collaborators of the Anne´e Politique Suisse at the University\nof Berne, Switzerland, and their content is classiﬁed into ﬁne-grained thematic categories.\nIt was thus easy to ﬁnd out all campaign-related articles and to code the relevant\ninformation.\nTwo levels of analysis: articles and issue statements. Overall 4303 articles were found and\ncoded. From these, a one-quarter sample of 1088 articles was randomly selected for additional analysis of the campaign issue statements.8 A maximum of three issue statements per\narticle were coded, with the selection occurring on the basis of internal importance within\neach article. In total, 2859 issues were coded out of our sample, making for an average of\n2.63 issues per article.\nIntensity and length. The number of articles and their size (in centimetres squared, then\nconverted in number of standard newspaper pages) were used to determine the intensity of\ncampaign coverage.9 Based on the date on which an article was published, the number of\ndays remaining before voting day was used to compute a measure of the ‘‘average campaign\nday’’ (mean of days remaining for separate ballots or newspapers).\nSource independence and inclusiveness. Up to ﬁve sources per article were coded to determine who the ‘‘speakers’’ were, i.e., which categories of individuals or groups were primarily involved in determining the content of an article. Multiple sources were considered; for\nexample, when a journalist interviews a political leader, two separate sources are coded.\nSimilarly, up to two sources were coded for each issue statement within an article. We thus\ncontrol for the fact that the original issue arguments delivered by a given source are often\nembedded in the discourse of a diﬀerent ‘‘speaker’’ through positive or negative references.\nFor example, it can be the case that all issue statements contained in an article where\n7\n\nThe 1998 circulation ﬁgures are taken from the WEMF marketing research institute. The two related outlets\n‘‘Blick’’ and ‘‘Sonntags-Blick’’ are considered here as one and the same paper. In contrast, the French-speaking\ntabloid ‘‘Le Matin’’ was not available in the archive of the Anne´e Politique Suisse, although it is one of the major\nSwiss papers. However, a glimpse at the data for ‘‘Blick’’, its counterpart in the German-speaking region, suggests\nthat tabloids play a relatively minor role in the coverage of referendum campaigns. As a matter of fact, the 28 outlets are edited in 16 diﬀerent Swiss cantons (out of a total of 26 cantons). Further, according to survey data, a\nmajority of people in all but three cantons (SO, TG, ZH) are ‘‘heavy’’ or ‘‘medium users’’ of at least one newspaper\namong those considered here (http://www.remp.ch/fr/glossar/index.php [accessed: 15.02.2010]).\n8\nThe selection was performed by simply picking every fourth article in the database, reinitializing the count for\neach ballot issue. Checks were made to ensure that the obtained sample was not diﬀerent from the whole dataset as\nconcerns provenance, format, and article bias. An issue statement is deﬁned here as an argument or set of arguments related to one speciﬁc matter addressed in the campaign about a ballot measure. It may pertain to a policy\ntheme (e.g., environment, energy), a driving value or principle (e.g., solidarity), distinctive qualities of the proposed\nmeasure (e.g., ﬂexibility), or the larger context of the ballots (e.g., rationale for a tactic vote). Issue statements have\nbeen classiﬁed into thirty-three categories (full list available upon request from the authors). For reasons of space\nlimitation, however, we will not dwell on the question of which speciﬁc issues were emphasized in the various referendum campaigns.\n9\nFor various reasons (e.g., missing part), 24 articles could not be coded as to their size and were assigned the median value of all calculated sizes (266 cm2).\n\n 2011 Swiss Political Science Association\n\nSwiss Political Science Review (2011) Vol. 17(2): 128–163\n\n16626370, 2011, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/j.1662-6370.2011.02015.x by Cochrane Israel, Wiley Online Library on [09/12/2022]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\nMedia Coverage of Referendum Campaigns\n\nLionel Marquis et al.\n\n‘‘journalists’’ are identiﬁed as the speaker category are in fact attributable to other actors to\nwhich journalists refer (e.g., partisan sources, government, and employees’ associations).\nBoth speakers and issue statements’ sources were coded into fourteen mutually exclusive\ncategories: (1) parties, politicians, and other party-related groups or individuals; (2) committees; (3) journalists and press agencies; (4) employers’ organizations; (5) employees’\norganizations; (6) economic associations; (7) national government; (8) local government; (9)\nscience- and education-related groups; (10) health-related groups; (11) disabled people’s\ngroups; (12) women’s and families’ groups; (13) other non-proﬁt organizations; (14) other\nactors. Together, categories (1) and (2) represent what we call ‘‘partisan actors’’.10\nThe importance of journalists (category 3) will be compared with that of the other actor\ncategories (both as ‘‘speakers’’ and as sources of issue statements) to assess the degree of\nthe press’ ‘‘source independence’’. Note, however, that the total for any two groups of\nactors may exceed 100 percent since, by deﬁnition, several actors may be considered as the\n‘‘sources’’ of an article or issue statement.\nFurther, we used the same source categories to build our measure of inclusiveness. It is\nsimply the number of discrete categories from which at least one source of issue statements\nis drawn. In some of the forthcoming analyses, we will satisfy ourselves with this ﬁrst measure (NBCAT). However, in order to control for the fact that some sources may be only\nmarginally involved in agenda-building (and inclusiveness may thus be overestimated), we\nalso assessed the degree of ‘‘equality’’ in the total references to sources of issue statements.\nThis was achieved through the Index of Qualitative Variation (IQV):\n\n\nk\nP\n2\n2\nk N  fi\nIQV ¼\n\ni¼1\n\nN2 ðk  1Þ\n\n;\n\nwhere k is the total number of source categories, N is the total number of (weighted) issue\nstatements, and fi is the frequency of all issue statements attributable to source i (N and f\nmay be computed either as absolute numbers or as proportions). IQV scores can be integrated with NBCAT into a standardized measure of inclusiveness: INC=(NBCAT ⁄ 14) ·\nIQV. Somewhat surprisingly, NBCAT and IQV scores are totally uncorrelated at the level of\nballot measures or of separate media outlets (r &lt; .06). In fact, restriction of range in IQV\nvalues (M = 0.87, SD = 0.04, N = 24 ballot measures) explains why NBCAT has a much\ngreater impact in determining INC scores.11 It also justiﬁes using NBCAT alone, for simpliﬁcation purposes, when IQV scores barely vary in cross-temporal and spatial comparisons.\nSubstantive coverage. The ‘‘format’’ of each article was coded following a three-fold distinction: (1) ‘‘opinion articles’’ (i.e., editorials, interviews, op-eds, free columns, or letters to\nthe editor); (2) ‘‘factual articles’’ (i.e., mere reporting); and (3) ‘‘horserace information’’\n(i.e., voting cues by parties and groups, opinion polls and other predictions, coverage of\ncampaign events). The percentage of articles of the ‘‘horserace’’ type is used to assess the\namount of ‘‘non-substantive’’ information. Among ‘‘substantive articles’’, the percentage of\n10\n\nUnlike in other policy areas such as foreign policy (see Marquis 2006), virtually all committees on welfare state\nissues are backed by parties and politicians.\n11\nAt the level of ballots and papers, INC is strongly associated with NBCAT (r &gt; 0.96) but not with IQV scores\n(r &lt; 0.19). This raises the question whether the aggregation rule (unweighted multiplication of the two measures)\nis appropriate, since both NBcat and IQV are important on theoretical grounds. At the empirical level, however, it\nmakes sense not to give more weight to the variable with lesser variation.\n\n 2011 Swiss Political Science Association\n\nSwiss Political Science Review (2011) Vol. 17(2): 128–163\n\n16626370, 2011, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/j.1662-6370.2011.02015.x by Cochrane Israel, Wiley Online Library on [09/12/2022]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n136\n\n137\n\n‘‘opinion’’ and ‘‘factual’’ articles is used to shed light on the ‘‘orientation’’ and ‘‘information’’ functions of press coverage, respectively.\nBias in coverage. Five broad categories aimed to assess the overall thrust of each article,\nbased on the general slant of its arguments: (1) predominantly pro (i.e., suggesting a ‘yes’\nvote on the ballot question); (2) predominantly con; (3) neutral; (4) mixed, i.e., controversial\nand including both sides; and (5) no argument. Similarly, the bias of each issue statement\nwas coded on the basis of the same categories (excepting the ‘‘no argument’’ category,\nwhich is irrelevant). The categories were collapsed across ballot measures or other relevant\nunits of analysis (newspapers, language areas, etc.) and combined to produce a summary\nindicator of coverage bias:\n\n\n\npro þ neu þ mix\nB¼\n 0:5  2;\npro þ con þ 2  ðneu þ mixÞ\nwhere pro is the number of positive items, con is the number of negative items, neu is the\nnumber of neutral items, and mix is the number of controversial items.12 Unbiased coverage\nis denoted by a B value of 0, while B values of –1 and +1 indicate the highest possible bias\nagainst and in favour of some proposal, respectively.\nHowever, B scores are not well adapted for comparing bias in coverage across several\nballot measures, since the meaning of pros and cons depends on how the ballot question\nwas framed. One strategy is to rely on absolute values (i.e., |B| scores) to get an estimation\nof the overall extent of bias, regardless of direction. Another strategy is to devise a measure\nthat expresses bias in terms of endorsement or opposition of welfare state policies. This\nrequires that ‘‘pro’’ and ‘‘con’’ categories be inverted for ballots in the ‘‘retrenchment’’ category, i.e., measures that seek to reduce welfare beneﬁts or to limit welfare expansion. Thus\nan original B score of +0.2 remains unchanged for expansion measures but its polarity is\ninverted and shifted to –0.2 for retrenchment measures. We call this new measure B*. Computing B* scores also implies that ballot measures can be clearly identiﬁed as ‘‘expansion’’\nor ‘‘retrenchment’’ measures. In other words, only media contents that can be clearly\nassigned to ‘‘pro-welfare’’ and ‘‘anti-welfare’’ positions should be considered. This leads us\nto remove three ‘‘populist’’ proposals (ballots 724, 732, and 781; see Table A1) from the\nanalysis whenever media bias is analyzed by means of B* scores.13 Except in this case, however, and unless indicated otherwise, all indicators used in this study are based on the total\nnumber of ballots (i.e., 24).\n\n12\nWe take into account ‘‘neutral’’ and ‘‘mixed’’ items because failing to do so leads to serious distortions in the\ncases where coverage is mostly neutral but where only a few biased stories would result in meaninglessly high scores\nof coverage bias. Thus, for example, a number of positive items twice larger than that of negative items but equal\nto the number of non-valenced (i.e., neutral and controversial) items yields a B of ‘‘only’’ about 0.14. By the same\ntoken, B scores are expected to be more polarized when measured at the level of issue statements rather than at the\nlevel of articles, because in the former case the number of non-valenced items is considerably lower. Note also that\nB is very strongly correlated (r &gt; 0.98) with alternative measures of media bias, such as that used by Zaller and\nChiu (1996).\n13\nFor our purposes, we deﬁne populist proposals as emanating from right-wing nationalist parties and groups,\nand being opposed by other parties. They consist of ‘‘radical’’, often ‘‘simplistic’’ solutions to welfare problems,\nusually charging ‘‘big business’’, high-proﬁt sectors, or available public wealth. However, as these proposals do not\nalign with the traditional ideological cleavages on welfare issues, opposition to them does not imply an endorsement of the welfare system. Accordingly, these proposals are hardly comparable with other proposals.\n\n 2011 Swiss Political Science Association\n\nSwiss Political Science Review (2011) Vol. 17(2): 128–163\n\n16626370, 2011, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/j.1662-6370.2011.02015.x by Cochrane Israel, Wiley Online Library on [09/12/2022]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\nMedia Coverage of Referendum Campaigns\n\nLionel Marquis et al.\n\nSpatial homogeneity. Indicators for each of the six above criteria of media fairness can\nbe compared between the two main cultural areas in order to assess the homogeneity\nof media coverage. Simple diﬀerences are calculated between values for the majority\n(German-speaking) cultural area and those of minority areas. The only Italian-speaking\npaper in our database (i.e., Corriere del Ticino) is considered together with the Frenchspeaking outlets.14\nValidity checks. Each of the two independent coders read half the selected articles and\nmeasured all particular aspects listed above. Intercoder disagreement was examined from a\nsample of all coded articles and settled through discussion, resulting in more ﬁne-grained\nand univocal coding procedures. The issue statements of all the 1088 selected articles were\nthen coded again. Finally, a sample of 24 articles (one for each proposal) was drawn to\ncheck for coding consistency. Intercoder reliability was found to be satisfactory (Cohen’s\nkappa=0.704).15\nWeighting procedures. Except for analyses bearing solely on the sheer number of articles,\ntwo weighting procedures were applied. First, in keeping with the literature on public attention and response (e.g., Neuman 1990; Price and Zaller 1993), the size of articles was logged\nand used as a weighting factor to account for the notion that marginal returns of an\nincrease in news volume are generally diminishing. That is, for example, the diﬀerence\nbetween a ﬁve-inch-squared, barely visible, short article and a half-page article is probably\nmore consequential than the diﬀerence between a full-page and a double-page article.\nSecond, the ‘‘internal length’’ of issue statements was used to weigh their importance within\nour sample.16\n\n4. An overall view of press performance\nTable 1 gives an overview of six of the seven criteria of media fairness presented in the theoretical and measurement sections — the ‘‘spatial homogeneity’’ dimension shall be analyzed\nseparately below.\n\nIntensity of coverage\nThere is little reason to expect journalists to devote equal attention to all subject matters;\nsome issues are of greater concern to them (and to their readers) and are thus more likely\nto ﬁnd their way onto the media agenda. In fact, there is a good deal of variation in the\n\n14\n\nJournalists and political actors from the two minority cultural areas usually share the same general orientation\ntoward welfare politics (though not in other important policy ﬁelds) and are probably closer to each other than\nthey are to journalists and political actors from the German-speaking majority. More importantly, diﬀerences\nbetween majority and all minority areas are more commented upon and likely to be more consequential for\nnational cohesion than diﬀerences between minorities.\n15\nThis is actually a more than ‘‘fair’’ performance according to the standard interpretation by Landis and Koch\n(1977: 165). 67 issue statements were coded by the two coders simultaneously, i.e., an average of 2.8 issues per article. Three issues were coded by only one coder and were removed from the analysis, thus very slightly overestimating intercoder reliability by obfuscating disagreement over the sheer number of issue statements present in a press\narticle. However, when the three issues are taken into account, the level of disagreement rises by a very small\namount, and j is still 0.67.\n16\nFive rough categories approximating the ‘‘internal length’’ of issue statements were recoded into ﬁve ratios\nreﬂecting their share in the whole article: 0.20, 0.35, 0.50, 0.65, and 0.80.\n\n 2011 Swiss Political Science Association\n\nSwiss Political Science Review (2011) Vol. 17(2): 128–163\n\n16626370, 2011, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/j.1662-6370.2011.02015.x by Cochrane Israel, Wiley Online Library on [09/12/2022]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n138\n\n 2011 Swiss Political Science Association\n\n22.5\n\n45.1\n\n23.8\n\n60.0\n\n64.5\n\n74.9\n\n71\n\n171\n\n100\n\n186\n\n238\n\n275\n\n21.1\n\n82\n\n48.9\n\n19.8\n\n83\n\n156\n\n64.7\n\n258\n\n48.9\n\n61.5\n\n216\n\n156\n\n95.1\n\n317\n\n91.7\n\n64.3\n\n185\n\n312\n\n96.7\n\nSurfaceb\n\n301\n\nNumbera\n\n0.757\n\n0.628\n\n0.732\n\n0.505\n\n0.587\n\n0.389\n\n0.444\n\n0.444\n\n0.729\n\n0.488\n\n0.434\n\n0.762\n\n0.618\n\n0.858\n\n0.666\n\n0.694\n\nInclusivenessc\n\n25.9\n\n26.2\n\n26.4\n\n20.9\n\n25.5\n\n22.0\n\n26.4\n\n26.4\n\n26.7\n\n23.3\n\n18.6\n\n24.5\n\n19.5\n\n23.0\n\n22.7\n\n22.7\n\nAverage\ncampaign\ndayd\n\n)0.315\n)0.013\n)0.122\n)0.208\n\n)0.071\n)0.046\n)0.064\n)0.086\n\n0.168\n\n)0.489\n\n)0.255\n\n0.075\n\n)0.139\n\n)0.098\n\n)0.210\n\n)0.176\n\n)0.151\n\n)0.071\n\n0.089\n\n)0.098\n\n)0.103\n\n)0.050\n\n0.249\n\n87.3\n\n)0.108\n\n)0.074\n\n0.054\n\n78.6\n\n)0.042\n\n)0.006\n\n89.8\n\n89.9\n\n92.1\n\n83.0\n\n89.1\n\n93.7\n\n85.5\n\n85.5\n\n81.5\n\n78.6\n\n78.8\n\n81.0\n\n81.6\n\n)0.142\n\n)0.062\n\n84.3\n\n% Journalists\n(articles)f\n\n0.144\n\nIssue\nbiase\n\n0.075\n\nArticle\nbiase\n\n21.0\n\n22.2\n\n4.6\n\n20.1\n\n12.1\n\n20.1\n\n19.6\n\n19.6\n\n21.8\n\n21.8\n\n3.2\n\n22.3\n\n15.2\n\n13.0\n\n21.7\n\n18.5\n\n% Parties\n(articles)f\n\n25.5\n\n17.2\n\n22.9\n\n31.1\n\n28.2\n\n44.9\n\n31.0\n\n31.0\n\n31.2\n\n17.1\n\n15.0\n\n20.4\n\n35.2\n\n22.8\n\n39.4\n\n28.7\n\n% Journalists\n(issues)f\n\n47.7\n\n57.7\n\n29.6\n\n57.1\n\n36.6\n\n23.0\n\n47.1\n\n47.0\n\n44.6\n\n47.5\n\n34.7\n\n42.8\n\n40.4\n\n39.6\n\n38.8\n\n43.7\n\n% Parties\n(issues)f\n\n42.0\n\n36.6\n\n61.9\n\n52.0\n\n63.8\n\n46.7\n\n49.2\n\n48.6\n\n53.6\n\n45.6\n\n35.5\n\n50.6\n\n50.8\n\n47.5\n\n49.2\n\n48.4\n\n% Reportingg\n\n31.7\n\n32.8\n\n33.2\n\n38.6\n\n27.8\n\n42.2\n\n39.9\n\n40.5\n\n35.4\n\n41.2\n\n33.6\n\n41.4\n\n42.2\n\n38.6\n\n36.8\n\n33.7\n\n% Opiniong\n\n26.3\n\n30.6\n\n4.9\n\n9.3\n\n8.4\n\n11.2\n\n10.9\n\n10.9\n\n11.0\n\n13.2\n\n30.9\n\n8.0\n\n7.0\n\n13.9\n\n14.0\n\n17.9\n\n% Horseraceg\n\n139\n\nSwiss Political Science Review (2011) Vol. 17(2): 128–163\n\n16626370, 2011, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/j.1662-6370.2011.02015.x by Cochrane Israel, Wiley Online Library on [09/12/2022]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n571 R ⁄ Reform of pension\nsystem (1995)\n572 I ⁄ ‘‘For extending the\npension system’’ (1995)\n602 R ⁄ Labour (weekend ⁄ night work) (1996)\n622 R ⁄ Unemployment\ninsurance (1997)\n643 I ⁄ Retirement age\n(1998)\n654 R ⁄ Labour (night\nwork, maternity) (1998)\n684 R ⁄ Disability\ninsurance (1999)\n685 R ⁄ Maternity\ninsurance (1999)\n721 I ⁄ Retirement age for\nwomen (2000)\n722 I ⁄ ‘‘Flexible\nretirement age’’ (2000)\n724 I ⁄ ‘‘Reduced hospital\ncosts’’ (2000)\n732 I ⁄ ‘‘Cheaper drugs’’\n(2001)\n752 I ⁄ ‘‘Secure pension\nsystem’’ (2001)\n762 I ⁄ Reduction of work\ntime (2002)\n781 I ⁄ ‘‘Gold to pensions’’\n(2002)\n782 CP ⁄ ‘‘Gold to\npensions’’ (2002)\n\nProject (R=referendum,\nI=initiative,\nCP=counterproposal)\n\nTable 1: Indicators of fairness for media coverage of 24 ballot issues\n\nMedia Coverage of Referendum Campaigns\n\n 2011 Swiss Political Science Association\n\n29.0\n\n48.9\n\n40.3\n\n109\n\n170\n\n131\n\n197.0\n(63.5)\n121.5\n(41.2)\n273.0\n(39.0)\n122.5\n(40.5)\n173.8\n(91.0)\n\nNumbera\n\n57.6\n(19.1)\n34.8\n(12.8)\n76.8\n(14.9)\n33.2\n(12.2)\n51.0\n(29.5)\n\nSurfaceb\n\n52.0\n(R=1248.1)\n22.4\n\n45.4\n\n163\n\n179.3\n(R=4303)\n73.5\n\n49.9\n\n152\n\n61.9\n\n21.8\n\n92\n\n234\n\n47.5\n\nSurfaceb\n\n145\n\nNumbera\n\n0.606\n(0.120)\n0.533\n(0.091)\n0.781\n(0.052)\n0.621\n(0.133)\n0.667\n(0.155)\n\nInclusivenessc\n\n0.131\n\n0.623\n\n0.834\n\n0.475\n\n0.683\n\n0.645\n\n0.754\n\n0.629\n\n0.525\n\n0.674\n\nInclusivenessc\n\n25.0\n(2.1)\n22.9\n(3.5)\n25.6\n(1.1)\n23.9\n(0.6)\n22.8\n(2.8)\n\nAverage\ncampaign\ndayd\n\n2.8\n\n24.1\n\n24.6\n\n27.2\n\n27.4\n\n22.9\n\n24.5\n\n26.5\n\n17.6\n\n26.9\n\nAverage\ncampaign\ndayd\n\n0.079\n(0.021)\n0.138\n(0.076)\n0.073\n(0.019)\n0.121\n(0.054)\n0.051\n(0.030)\n\nArticle\nbiash\n0.135\n(0.070)\n0.300\n(0.179)\n0.353\n(0.103)\n0.308\n(0.098)\n0.061\n(0.042)\n\nIssue\nbiash\n\n0.138\n\n0.182h\n\n0.086h\n0.049\n\n0.456\n\n0.252\n\n0.091\n\n0.114\n\n0.026\n\n0.008\n\n)0.046\n0.081\n\n0.406\n\n)0.008\n\n)0.070\n0.067\n\n0.387\n\n0.156\n\nIssue\nbiase\n\n0.156\n\n0.064\n\nArticle\nbiase\n\n86.3\n(4.5)\n88.3\n(4.2)\n87.6\n(6.1)\n80.5\n(1.9)\n86.4\n(4.0)\n\n% Journalists\n(articles)f\n\n5.2\n\n86.4\n\n93.7\n\n93.1\n\n92.0\n\n85.1\n\n82.4\n\n88.7\n\n81.8\n\n96.0\n\n% Journalists\n(articles)f\n\n20.8\n(1.2)\n17.7\n(3.6)\n17.2\n(4.6)\n19.2\n(2.5)\n8.8\n(4.9)\n\n% Parties\n(articles)f\n\n5.2\n\n17.3\n\n12.6\n\n21.0\n\n21.6\n\n14.3\n\n16.7\n\n17.3\n\n21.3\n\n13.1\n\n% Parties\n(articles)f\n\n25.9\n(7.4)\n28.6\n(10.5)\n28.6\n(2.5)\n17.6\n(0.5)\n23.6\n(6.7)\n\n% Journalists\n(issues)f\n\n7.9\n\n25.8\n\n26.1\n\n21.8\n\n13.1\n\n33.8\n\n18.1\n\n26.0\n\n15.5\n\n22.1\n\n% Journalists\n(issues)f\n\n48.2\n(6.6)\n40.4\n(16.1)\n44.5\n(0.1)\n46.8\n(0.7)\n36.3\n(4.6)\n\n% Parties\n(issues)f\n\n9.5\n\n43.5\n\n44.3\n\n42.5\n\n57.8\n\n41.2\n\n46.0\n\n66.7\n\n35.3\n\n32.5\n\n% Parties\n(issues)f\n\n49.3\n(6.2)\n50.9\n(7.5)\n55.5\n(1.9)\n51.4\n(5.8)\n47.2\n(9.5)\n\n%\nReportingg\n\n7.5\n\n50.6\n\n57.5\n\n58.0\n\n58.3\n\n44.1\n\n57.2\n\n48.0\n\n45.1\n\n64.8\n\n% Reportingg\n\n36.1\n(3.6)\n37.3\n(5.8)\n28.8\n(6.6)\n35.2\n(6.0)\n33.0\n(4.2)\n\n% Opiniong\n\n5.3\n\n35.2\n\n22.2\n\n34.3\n\n31.7\n\n26.6\n\n29.1\n\n37.6\n\n41.8\n\n32.0\n\n% Opiniong\n\n14.6\n(7.6)\n11.8\n(2.2)\n15.7\n(4.7)\n13.4\n(0.2)\n19.7\n(10.8)\n\n% Horseraceg\n\n7.7\n\n14.2\n\n20.3\n\n7.7\n\n10.0\n\n29.3\n\n13.6\n\n14.4\n\n13.1\n\n3.2\n\n% Horseraceg\n\nLionel Marquis et al.\n\nSwiss Political Science Review (2011) Vol. 17(2): 128–163\n\n16626370, 2011, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/j.1662-6370.2011.02015.x by Cochrane Israel, Wiley Online Library on [09/12/2022]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\nLabour regulation (N = 4)\n\nDisability (N = 2)\n\nMaternity (N = 2)\n\nHealth (N = 4)\n\nPensions (N = 10)\n\nMeans for categories of\nproject (standard\ndeviations in parentheses)\n\nStd. dev.\n\n792 R ⁄ Unemployment\nbeneﬁts (2002)\n802 R ⁄ Financing of\nhospital treatments (2003)\n815 I ⁄ ‘‘Health must\nremain aﬀordable’’ (2003)\n816 I ⁄ ‘‘Equal rights for\ndisabled’’ (2003)\n819 I ⁄ ‘‘Apprenticeship\nplaces’’ (2003)\n831 R ⁄ Increase of\npension age for women\n(2004)\n832 R ⁄ Financing\npensions through VAT\n(2004)\n844 R ⁄ Maternity\ninsurance (2004)\nMean (N = 24)\n\nProject (R=referendum,\nI=initiative,\nCP=counterproposal)\n\nTable 1: (Continued)\n\n140\n\n 2011 Swiss Political Science Association\n\n180.5\n(35.5)\n205.2\n(83.5)\n160.0\n(68.6)\n172.8\n(67.3)\n189.4\n(87.7)\n162.1\n(51.1)\n\nNumbera\n\n54.5\n(7.0)\n61.8\n(26.9)\n44.0\n(17.1)\n49.7\n(20.0)\n55.1\n(27.9)\n47.3\n(14.6)\n\nSurfaceb\n0.646\n(0.028)\n0.669\n(0.110)\n0.535\n(0.104)\n0.622\n(0.134)\n0.638\n(0.137)\n0.599\n(0.122)\n\nInclusivenessc\n23.2\n(3.7)\n23.8\n(2.7)\n24.6\n(1.8)\n24.1\n(2.9)\n23.4\n(3.4)\n24.6\n(1.9)\n\nAverage\ncampaign\ndayd\n0.069\n(0.005)\n0.079\n(0.050)\n0.137\n(0.083)\n0.079\n(0.029)\n0.089\n(0.045)\n0.084\n(0.054)\n\nArticle\nbiash\n0.132\n(0.024)\n0.114\n(0.064)\n0.337\n(0.116)\n0.178\n(0.140)\n0.191\n(0.132)\n0.175\n(0.149)\n\nIssue\nbiash\n87.4\n(8.6)\n85.1\n(6.7)\n90.9\n(2.0)\n86.0\n(4.5)\n86.2\n(6.2)\n86.3\n(4.3)\n\n% Journalists\n(articles)f\n14.2\n(1.1)\n17.2\n(3.7)\n18.1\n(4.4)\n17.1\n(5.9)\n16.7\n(5.6)\n17.5\n(4.9)\n\n% Parties\n(articles)f\n28.7\n(6.6)\n23.2\n(7.3)\n30.1\n(11.4)\n25.9\n(6.8)\n22.6\n(6.8)\n28.7\n(8.0)\n\n% Journalists\n(issues)f\n36.4\n(3.9)\n43.6\n(7.8)\n39.1\n(14.3)\n44.4\n(8.7)\n42.1\n(6.7)\n44.5\n(11.7)\n\n% Parties\n(issues)f\n57.8\n(7.0)\n52.6\n(6.8)\n49.0\n(11.2)\n50.2\n(6.7)\n51.4\n(7.8)\n50.7\n(7.2)\n\n%\nReportingg\n\n37.1\n(5.1)\n36.6\n(4.3)\n34.2\n(6.0)\n34.8\n(5.5)\n35.1\n(5.5)\n35.5\n(5.2)\n\n% Opiniong\n\n5.1\n(1.9)\n10.9\n(4.8)\n16.7\n(9.9)\n15.0\n(7.8)\n13.5\n(7.2)\n13.8\n(7.7)\n\n% Horseraceg\n\n141\n\nSwiss Political Science Review (2011) Vol. 17(2): 128–163\n\n16626370, 2011, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/j.1662-6370.2011.02015.x by Cochrane Israel, Wiley Online Library on [09/12/2022]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\nNotes: a: Number of articles. b: Surface in standard newspaper pages (one page=1247 cm2). c: Number of actor categories · IQV (see text for calculation details). d: Mean number of days remaining before voting day. e: B scores comprised between -1 (all articles ⁄ issues against project) and\n+1 (all articles ⁄ issues in favour of project); see text for calculation details. f: Sum of categories ‘‘journalists’’ (including press agencies) and ‘‘parties’’ (including referendum ⁄ initiative committees) may exceed 100%, since several actors may be considered as the ‘‘sources’’ of an article. g: Percentage of articles in each category (see text for deﬁnition). h: Means computed from absolute values for each ballot, i.e. |B|. i: Not considered\nhere: counterproposal to the ‘‘gold’’ initiative (ballot #782; see Appendix).\n\nInitiative (N = 12)i\n\nExpansion proposal\n(N = 15)\nReferendum (N = 11)i\n\nRetrenchment proposal\n(N = 6)\nPopulist proposal (N = 3)\n\nUnemployment (N = 2)\n\nMeans for categories of\nproject (standard\ndeviations in parentheses)\n\nTable 1: (Continued)\n\nMedia Coverage of Referendum Campaigns\n\nLionel Marquis et al.\n\nintensity of media coverage of the various ballot issues (SD is about 22 full newspaper\npages for an overall mean of 52 pages). As shown in Table 1, the intensity of press coverage\nvaries between the speciﬁc policy ﬁelds subsumed under the heading ‘‘welfare state issues’’.\nMaternity seems by far the most important category of issues; in contrast, health and disability are of less importance. Further, it seems that retrenchment proposals lead to more\nintense campaigns than expansion or populist proposals. This is understandable, since cutting back beneﬁts may lead to more mobilization by target groups than the promise of\ndeveloping beneﬁts can possibly achieve. As noted above, though, the two proposals to\nintroduce a maternity insurance are a notable exception in this regard as they elicited substantial press coverage, probably due to the fact that the constitutional mandate to implement a maternity leave program dated back to the end of World War II and thus the issue\nwas a recurring one.17 Likewise, referendums are somewhat more covered by the press than\ninitiatives, probably because they bear more frequently on retrenchment programs, but the\ndiﬀerence is modest.\nIn brief, the intensity of press coverage appears to vary between the diﬀerent policy ﬁelds,\narguably as a result of the journalists’ intrinsic interest in them and due to the varying\nmobilization of political and civil society actors. But, to a large extent, campaign intensity\nis probably also exogenously determined by the uncertainty of the result of the ballot. Both\ncampaigners and voters are more likely to get involved in referendums when the margin of\nvictory is perceived to be small (e.g., Downs 1957; Kirchgässner and Schulz 2005). Likewise,\nin our dataset the overall intensity of press coverage is substantially correlated with the\ncloseness of the voting results.18 The closer the (expected) outcome, the more journalists\nreport on issues.\n\nSource inclusiveness\nOne indicator of the fairness of the coverage of a ballot issue is how many diﬀerent\npolitical actors are involved in the deliberation process. Our standardized measure of\ninclusiveness points to a respectable diversity in the sources of issue statements in press\ncoverage (overall mean=0.62). It is highest for maternity issues, and lowest for public\nhealth issues, as well as for issues comprised in the ‘‘populist’’ category. We thus assume\nthat inclusiveness depends on the nature of issues (see Pfetsch 2004: 87–93), as further\nsuggested by the fact that campaign intensity varies between policy ﬁelds, as we have\nseen. Indeed, intensity was shown to depend strongly on inclusiveness, as far as advertisement campaigns are concerned (Marquis and Bergman 2009). Similarly, in press coverage, the correlation between our indicators of intensity and inclusiveness is fairly high\n(Pearson’s r = 0.76). In other words, the more political actors enter the ﬁeld (and journalists’ accounts presumably reﬂect this increasing diversity), the more likely the issue at\nhand is to receive general attention from the press — whether because inclusiveness\ndetermines intensity or because both coverage features are determined by some higherlevel concept such as ‘‘issue importance’’.\n\n17\n\nPrior to the 1999 and 2004 votes, no less than three proposals to create a maternity insurance had already been\nrejected at the polls (Dec. 8, 1974; Dec. 2, 1984; and Dec. 6, 1987).\n18\nThe correlation coeﬃcient is –.54 for the number of articles and –.48 for total surface. Closeness is measured as\nthe absolute diﬀerence between the actual result and a perfectly balanced result: CLOSENESS = | 50 – RESULT |.\n\n 2011 Swiss Political Science Association\n\nSwiss Political Science Review (2011) Vol. 17(2): 128–163\n\n16626370, 2011, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/j.1662-6370.2011.02015.x by Cochrane Israel, Wiley Online Library on [09/12/2022]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n142\n\n143\n\nLength of coverage\nCampaigns in Swiss direct democracy, whether through paid advertisements or in press\nreporting, usually unfold in two phases: there is ﬁrst an accelerating expansion of coverage\nand then a sharp decline in the last days before the vote (e.g., Marquis 2006: 429–438;\nTresch 2008: 147–149). Based on all articles published during all 24 campaigns, a thirdorder polynomial function modelling this typical development accounts for 67 percent of\nthe variance in the total daily surface of articles.19 Hence, at the aggregate level, the twophase pattern ﬁts the data quite well: referendum campaigns on welfare state issues develop\nin a typical way, similar to what has been observed for other types of issues.20 In addition,\nall policy ﬁelds follow roughly the same pattern — the only notable diﬀerence is that campaigns on health matters (including maternity and disability) reach their peak earlier than\nother types of issues.\nThese ﬁndings may dispel concerns that most media information on ballot issues is\nreleased too fast and too late for citizens to use it eﬃciently in their decision making. But\nactually how long is the press coverage of ballot issues? Drawing on our indicator of the\n‘‘average campaign day’’, the ‘‘average information’’ is published some 24 days before the\nvoting date (i.e, it corresponds to the ‘‘mean average day’’ for all 24 ballots). In the same\nperspective, the average median day is 21.6; this means that, in a typical campaign, half of\nthe total (weighted) sum of all information has been released three weeks before voting day,\nand the other half is to be released in the remaining weeks. In addition, there appears to be\nlittle diﬀerence between the various ballot issues — even though ballot issues that draw\nmore intense coverage (pensions, maternity) also tend to be covered for a slightly longer\ntime. To be sure, actual exposure to the ﬂow of information delivered during referendum\ncampaigns may greatly vary from one citizen to another, depending for example on which\nmedia outlet (if any) they use for their information (see below). But our ﬁndings are hardly\ncompatible with the argument that citizens are too time-pressed to make up their minds\nand are not given a chance to learn what the ballot issues are all about.\n\nSource independence\nThe question of media independence and autonomy is important, both from the perspective\nof media practices and ethics and for the purposes of deliberative processes. For example,\nTresch (2008: 142–149) shows that ‘‘agenda-building’’ by political actors through press conferences, party meetings, and other public relations activities, constitutes a substantial part\nof media content in referendum campaigns. Even though such coverage is certainly not\nwithout merit for citizens’ information and orientation, an overly reliance on external\nagenda-building eﬀorts may lead to media instrumentalization beyond that which stems\nfrom patterns of media ownership (see Hallin and Mancini 2004).\n19\nThe following function was ﬁtted to the data: Surface = 0.0015 time3 – 0.1464 time2 + 3.7937 time + 4.3653,\nwith surface measured as a three-day moving average and time measured as the number of days remaining until\nvoting day. The proportion of explained variance rises to 80 percent when Sunday editions are removed. Because\nof their non-continuous time structure, such editions introduce ‘‘noise’’ into the data. However, only ﬁve outlets\nare weekly papers or are published on Sunday, and only 2 percent of articles were published on that weekday.\n20\nAt the level of each single ballot, the characteristic sigmoid shape can be found for all but four campaigns. With\nthe exception of ballots 572, 622, 781, and 782, all functions have positive values for time3 and time, and negative\nvalues for time2. Article size (rather than number) was used, and time was recoded in eight weekly periods to avoid\nhigh ﬂuctuation in daily values. The average R2 for all 24 campaigns is 0.61 (SD=0.23).\n\n 2011 Swiss Political Science Association\n\nSwiss Political Science Review (2011) Vol. 17(2): 128–163\n\n16626370, 2011, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/j.1662-6370.2011.02015.x by Cochrane Israel, Wiley Online Library on [09/12/2022]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\nMedia Coverage of Referendum Campaigns\n\nLionel Marquis et al.\n\nIf we ﬁrst consider journalists and partisan actors (i.e., parties and committees) in their\nrole of ‘‘speakers’’ at the level of articles, it comes as no surprise that journalists outweigh\npartisan actors (86 percent vs. 17 percent, on average). More remarkably, the diﬀerences\nbetween the various ballot proposals are quite limited. Journalists were identiﬁed as ‘‘speakers’’ in 80 to 88 percent of all articles in each of the six policy ﬁelds, while the corresponding interval is 14 to 21 percent for partisan actors — with the notable exception of labour\nmarket policies, where the parties’ share is only 9 percent.21 As for governmental speakers,\nthey appear in less than 10 percent of articles in all policy ﬁelds. (Note that the percentages\nfor the various categories add up to more than 100 percent, since any article can draw on\nseveral types of speakers.)\nAt the level of issue statements, recall that a total of 2859 issues were coded out of our\nsample of 1088 articles. Parties and committees account for about 44 percent of all coded\nissue statements, journalists and press agencies for 26 percent, governmental actors for 17\npercent, employers’ and employees’ organizations for 14 percent, and other types of actors\nfor 20 percent (similar to articles, percentages do not add to 100 percent because several\ncategories of actors can be involved as sources of one issue).22 Overall, then, journalists\nappear much less important as a source of issue statements than as a source of articles\n(i.e., as ‘‘speakers’’), which makes perfect sense. Conversely, even though partisan actors\nare rarely directly involved as speakers (e.g., through interviews or op-eds), they largely\nsucceed in inﬂuencing campaign news by having their issue statements reported in the press\n(Bonfadelli and Blum 2000; Tresch 2008).\n\nBias in media coverage\nThe extent of bias in press coverage can also be investigated at two levels: at the level of\nwhole articles and at the level of issue statements. The analysis draws in both cases on the\nclassiﬁcation of newspapers’ content into partial (pro or con), neutral, and controversial\ncategories. The resulting B scores theoretically vary between –1 (all content is against ballot\nproposal) and +1 (all content is in favour of proposal). B scores are generally higher for\nissues than for articles, because, for one thing, there are far less ‘‘neutral’’ and ‘‘controversial’’ items (about 27 percent) in issue statements than at the level of whole articles (about\n68 percent).\nA striking feature of our results in Table 1 is the relative neutrality with which Swiss\njournalists report on the issues, at least taken collectively — the question of diﬀerences\nbetween media outlets will be addressed below. The mean absolute B value is 0.09 for\narticles and 0.18 for issue statements. In fact, a good deal of this ‘‘directional thrust’’\nin media reporting is concentrated on ballot issues dealing with health matters (public\nhealth, maternity, disability). For example, the initiatives for ‘‘reduced hospital costs’’\n(2000) and ‘‘equal rights for disabled people’’ (2003), as well as the referendum to\noppose paid maternity leaves (2004), stirred up considerable criticism or enthusiasm\nfrom journalists, leading them to take a clear position on these subjects. On closer\n21\n\nIn this domain, parties are secondary to other representative bodies with which they often have strong ties, in\nparticular employers’ and employees’ associations (15 percent).\n22\nThese shares expectedly vary across policy ﬁelds. Journalists are most involved in unemployment, public health,\nand maternity issues (all 29 percent) and least involved with respect to disability (18 percent). Parties and committees are much quoted sources of statements on pensions and disability (48 and 47 percent) and less so on unemployment and labour regulation (36 percent).\n\n 2011 Swiss Political Science Association\n\nSwiss Political Science Review (2011) Vol. 17(2): 128–163\n\n16626370, 2011, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/j.1662-6370.2011.02015.x by Cochrane Israel, Wiley Online Library on [09/12/2022]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n144\n\n145\n\ninspection, it appears that the general type of ballot proposals combines with their subject matter. As a category, the three populist proposals of the years 2000–2002 (including hospital costs) were treated in a far less even-handed manner than retrenchment or\nexpansion proposals, especially as concerns bias in issue statements.23 We may summarize these ﬁndings by saying that journalists, most of the time, report about ballot\nissues in a quite balanced way.\nInterestingly, bias in press coverage is substantially related to several other features examined here (correlations of 0.4 or higher). To begin with, article bias decreases as coverage\nintensity and inclusiveness increase. In other words, imbalance in coverage looms large\nwhen journalists have little to report and rely on fewer sources. At the level of issue statements, a crucial link seems to be with the importance of partisan actors as sources of statements: the more parties and committees are referred to as a basis for issue analysis, the\nlesser the bias.\nUsing B* values we can also determine how media coverage is basically oriented\ntoward welfare state schemes (i.e., the extent to which campaign news systematically\nsupports welfare state expansion ⁄ protection rather than welfare state limitation ⁄\nretrenchment). In so doing, we obtain a mean B* value of 0.01 for articles and 0.06\nfor issues — the press thus displays a very slight pro-welfare bias. This strongly suggests that there is no overall systematic pro-welfare or anti-welfare bias in press coverage. However, diﬀerences exist between the various policy ﬁelds. While neutral with\nrespect to pensions, labour market and unemployment, Swiss media outlets as a whole\nseem committed to more generous social protection programmes in the area of public\nhealth (B*=0.19 for issue statements), including maternity and invalidity (B*=0.35 and\n0.31, respectively).\nIt should also be noted that editorials and interviews are more critical of the welfare state\nthan other format types. B* scores (issue statements) for editorials and interviews are about\n–0.2 and –0.1, respectively, as compared to +0.1 or higher for reports, letters to the editor,\nand campaign news. Journalists opposed to welfare policies, it would seem, rely heavily on\nwriting editorials and selecting anti-welfare interviewees. But this does not suﬃce to countervail the bulk of pro-welfare coverage that stems from ‘‘factual reporting’’ or coverage of\ncampaign events. Besides, the general thrust of media coverage is highly dependent on\nwhich types of actors get their messages across in the various newspapers, since more often\nthan not journalists are balanced in their own issue statements (B* = –0.02). For instance,\nin the many cases where the parties’ issues ﬁnd their way into the media, center-left parties\nare much more likely to voice pro-welfare issues than are right parties (B* = 0.42 vs.\n–0.26). Similarly, the bias brought about by employees’ organizations counterbalances that\nof employers’ organizations (B* = 0.35 vs. –0.29). Overall, referendum committees are relatively neutral toward the welfare system (B* = 0.04), but the organizations representing\nspeciﬁc population groups (retired people, disabled people, youth, women, families) are\noverwhelmingly pro-welfare. Interestingly, the bias exhibited by governmental sources is\nquite diﬀerent depending on the level of government. Institutions and civil servants at the\nnational level (e.g., federal councillors, senior oﬃcials) are clearly less supportive of the\n\n23\n\nThe 2000 and 2001 initiatives (‘‘reduced hospital costs’’ and ‘‘cheaper drugs’’) were fought by all parties, while\nthe 2002 initiative (‘‘gold reserves to retirement pensions’’) was launched by the Swiss People’s Party and backed\nby other smaller populist right parties. Accordingly, the latter was supported by an important minority of 46% of\nthe popular votes, compared to 18% and 31% for the two earlier initiatives.\n\n 2011 Swiss Political Science Association\n\nSwiss Political Science Review (2011) Vol. 17(2): 128–163\n\n16626370, 2011, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/j.1662-6370.2011.02015.x by Cochrane Israel, Wiley Online Library on [09/12/2022]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\nMedia Coverage of Referendum Campaigns\n\nLionel Marquis et al.\n\nwelfare system (or, at least, of welfare state expansion) than are their counterparts at the\ncantonal or local level (B* = –0.15 vs. 0.48).24\n\nSubstantive coverage\nWe focus here on the three main formats of campaign coverage (i.e., ‘‘opinion articles’’,\n‘‘factual reporting’’, and ‘‘horserace information’’) to determine how ‘‘substantive’’ campaign coverage is. Concerns are often expressed that media news that unduly revolves\naround the horserace and ‘‘game’’ aspects of campaigns has a deleterious inﬂuence on public deliberation and citizens’ decision-making. In fact, horserace information was present in\nall campaigns but it is quite weak (14 percent of the overall amount of news, on average)\ncompared to ‘‘opinion articles’’ (35 percent) and ‘‘factual articles’’ (51 percent). Horserace\ncoverage varies between a minimum of 5 percent for unemployment ballot issues and a\nmaximum of 20 percent for labour regulation ballot issues. In comparison, the share of\nopinion articles varies between 29 and 37 percent depending on policy domains, while the\nshare of factual articles varies between 47 and 58 percent. Thus, unlike in other contexts,\nthe horserace is clearly not the leading theme in Swiss referendum campaigns.25\n\n5. A dynamic view of independence, inclusiveness, bias, and substance\nThe picture thus far supports the notion of rather ‘‘fair’’ journalistic practices, as media\ncoverage appears to be relatively balanced, autonomous and substantive. We now examine\nthe hypothesis that journalists are however not immune to the pressure of the political environment and that they become increasingly dependent on external and ⁄ or biased sources of\ninformation over the course of campaigns. In other words, we ask whether the ‘‘fairness’’ of\ncampaign coverage is aﬀected by agenda-building eﬀorts of political actors. We thus investigate the dynamics of campaign coverage, focusing on four aspects examined above: source\nindependence and inclusiveness, bias in coverage and substantive coverage. We believe that\nthe time dimension can add to our understanding of how much journalistic output is autonomous, inclusive, unbiased, and substantive — and possibly also why it deviates from such\nnorms.\nBeginning with source independence, one interesting result is that journalists are the only\nactor category whose importance as a source of issue statements increases throughout the\n24\n\nThis diﬀerence is consistent across all types of issues, excepting maternity, and it may be explained in two ways.\nOn the one hand, the Swiss federal political system is largely based on the ‘‘principle of subsidiarity’’, which posits\nthat the federal state should assume only those tasks which cannot be performed eﬀectively at a more local level.\nHowever, the federal state assumes the greatest responsibility for managing social beneﬁts and ﬁnancing pension\nand health insurances, while local governments may feel less under pressure. In addition, cantons and municipalities have to draw on their own resources to provide social assistance to needy people who fall through the cracks\nof the social safety net and are no longer eligible for national welfare beneﬁts. Accordingly, local authorities are\nless likely to endorse cutbacks in social beneﬁts and are more inclined to support their constituencies’ demands for\nsocial services. On the other hand, the principle of collegiality implies that federal councillors from left parties are\ncompelled to advocate the view of the government as a collective body even when it contradicts the position of\ntheir party or their own opinion. For example, Federal Councillor Ruth Dreifuss, member of the Social Democratic Party and head of the Federal Department of Home Aﬀairs (1994–2002), often had to oppose initiatives\nfrom left groups aiming at expanding or introducing new welfare programmes.\n25\nThis is all the more signiﬁcant as the deﬁnition of horserace information used in this study is rather broad. As a\nmatter of fact, voting cues by parties and major organizations account for the bulk of ‘‘horserace information’’ as\noperationalized here — rather than opinion surveys or descriptions of campaign events.\n\n 2011 Swiss Political Science Association\n\nSwiss Political Science Review (2011) Vol. 17(2): 128–163\n\n16626370, 2011, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/j.1662-6370.2011.02015.x by Cochrane Israel, Wiley Online Library on [09/12/2022]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n146\n\n147\n\n100\n\n10\n\n90\n\n9\n\n80\n\n8\n\n70\n\n7\n\n60\n\n6\n\n50\n\n5\n\n40\n\n4\n\n30\n\n3\n\n20\n\n2\n\n10\n\n1\n\n0\n\nNumber of source categories\n\nPercent of total sources\n\nFigure 1: Source independence and inclusiveness over time.\n\n% journalists (articles)\n% partisan (articles)\n% journalists (issues)\n% partisan (issues)\nNumber of source\ncategories (issues)\n\n0\n8\n\n7\n6\n5\n4\n3\n2\nWeeks remaining until voting day\n\n1\n\nwhole campaign period, even though their share as ‘‘speakers’’ actually declines (see Figure 1). In contrast, parties and committees become a bit less marginal as speakers (gaining\non average some 10 percent of the total share in the last seven weeks). But, most notably,\npartisan actors remain a major source of issue statements for press journalists during the\nwhole campaign period; their share varies between 33 and 61 percent, but without clear\nupward or downward trend over time.26 Just as signiﬁcant, however, is the fact that the\nsheer number of actor categories used as sources of issue statements grows almost linearly\nduring the campaign period — except in the very last weeks — and actually almost doubles\n(from 3 to 6) in the ﬁrst six weeks (see again Figure 1, right-hand axis). To use our terminology, this shows that press coverage becomes more inclusive as time goes by.27 For example, actors that may be regarded as ‘‘outsiders’’ (science and health professionals,\nassociations defending the rights of women ⁄ families ⁄ disabled people, non-proﬁt organizations, and other similar actors) rise from an average number of 0.7 (8th week before vote) to\nan average of 1.5 (4th week) and manage to keep their share of issue statements at just\nunder 20 percent throughout the campaign.\nTaken together, these results suggest that the dynamics of campaign coverage on welfare\nstate issues is hardly compatible with a radical understanding of the ‘‘determination theory’’. Our analysis reveals that journalists maintain their control over the newsgathering\nprocess and that no single source of campaign issue statements holds sway. As Figure 2\nshows, however, this does not mean that journalists downplay the inﬂuence of political\nactors — or even try to do so in a collective sense. In fact, press coverage increasingly takes\nthe form of ‘‘opinion articles’’ over time, while the share of ‘‘factual reporting’’ sharply\ndeclines. In the eight-week period, opinion articles become at least twice more frequent in\nrelative terms (from 22 to 52 percent), while the share of factual articles considerably\ndecreases (from 69 to 26 percent). With two exceptions, all campaigns exhibit this pattern,\n26\nThe same holds, albeit to a smaller extent, for governmental actors, who account for over one quarter of all\nissue statements in the ﬁrst four campaign weeks, but whose importance then declines to less than 15 percent. Yet,\nas compared to their tiny share of 5 percent of articles as ‘‘speakers’’, governmental actors do succeed in getting\ntheir issues on the campaign agenda, especially at the earlier stage.\n27\nFor the sake of simplicity, Figure 1 displays only the number of issue source categories, because IQV values\nhardly vary (min: 0.85, max: 0.90) and thus the variations over time of the composite index INC boil down to variations in the number of source categories.\n\n 2011 Swiss Political Science Association\n\nSwiss Political Science Review (2011) Vol. 17(2): 128–163\n\n16626370, 2011, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/j.1662-6370.2011.02015.x by Cochrane Israel, Wiley Online Library on [09/12/2022]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\nMedia Coverage of Referendum Campaigns\n\nLionel Marquis et al.\n\nFigure 2: Information format over time (substantive vs. non-substantive content).\n100\nPercent of total sources\n\n90\n80\n70\n% reporting\n\n60\n\n% opinion\n\n50\n40\n\n% horserace\n\n30\n20\n10\n0\n8\n\n7\n6\n5\n4\n3\n2\nWeeks remaining until voting day\n\n1\n\nwhereby opinions gain prominence at the expense of the presentation of facts as voting day\ndraws nearer. This may be related to another ﬁnding stressed above, namely that ‘‘speakers’’, i.e. those actors accountable for the content of an article, comprise less journalists and\nmore partisan actors as time goes by. As for horserace news, its salience ﬂuctuates within a\nnarrow 9–16 percent range –– with the understandable exception of the last campaign week,\ngiven the close proximity of the vote and mobilization eﬀorts by parties and other groups.28\nIn sum, there does not seem to be a strong focus or even focalization on the horserace during referendum campaigns in Switzerland. To a large extent, the overall evolution in the format of information is understandable, and some observers may ﬁnd it comforting from a\nnormative perspective. It can be argued that, as a collective actor, journalists ﬁrst proceed\nto present the facts and main arguments (i.e., information function) and then provide citizens with particular opinions to help them take position on the issues (i.e., orientation function).\nLikewise, the ‘‘tone’’ of media coverage changes over time. To begin with, articles become\nmore ‘‘partial’’, i.e., they increasingly take sides for or against the various ballot proposals\n(see Figure 3). The share of partial articles amounts to slightly more than 20 percent in the\nﬁrst three weeks and approaches 50 percent by the last two weeks.29 In contrast, the share\nof partial issue statements — even though much higher than that of articles in overall level\n— slightly decreases over time. To a large extent this stems from the fact that issue statements get more ‘‘controversial’’ (rather than more ‘‘neutral’’) as voting day draws closer,\nand this is probably reﬂective of the growing recognition by journalists of the complexity of\nballot issues. That the press becomes more committed to its opinion-giving and orientation\nfunction in the later stages of campaigns does not entail, however, that it provides an\nincreasingly biased picture of the ballot issues. As shown in Figure 3 (right-hand axis), it is\nstriking how little deviation there is from perfect neutrality (i.e., B*=0) throughout the\neight campaign weeks, based either on articles or on separate issue statements. In fact, 87\npercent of the weekly bias scores for articles are comprised in a range of ± 20 percent\n28\nWe may also add that most newspapers publish a summary list of all parties’ and important organizations’ voting cues in the last week before voting day (which we also included in the horserace category).\n29\nOne natural hypothesis is that journalists themselves become less impartial over time. There is strong evidence\nsupporting this assumption, as the share of partial items among journalists’ articles rises from a low 16 percent\nseven weeks before voting day to a full 44 percent in the last campaign week.\n\n 2011 Swiss Political Science Association\n\nSwiss Political Science Review (2011) Vol. 17(2): 128–163\n\n16626370, 2011, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/j.1662-6370.2011.02015.x by Cochrane Israel, Wiley Online Library on [09/12/2022]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n148\n\n149\n\n100\n\n1.0\n\n90\n\n0.8\n\n80\n\n0.6\n\n70\n\n0.4\n\n60\n\n0.2\n\n50\n\n0.0\n\n40\n\n–0.2\n\n30\n\n–0.4\n\n20\n\n–0.6\n\n10\n\n–0.8\n\n0\n\nArticle/issue bias (B* score)\n\nPercent of total\n\nFigure 3: Bias in media coverage over time.\n\nArticle partiality\n(pro+con/total)\nIssue partiality\n(pro+con/total)\nArticle bias\n(B* score)\nIssue bias\n(B* score)\n\n–1.0\n8\n\n7\n6\n5\n4\n3\n2\nWeeks remaining until voting day\n\n1\n\naround the neutral value.30 In sum, the fact that campaigns become more partisan over time\ndoes not necessarily imply that one side comes to prevail over the other. More often than\nnot media campaign coverage remains remarkably balanced until the end. As we have\nshown above, ballot campaigns usually heat up in the ﬁnal weeks, as agenda-building\neﬀorts by partisan groups intensify. But even then there is very little to suggest that the\npress gives in to external pressure and leaves the ‘‘undecided’’ citizens or ‘‘late deciders’’\nexposed to the unchecked inﬂuence of partisan or special interests.\n\n6. A particularistic view of media fairness\nBased on a number of formal criteria, the press coverage of ballot issues on welfare state\nissues appears relatively ‘‘fair’’. However, such evidence may be somehow illusory if it stems\nfrom the aggregation of contradictory patterns from diﬀerent types of media outlets. For\nexample, a balanced, unbiased portray of ballot issues at the aggregate level may conceal\nmuch greater variation between individual papers, some of which may be slanted toward\nwelfare state expansion while others may be committed to welfare retrenchment. As most\ncitizens read only one newspaper on a regular basis, this would mean that a majority of\npotential voters are exposed to a one-sided communication ﬂow. In addition, if the papers\non one side of the fence (e.g., the ‘‘pro-welfare camp’’) have a larger overall readership than\nthose on the other side, then the societal balance of information would no longer be guaranteed, despite indications to the contrary such as those provided in the preceding sections.\nTo investigate this question, we ask how the coverage of welfare state issues compares\nacross media outlets. Table 2 displays the whole range of indicators used thus far and shows\nhow they vary between the 28 media outlets included in this analysis.\n30\nThe same result is obtained if one takes absolute B scores (and hence all 24 campaigns are considered). However, when issue statements are considered instead of articles, the proportion of biased weekly values (|B|&gt;0.2) rises\nto 52 percent. To some extent this stems from our sampling procedure, because many weekly values are derived\nfrom a limited number of cases — while the whole data is used for articles’ bias. Accordingly, compared to statistics based on exhaustive data, there is a heightened probability that more extreme values of issue bias are produced\nby chance alone. Moreover, these frequent deviations from neutrality are in general temporary and unlikely to last\nfor longer than one week. In fact, the percent of adjacent weeks that exhibit a similar biased value (e.g., B scores\ngreater than 0.2 for both weeks) is only 19 percent.\n\n 2011 Swiss Political Science Association\n\nSwiss Political Science Review (2011) Vol. 17(2): 128–163\n\n16626370, 2011, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/j.1662-6370.2011.02015.x by Cochrane Israel, Wiley Online Library on [09/12/2022]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\nMedia Coverage of Referendum Campaigns\n\n115.2\n\n97.6\n83.1\n80.1\n83.7\n77.7\n73.5\n58.4\n70.9\n68.0\n56.9\n42.6\n\n57.4\n35.2\n33.5\n37.7\n24.9\n26.8\n23.0\n\n19.9\n18.5\n\n18.9\n\n14.2\n8.6\n\n6.3\n\n5.7\n6.0\n\n321\n293\n286\n285\n263\n245\n222\n208\n201\n179\n170\n\n166\n165\n122\n104\n97\n90\n89\n\n70\n60\n\n38\n\n33\n21\n\n18\n\n17\n7\n\nSurfaceb\n\n528\n\nNumbera\n\n 2011 Swiss Political Science Association\n0.202\n0.194\n\n0.206\n\n0.143\n0.277\n\n0.222\n\n0.458\n0.752\n\n0.781\n0.750\n0.484\n0.745\n0.696\n0.409\n0.687\n\n0.873\n0.845\n0.865\n0.816\n0.664\n0.874\n0.866\n0.837\n0.783\n0.818\n0.840\n\n0.891\n\n22.8\n32.0\n\n16.3\n\n26.7\n38.7\n\n30.9\n\n27.4\n17.0\n\n26.1\n16.8\n14.6\n22.1\n22.5\n21.9\n20.6\n\n26.0\n27.2\n24.5\n26.0\n26.2\n28.0\n17.3\n26.4\n24.6\n28.7\n18.0\n\n27.3\n\nAverage\ncampaign\ndayd\n\n0.076\n0.000\n\n)0.200\n\n)0.026\n0.054\n0.106\n\n0.664\n0.051\n\n0.435\n\n0.142\n0.003\n\n0.164\n0.448\n)0.106\n0.206\n)0.007\n)0.124\n0.126\n\n0.675\n0.117\n\n0.425\n\n0.066\n)0.108\n\n0.038\n0.145\n)0.094\n0.012\n)0.026\n0.063\n0.046\n\n0.118\n)0.030\n0.153\n0.133\n0.023\n0.111\n)0.014\n0.057\n0.009\n)0.128\n0.040\n\n)0.279\n\n)0.127\n0.044\n)0.018\n0.011\n0.063\n)0.032\n)0.023\n)0.023\n0.006\n0.004\n)0.027\n)0.038\n\nIssue\nbiase\n\nArticle\nbiase\n\n87.3\n100.0\n\n100.0\n\n82.3\n81.2\n\n82.8\n\n84.9\n86.6\n\n91.0\n54.6\n72.3\n92.2\n90.2\n82.9\n91.9\n\n81.4\n93.6\n85.3\n96.4\n83.0\n98.8\n81.2\n91.4\n91.4\n90.0\n75.4\n\n81.0\n\n% Journalists\n(articles)f\n\n19.0\n34.2\n\n6.8\n\n7.6\n9.4\n\n11.2\n\n26.7\n14.6\n\n17.8\n35.6\n33.0\n11.8\n7.9\n24.1\n8.7\n\n20.9\n14.2\n16.7\n8.8\n25.1\n8.5\n25.7\n18.5\n10.8\n25.2\n20.0\n\n14.7\n\n% Parties\n(articles)f\n\n0.0\n77.6\n\n33.3\n\n57.4\n61.4\n\n67.4\n\n28.3\n24.5\n\n15.9\n10.5\n27.7\n25.7\n10.6\n16.8\n22.8\n\n23.1\n23.8\n26.0\n41.4\n18.2\n29.2\n23.8\n22.5\n16.7\n22.7\n34.7\n\n34.6\n\n% Journalists\n(issues)f\n\n91.3\n47.3\n\n66.7\n\n0.0\n24.5\n\n30.2\n\n36.7\n24.0\n\n56.3\n62.7\n43.9\n44.6\n58.8\n53.2\n58.1\n\n45.2\n38.7\n50.0\n25.5\n55.7\n44.7\n43.4\n45.6\n42.3\n47.1\n42.0\n\n35.3\n\n% Parties\n(issues)f\n\n47.6\n65.8\n\n55.9\n\n22.3\n44.1\n\n37.6\n\n48.1\n48.6\n\n55.0\n37.3\n32.1\n67.2\n65.6\n48.4\n59.7\n\n50.9\n53.7\n49.9\n54.0\n47.6\n64.5\n63.6\n57.1\n61.1\n47.3\n46.1\n\n35.9\n\n% Reportingg\n\n12.7\n34.2\n\n24.0\n\n69.7\n52.0\n\n51.6\n\n33.6\n34.1\n\n32.1\n56.5\n42.2\n21.6\n21.1\n46.5\n27.8\n\n34.6\n27.0\n30.8\n26.5\n39.3\n20.2\n30.9\n31.7\n29.3\n35.0\n51.3\n\n46.2\n\n% Opiniong\n\n39.7\n0.0\n\n20.1\n\n8.0\n3.9\n\n10.7\n\n18.3\n17.2\n\n12.9\n6.2\n25.7\n11.1\n13.2\n5.1\n12.5\n\n14.5\n19.3\n19.4\n19.5\n13.1\n15.2\n5.6\n11.2\n9.6\n17.6\n2.6\n\n17.9\n\n% Horseraceg\n\nLionel Marquis et al.\n\nSwiss Political Science Review (2011) Vol. 17(2): 128–163\n\n16626370, 2011, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/j.1662-6370.2011.02015.x by Cochrane Israel, Wiley Online Library on [09/12/2022]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\nNeue Zürcher Zeitung\n(NZZ)\nBasler Zeitung\nLe Temps (March 1998 –)\nSt. Galler Tagblatt\nTages-Anzeiger\nAargauer Zeitung\nLa Liberté\nCorriere del Ticino\nBund\n24 Heures\nNeue Luzerner Zeitung\nNouvelliste et Feuille\nd’Avis du Valais\nBerner Zeitung\nLe Quotidien Jurassien\nSchaﬀhauser Nachrichten\nTribune de Genève\nL’Express\nSolothurner Zeitung\nSüdostschweiz ⁄ Bündner\nZeitung\nBlick ⁄ Sonntags-Blick\nJournal de Genève\n(1995 – Feb. 1998)\nBerner Tagwacht\n(1995 – 1997)\nWochenzeitung\nSchweizerische\nHandelszeitung\nLe Nouveau Quotidien\n(1995 – Feb.1998)\nSonntags-Zeitung\nDie andere Zeitung (DAZ)\n(1995 – 1997)\n\nNewspaper\n\nInclusivenessc\n\nTable 2: Indicators of fairness for all articles published in 28 daily and weekly newspapers\n\n150\n\n 2011 Swiss Political Science Association\n\n236.6\n(184.9)\n126.4\n(95.7)\n172.3\n(74.6)\n19.0\n(10.0)\n\n61.6\n(41.6)\n37.0\n(27.3)\n51.7\n(22.8)\n8.1\n(4.0)\n\nSurfaceb\n0.641\n(0.271)\n0.666\n(0.238)\n0.702\n(0.190)\n0.203\n(0.048)\n\nInclusivenessc\n\n0.138\n\n0.191\n0.749\n\nInclusivenessc\n\n28.0\n(2.1)\n21.4\n(4.7)\n22.9\n(4.6)\n26.0\n(8.4)\n\nAverage\ncampaign\ndayd\n\n5.5\n\n15.7\n24.0\n\nAverage\ncampaign\ndayd\n\n0.018\n(0.083)\n)0.021\n(0.051)\n0.036\n(0.121)\n0.125\n(0.364)\n\nArticle\nbiase\n\n0.062\n\n-0.347\n).001\n\nArticle\nbiase\n\n% Journalists\n(articles)f\n91.2\n(7.1)\n90.8\n(5.2)\n83.5\n(10.6)\n87.7\n(7.5)\n\n)0.007\n(0.153)\n)0.009\n(0.138)\n0.097\n(0.166)\n0.220\n(0.257)\n\n9.5\n\n100.0\n85.5\n\n% Journalists\n(articles)f\n\nIssue\nbiase\n\n0.146\n\n0.089\n.050\n\nIssue\nbiase\n\n19.7\n(9.3)\n14.4\n(6.5)\n19.4\n(8.5)\n9.0\n(6.8)\n\n% Parties\n(articles)f\n\n7.9\n\n0.0\n18.5\n\n% Parties\n(articles)f\n\n41.1\n(19.2)\n25.9\n(3.9)\n24.5\n(13.6)\n38.6\n(24.3)\n\n% Journalists\n(issues)f\n\n7.5\n\n35.7\n23.8\n\n% Journalists\n(issues)f\n\n36.7\n(7.0)\n49.2\n(14.3)\n47.8\n(8.2)\n29.0\n(37.4)\n\n% Parties\n(issues)f\n\n9.9\n\n0.0\n45.4\n\n% Parties\n(issues)f\n\n51.5\n(9.7)\n52.3\n(4.7)\n52.4\n(11.1)\n48.9\n(21.2)\n\n% Reportingg\n\n9.5\n\n81.5\n52.1\n\n% Reportingg\n\n33.5\n(7.1)\n30.4\n(4.1)\n36.4\n(11.5)\n38.2\n(23.6)\n\n% Opiniong\n\n9.5\n\n18.5\n34.2\n\n% Opiniong\n\n15.0\n(7.5)\n17.4\n(2.7)\n11.2\n(5.5)\n12.9\n(15.7)\n\n% Horseraceg\n\n5.6\n\n0.0\n13.7\n\n% Horseraceg\n\n151\n\nSwiss Political Science Review (2011) Vol. 17(2): 128–163\n\n16626370, 2011, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/j.1662-6370.2011.02015.x by Cochrane Israel, Wiley Online Library on [09/12/2022]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\nNotes: a: Number of articles. b: Surface in standard newspaper pages (one page=1247 cm2). c: Number of actor categories · IQV (see text for calculation details). d: Mean number of days remaining before voting day. e: B* scores comprised between –1 (all articles ⁄ issues against welfare state\npreservation ⁄ extension) and +1 (all articles ⁄ issues in favour of welfare state preservation ⁄ extension); populist ballots removed; see text for calculation details. f: Sum of categories ‘‘journalists’’ (including press agencies) and ‘‘parties’’ (including referendum ⁄ initiative committees) may exceed\n100%, since several actors may be considered as the ‘‘sources’’ of an article. g: Percentage of articles in each category (see text for deﬁnition). h:\nNewspapers with fewer than 50 articles are excluded; all papers retained (N = 28) for number, surface, and average campaign day. i: National\npapers: Blick, Le Temps, NZZ, Tages-Anzeiger, DAZ. Supraregional papers: Journal de Genève, Neue Luzerner Zeitung, Le Nouveau Quotidien,\nSt. Galler Tagblatt, Südostschweiz. Cantonal papers: Aargauer Zeitung, Basler Zeitung, Bund, Berner Zeitung, Corriere del Ticino, L’Express, La\nLiberté, Nouvelliste, Le Quotidien Jurassien, Schaﬀhauser Nachrichten, Solothurner Zeitung, Tribune de Genève, 24 Heures, Berner Tagwacht.\nWeekly papers: Schweizerische Handelszeitung, Sonntags-Zeitung, Wochenzeitung, Weltwoche.\n\nWeeklyi\n\nCantonali\n\nSupraregionali\n\nNationali\n\nMeans for categories of\nnewspapers (standard\ndeviations in parentheses)\n\nb\n\n3.7\n44.6\n(R=1248.1)\n31.3\n\nSurface\n\nNumbera\n\n5\n153.7\n(R=4303)\n121.1\n\nDie Weltwoche\nMean (N = 21)h\n\nStandard deviation\n\nNumber\n\na\n\nNewspaper\n\nTable 2: (Continued)\n\nMedia Coverage of Referendum Campaigns\n\nLionel Marquis et al.\n\nWe will focus here on the three aspects of independence, bias, and substance. An inspection of individual values and standard deviations shows that there is indeed some variation\nbetween newspapers on these three criteria, but that it is rather limited. According to our\nbias measure B* for articles, the economy-oriented newspapers NZZ and Journal de Gene`ve\nmay qualify as ‘‘anti-welfare’’, as they were, on average, predominantly lopsided against\nwelfare state preservation ⁄ extension. In contrast, the left-wing paper Quotidien Jurassien\nappears as ‘‘pro-welfare’’.31 At the level of issue statements, there is greater variation in B*\nscores, but few papers stand outside a ± 20 percent range around the neutral value (antiwelfare: NZZ; pro-welfare: Quotidien Jurassien and Tribune de Gene`ve). In addition, few\nsingle ballots gave way to skewed information even in any separate outlet. Of all instances\nwith a suﬃcient number of cases, 11 percent were rather pro-welfare and 8 percent were\nrather anti-welfare.32 Weekly (mostly Sunday) papers represent an exception to this overall\nmoderation in dealing with welfare issues, as they appear slanted toward pro-welfare positions (but self-evidently not the right-wing Weltwoche). In sum, the concern that our ﬁnding\nof balanced coverage might be an artifact of aggregation is not substantiated by our data.\nOur ﬁndings concerning the criteria of source independence and substantive coverage tell\na similar story. Signiﬁcant variation between media outlets is scarce, even though some\npeculiarities are worth pointing out. For example, journalists outweigh partisan sources in\nthe Tages-Anzeiger, while in other outlets (e.g., Quotidien Jurassien) these two categories are\nalmost equally present — as speakers or sources of issue statements. In general, journalists\nplay a larger role in national papers than in regional and cantonal papers, which may have\nto do with the resources media organizations must invest to have their journalists closely\ncover ballot campaigns. Finally, the ‘‘format’’ of articles shows little diﬀerence between\nnewspapers. Some of them are more preoccupied with opinion formation than with factual\nreporting (NZZ, Nouvelliste, Quotidien Jurassien, Schaﬀhauser Nachrichten), while the\nopposite holds especially for a couple of French-speaking papers (La Liberte´, Tribune de\nGene`ve, L’Express). With one single exception, horserace information account for less than\n20 percent of campaign coverage in all papers. In any account, then, the overall variation in\nsource independence and information format is limited. The existence of such variation cannot be taken to mean that the press coverage of ballot campaigns in Switzerland is a\nmere collection of disjoint ‘‘speciﬁc campaigns’’ with potentially divergent consequences for\ndiﬀerent segments of the electorate.\n\n7. Spatial patterns of media coverage\nWe now turn to the issue of spatial homogeneity. Provided that the Swiss media system is\nhighly fragmented along linguistic lines, is this segmentation reﬂected in region-speciﬁc\n‘‘styles’’ of media coverage? More speciﬁcally, to what extent are the particular dimensions\nof media coverage that we analyzed thus far similar in the two main Swiss regions? As\n\n31\nIn the following, we will not comment on results for newspapers with a small number of published articles\n(N &lt; 50). Note that the indicators in this section are computed on the basis of all articles published in each newspaper, without distinction of the speciﬁc ballot measures.\n32\n292 campaigns in individual papers were analyzed and B* scores for articles were computed for each of them.\nOnly cases where a newspaper published more than 4 articles on a given campaign were considered. When the same\nprocedure is applied to B* scores for issue statements, the proportion of seemingly ‘‘biased campaigns’’ is much\nhigher, but also less reliable (see above note 30).\n\n 2011 Swiss Political Science Association\n\nSwiss Political Science Review (2011) Vol. 17(2): 128–163\n\n16626370, 2011, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/j.1662-6370.2011.02015.x by Cochrane Israel, Wiley Online Library on [09/12/2022]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n152\n\n153\n\nbecomes clear from Table 3, diﬀerences are in general rather limited, and sizable diﬀerences\nare only found for particular types of ballot issues.\nCampaign intensity as reported in Table 3 is a composite measure derived from the surface of articles, from the circulation ﬁgures of the newspapers, and from the number of people eligible to vote.33 The measure thus points out diﬀerences in the overall potential\nexposure of individual citizens and shows that campaigns are clearly more intense in the\nGerman-speaking region. The diﬀerence is particularly noticeable and systematic with\nrespect to pension and maternity issues. As concerns campaign length, the ‘‘average campaign day’’ indicator suggests that overall diﬀerences are slight. Campaigns may last a bit\nlonger in the German-speaking press, but mainly for measures pertaining to unemployment,\nand in any case not for measures on disability. Further, as would be expected from these\ndiﬀerences in intensity and length, media campaigns are also somewhat more ‘‘inclusive’’ in\nthe German-speaking press, where, on average, two additional actor categories are used as\nsources of issue statements.\nTurning to coverage bias, we again note that diﬀerences between the regions are marginal.\nHowever, as already pointed out above, bias is more conspicuous when measured at the level\nof issue statements rather than at the level of whole articles. Although the overall diﬀerence\nis negligible, this stems from the fact that the small diﬀerences that do exist cancel each other\nout across the various policy ﬁelds. Thus coverage in the German-speaking press was more\nskewed on health and disability measures, while it was less skewed on maternity issues (most\nnotably, French- and Italian-speaking papers had been much more enthusiastically endorsing the 1999 maternity leave project than had their German-speaking counterparts).\nNext, we compared the independence of journalists from external sources of information\nin the two regions. On the whole, the diﬀerences are modest. As speakers, German-speaking\njournalists were less prominent than journalists from other regions on maternity and disability issues, but they were more ‘‘autonomous’’ on unemployment. Diﬀerences were somewhat more pronounced as regards the source of issue statements. Journalists were more\nlikely to be the source of reported issues in the German-speaking press than in the Frenchand Italian-speaking press, especially for unemployment and labour regulation (but not for\nmaternity and disability issues). For example, French-speaking journalists sometimes draw\non scientiﬁc or educational sources (8 percent) for addressing labour and unemployment\nissues, while their German-speaking colleagues never do so (at least in our sample). These\nregional patterns may have at least two diﬀerent causes. On the one hand, turning to external sources may occur because journalists are less sure where they stand (or should stand)\non the issues and ⁄ or because they believe that public opinion on the issues is not crystallized\nyet and need orientating information from political sources. On the other hand, the\nobserved regional gaps may simply be reﬂective of more routinized relationships between\njournalists and political actors in some policy domains.\nThe ‘‘format’’ of information was then considered, with a focus on horserace information. Marginal regional gaps were observed, as the share of horserace items rarely exceeded\n20 percent in any region, but the diﬀerences were rather systematic. In general, information\nin German-speaking papers is more often of the ‘‘horserace’’ type than in French- or\nItalian-speaking papers (an overall 4 percent gap), and this diﬀerence holds for 17 of 24\n33\n\nLe Nouveau Quotidien, DAZ, and Berner Tagwacht are not taken into account in this analysis, due to unavailable circulation ﬁgures, but they represent only about 2 percent of articles and surface. More consequential is the\nfact that the French-speaking tabloid Le Matin, which has the largest circulation in the western region, is not\nincluded in our data. The absence of this paper probably explains part of the diﬀerence between the two regions.\n\n 2011 Swiss Political Science Association\n\nSwiss Political Science Review (2011) Vol. 17(2): 128–163\n\n16626370, 2011, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/j.1662-6370.2011.02015.x by Cochrane Israel, Wiley Online Library on [09/12/2022]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\nMedia Coverage of Referendum Campaigns\n\n 2011 Swiss Political Science Association\n\n0.5 (10.5)\n4.0 (9.5)\n\n2.0 (9.5)\n\n3.1 (8.8)\n\n9.3 (15.6)\n2.7 (5.5)\n\n2.0 (9.6)\n1.9 (9.7)\n1.3 (7.8)\n2.5 (10.8)\n\n4.2 (9.2)\n4.7 (10.3)\n2.5 (6.4)\n3.2 (8.3)\n\n2.2 (26.4)\n)4.1 (23.9)\n\n8.9 (27.0)\n\n1.6 (24.8)\n1.2 (25.3)\n0.3 (22.8)\n3.0 (23.8)\n\nAverage\ncampaign\nday\n\n)0.236 (0.297)\n0.178* (0.365)\n\n0.002 (0.120)\n\n)0.036 (0.053)\n)0.035 (0.065)\n0.047 (0.131)\n\n0.013 (0.207)\n0.011 (0.145)\n0.125 (0.420)\n)0.045 (0.071)\n\nIssue\nbias\n(|B| scores)\n\n0.009 (0.091)\n)0.010 (0.075)\n0.037 (0.157)\n0.052 (0.080)\n\nArticle\nbias\n(|B| scores)\n\n)4.0 (86.5)\n)11.3 (78.9)\n\n0.5 (28.7)\n)1.6 (16.0)\n\n10.2 (30.9)\n\n6.1** (28.1)\n5.1 (27.7)\n6.0 (33.0)\n13.1* (28.5)\n\n)0.7 (86.3)\n)1.5 (85.9)\n2.2 (89.1)\n1.3 (86.7)\n6.8 (89.2)\n\nSource\nof issues\n= journalists\n\nSource of\narticles\n= journalists\n\n)4.5 (14.2)\n)1.8 (14.0)\n\n3.9 (7.3)\n\n3.9 (15.7)\n5.7 (16.6)\n5.6 (14.2)\n4.6 (21.0)\n\nImportance of\nhorserace\ninformation\n\nLionel Marquis et al.\n\nSwiss Political Science Review (2011) Vol. 17(2): 128–163\n\n16626370, 2011, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/j.1662-6370.2011.02015.x by Cochrane Israel, Wiley Online Library on [09/12/2022]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n**: p&lt;.05; *: p&lt;.10 (two-tailed tests checking for diﬀerences between paired-sample means).\n\nNotes: The ﬁgures show the diﬀerence between the two main linguistic areas (German-speaking vs. French- or Italian-speaking). A positive (negative) diﬀerence means that a given quantity is higher (lower) in the German-speaking area. In parentheses are the reference quantities, i.e., those\nmeasured in the German-speaking area.\nPk Pn\ncirci surfij\nj¼1\na: Intensity = i¼1 voters\n, with circi=circulation of newspaper i, surfij=surface of article j (in pages) in newspaper i, and voters=people eligible to vote (in thousands).\n\nTotal (N=24)\nPensions (N=10)\nHealth (N=4)\nLabour regulation\n(N=4)\nUnemployment\n(N=2)\nMaternity (N=2)\nDisability (N=2)\n\nIntensity a\n\nInclusiveness\n(# of source\ncategories)\n\nTable 3: Interregional diﬀerences with respect to eight indicators of media fairness and six policy ﬁelds\n\n154\n\n155\n\nballot proposals. The largest between-region diﬀerences in the share of horserace articles\nare found for pension and health issues, but there was actually more horserace coverage in\nthe French ⁄ Italian-speaking area on maternity and disability issues.34\n\n8. Conclusion\nThe aim of this study was to assess the fairness of the press coverage of referendums on\nwelfare state issues in Switzerland. We distinguished seven dimensions of media coverage in\norder to determine how it compares to idealized notions of the media’s role in the democratic process. In this concluding section we summarize our analysis of media coverage\nquality and, whenever possible and sensible, we relate our results to those of other studies.\nFirst, as regards the intensity of press coverage in daily newspapers, we found about eight\narticles per proposal and newspaper, amounting to two or three full pages of coverage.\nWith only modest variation across votes (though variation is more pronounced across\nmedia outlets), such coverage might be considered as a ‘‘fair’’ performance.35\nSecond, turning to the question of campaign duration, our results show that the press coverage peaked two to three weeks before the voting day, and that half of the total information had been released some 22 days before the poll. Although comparable studies of\nEuropean referendums are lacking (see Novik 2009: 13) we interpret this average duration\nas ‘‘appropriate’’, since press coverage was relatively gradual and did not surge in the last\ncampaign days.\nThird, we investigated the bias in media coverage. Overall, we ﬁnd a minimal to nonexistent bias at the level of press articles (i.e., the general thrust of news stories) toward prowelfare state orientation. When looking at particular issue statements, the bias appeared\nlarger but still not overwhelming. In fact, bias was largely restricted to matters of public\nhealth in the larger sense (hospital costs, price of drugs, maternity, disability, etc.) and was\nvirtually absent from campaigns on such themes as pensions, labour regulation, and unemployment. The variation in balance among diﬀerent newspaper outlets is rather limited.\nConcerning articles in daily newspapers, only the Journal de Gene`ve and the NZZ adopted\na rather ‘‘anti-welfare’’ position while the Quotidien Jurassien was lopsided toward the\n‘‘pro-welfare’’ camp. However, compared to news balance in other referendum campaigns,\nsuch as Pilon’s (2009) report on the Ontario Provincial Referendum and Hobolt’s (2009:\n186–189) analysis of the ﬁrst Irish referendum on the Nice Treaty, we conclude that the\nreporting on welfare state issues in the Swiss print media generally provides little ammunition to those who suspect or condemn a systematic bias in media coverage.\n\n34\nIt may be that, due to a more skeptical opinion climate toward the latter issues in their region, German-speaking\njournalists had more incentives to engage in opinion formation rather than stick to hard facts and descriptions of\nthe situation. This is especially true for maternity issues, as welfare state support is 30 percent higher on average\namong French- and Italian-speaking voters than among German-speaking voters (our own calculations from the\nmunicipality-level data available in the ‘‘Political Atlas of Switzerland’’, Swiss Federal Statistical Oﬃce).\n35\nThis rather lenient judgment may be qualiﬁed by comparing our results with those of Hobolt (2009: 207)208),\nwho found more intense press coverage for the 2005 French and the Dutch referendum campaigns on the European\nConstitution, even if we restrict the analysis to the country’s two major broadsheet newspapers (NZZ and\nLe Temps, in the Swiss case) as Hobolt did. However, the ﬁgures might not be fully comparable; the vote on the\nEuropean Constitution might have been of greater salience than single welfare state proposals. Moreover, the lower\nfrequency of popular votes in France and the Netherlands as compared to Switzerland may explain a more intense\ncoverage of those (rare) referendums. We should therefore await further research on news reporting in the Swiss\ncontext in order to compare our ﬁndings.\n\n 2011 Swiss Political Science Association\n\nSwiss Political Science Review (2011) Vol. 17(2): 128–163\n\n16626370, 2011, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/j.1662-6370.2011.02015.x by Cochrane Israel, Wiley Online Library on [09/12/2022]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\nMedia Coverage of Referendum Campaigns\n\nLionel Marquis et al.\n\nFourth, the relative independence of journalists from external (particularly partisan)\nsources was examined. Similar to coverage bias, the dimension of media autonomy was\ninvestigated at two levels: articles and issue statements. At the level of articles, it was found\nthat journalists are by far the most important ‘‘speakers’’. Although they can delegate their\nagenda-setting role to other actors by soliciting their opinions through interviews, op-eds,\nor free columns, they rarely do so — at least not until the last three or four campaign\nweeks. Hence, our analysis suggests that the Swiss newspapers’ progressive loss of attachment to parties and other political groups (Meier and Schanne 1994) may have enhanced\ntheir ‘‘autonomy’’ — a concept not to be confused with that of journalistic ‘‘objectivity’’,\nwhich is outside the scope of this study. However, when evaluated at the level of issue\nstatements, the press coverage of referendum campaigns appears in a somewhat diﬀerent\nlight. Without there being anything ‘‘unfair’’ on a priori grounds, media coverage appears\nto largely reﬂect the issue agenda of the major political forces involved in campaigns (i.e.,\npolitical parties and committees, governmental agencies, professional associations, etc.).\nAltogether, these non-media sources account for about three times the number of journalistic issue items.36 In our view, there is no basic contradiction between the ﬁndings from these\ndistinct levels of analysis; taken together, they are reﬂective of both main functions traditionally assigned to news media — an information and an orientation function.\nThe dominance of journalists as sources of articles suggests that press journalists function\nas gatekeepers, selecting who may intervene directly in the news process, rather than as\nmere providers of a ‘‘free forum’’ to which any group or individual is granted equal and\nunlimited access. On the other hand, the press coverage of ballot issues is of course reﬂective — and possibly ‘‘reﬂexive’’ in the Luhmannian sense (Neidhardt 1994) — of what the\nmain political actors have to say. Therefore, media agenda-setting is not exogenous, but largely driven by the agenda of various political elites. As our measure of source inclusiveness\nsuggests, Swiss press journalists tend to rely on a broad array of diﬀerent actor categories,\nand media coverage may thus be said to be ‘‘inclusive’’. However, this substantial diversity\nin news coverage does not allow us to rule out the gatekeeping hypothesis: it is questionable\nwhether all actors who wanted to participate in the campaign debates ﬁnally found their\nway in news reporting.\nNext, our data provided insight into the substance of media coverage. We found that the\noften-criticized ‘‘horserace’’ nature of election coverage is not prevalent in the context of\nSwiss referendum campaigns. For one thing, horserace information accounts for only about\n13 percent of all news items — a tiny proportion compared, for instance, with U.S. presidential campaigns (Strömbäck and Dimitrova 2006), the 1995 Quebec secession referendum\n(see Pilon 2009), or even with more readily comparable campaigns such as the 2000 Danish\nreferendum on the adoption of the Euro (De Vreese and Semetko 2002). In addition,\nalthough the number of ‘‘factual information’’ items exceeds that of ‘‘opinion’’ items, both\nare important categories of press coverage. This again suggests that information and orientation are important functions performed by the media, and that in assuming these functions the media go well beyond predicting which side is likely to win.\nLastly, spatial homogeneity was deﬁned as an additional norm of media coverage. This\nnorm is rooted in the perception that public opinion on welfare state issues frequently\ndiverges across the main linguistic areas. Accordingly, distinctive media coverage threatens\nto reinforce the pre-existing cleavages and to undermine national cohesion by promoting a\n36\n\nAs compared to Tresch’s (2008: 142–149) ﬁndings on media coverage of European politics, state actors play a\nslightly less prominent role in debates on welfare state issues.\n\n 2011 Swiss Political Science Association\n\nSwiss Political Science Review (2011) Vol. 17(2): 128–163\n\n16626370, 2011, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/j.1662-6370.2011.02015.x by Cochrane Israel, Wiley Online Library on [09/12/2022]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n156\n\n157\n\n‘‘tyranny of the majority’’. It was found that regional gaps in the press coverage of campaigns were minimal and largely restricted to three criteria: intensity of coverage, horserace\nreporting and source independence. In the German-speaking region, campaigns were somewhat more intense, more inclined toward horserace information, and journalists were more\noften the source of issues than was the case in the other linguistic areas. This may reﬂect\neither economic or cultural diﬀerences of media organizations between the regions. But\nagain, the hallmark of this analysis is similarity rather than discrepancy. Even when looking\nat speciﬁc welfare policy areas (pensions, health, etc.), press coverage appears — with few\nexceptions — strikingly similar across both regions, in line with the more detailed analysis\nof Swiss European policy by Tresch (2008).\nIn conclusion, the coverage of referendum campaigns on welfare state issues by the Swiss\npress appears reasonably fair when examined from a broad perspective. However, this\nappraisal of ‘‘satisfactory’’ (though certainly not ‘‘optimal’’) media campaign coverage is\nbased on quantitative indicators that may fail to capture many intrinsic qualities of media\ncoverage. Admittedly, the standards used to assess the fairness of media coverage were not\nexcessively high, and the conclusions we draw from our analysis await conﬁrmation from\nindependent datasets and studies, in particular from more qualitative-oriented research\n‘‘putting qualitative ﬂesh on quantitative bones’’ (Tarrow 2004: 176). Accordingly, we call\nfor further testing of the normative framework developed in the present study and for replications of our analysis in other settings. In addition, there is clearly a need of going beyond\nthe mere ‘‘procedural’’ approach to fairness developed here. Although they can be estimated by quantitative indicators, procedure-independent criteria of media fairness may be\nbest deﬁned through qualitative methods.\n\nReferences\nAlthaus, S. (1998). Information Eﬀects in Collective Preferences. American Political Science Review\n92(3): 545–558.\nBächtiger, A., S. Niemeyer, M. Neblo, M. Steenbergen and J. Steiner (2010). Disentangling Diversity\nin Deliberative Democracy: Competing Theories, Their Blind Spots and Complementarities. Journal of Political Philosophy 18(1): 32–63.\nBaerns, B. (1979). Öﬀentlichkeitsarbeit als Determinante journalistischer Informationsleistungen. Publizistik 24(3): 301–316.\nBarrett, A. and L. Barrington (2005). Bias in Newspaper Photograph Selection. Political Research\nQuarterly 58(4): 609–618.\nBartels, L. (1988). Presidential Primaries and the Dynamics of Public Choice. Princeton: Princeton University Press.\n—— (1996). Uninformed Votes: Information Eﬀects in Presidential Elections. American Journal of\nPolitical Science 40(1): 194–230.\nBennett, L., R. Lawrence and S. Livingston (2007). When the Press Fails. Political Power and the News\nMedia from Iraq to Katrina. Chicago: University of Chicago Press.\nBentele, G. (2003). Kommunikatorforschung: Public Relations. In Bentele, G., H.-B. Brosius and O.\nJarren (eds.), Öﬀentliche Kommunikation. Handbuch Kommunikations- und Medienwissenschaft.\nWiesbaden: Westdeutscher Verlag (54–78).\nBerelson, B. (1952). Democratic Theory and Public Opinion. Public Opinion Quarterly 16(3): 313–330.\nBerelson, B., P. Lazarsfeld and W. McPhee (1954). Voting. A Study of Opinion Formation in a Presidential Campaign. Chicago: The University of Chicago Press.\n\n 2011 Swiss Political Science Association\n\nSwiss Political Science Review (2011) Vol. 17(2): 128–163\n\n16626370, 2011, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/j.1662-6370.2011.02015.x by Cochrane Israel, Wiley Online Library on [09/12/2022]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\nMedia Coverage of Referendum Campaigns\n\nLionel Marquis et al.\n\nBerkowitz, D.A. (2009). Reporters and Their Sources. In Wahl-Jorgensen, K. and T. Hanitzsch (eds.),\nThe Handbook of Journalism Studies. New York: Routledge (102–115).\nBlum, R. (2003). Medienstrukturen der Schweiz. In Bentele, G., H.-B. Brosius and O. Jarren (eds.),\nÖﬀentliche Kommunikation. Handbuch Kommunikations- und Medienwissenschaft. Wiesbaden: Westdeutscher Verlag (366–381).\nBonfadelli, H. and R. Blum (2000). Helvetisches Stiefkind: die Rolle der Massenmedien bei der Vermittlung schweizerischer Aussenpolitik. Bern, NFP 42 Synthesis n 28.\nBornschier, S. (2010). Cleavage Politics and the Populist Right: The New Cultural Conﬂict in Western\nEurope. Philadelphia: Temple University Press.\nBützer, M. and L. Marquis (2002). Public Opinion Formation in Swiss Federal Referendums. In\nFarrell, D. and R. Schmitt-Beck (eds.), Do Political Campaigns Matter? Campaign Eﬀects in Elections and Referendums. London and New York: Routledge (163–182).\nBunton, K. (1998). Social Responsibility in Covering Community: A Narrative Case Analysis. Journal\nof Mass Media Ethics 13(4): 232–246.\nChaﬀee, S. and S.Y. Choe (1980). Time of Decision and Media Use During the Ford-Carter\nCampaign. Public Opinion Quarterly 44(1): 53–69.\nChristians, C. and K. Nordenstreng (2004). Social Responsibility Worldwide. Journal of Mass Media\nEthics 19(1): 3–28.\nCohen, J. (1986). An Epistemic Conception of Democracy. Ethics 97(1): 26–38.\n—— (1989). Deliberation and Democratic Legitimacy. In Hamlin, A. and P. Pettit (eds.), The Good\nPolity. Oxford: Blackwell (17–34).\nConnolly, S. and S. Hargreaves Heap (2007). Cross Country Diﬀerences in Trust in Television and\nthe Governance of Public Broadcasters. Kyklos 60(1): 3–14.\nConnolly, W.E. 1991. Identity, Diﬀerence: Democratic Negotiations of Political Paradox. Ithaca:\nCornell University Press.\nDahl, R.A. 1989. Democracy and Its Critics. New Haven [etc.]: Yale University Press.\nDe Vreese, C. and H. Semetko (2002). Public Perception of Polls and Support for Restrictions on the\nPublication of Polls: Denmark’s 2000 Euro Referendum. International Journal of Public Opinion\nResearch 14(4): 367–390.\nDelli Carpini, M. and S. Keeter 1996. What Americans Know about Politics and Why It Matters. New\nHaven and London: Yale University Press.\nDiskin, A., A. Eschet-Schwarz and D. Felsenthal (2007). Homogeneity, Heterogeneity and Direct\nDemocracy: The Case of Swiss Referenda. Canadian Journal of Political Science 40(2): 317–342.\nDonovan, T. and S. Bowler (1998). An Overview of Direct Democracy in the American States. In\nBowler, S., T. Donovan and C. Tolbert (eds.), Citizens as Legislators. Direct Democracy in the\nUnited States. Columbus: Ohio State University Press (1–21).\nDonsbach, W. and A. Wenzel (2002). Aktivität und Passivität von Journalisten gegenüber parlamentarischer Pressearbeit. Publizistik 47(4): 373–387.\nDowns, A. (1957). An Economic Theory of Democracy. New York: Harper Collins.\nDryzek, J.S. (1990). Discursive Democracy: Politics, Policy, and Political Science. New York:\nCambridge University Press.\nDuVivier, K. (2006). The United States as a Democratic Ideal? International Lessons in Referendum\nDemocracy. Temple Law Review 79: 821–876.\nEntman, R. (2004). Projections of Power. Framing News, Public Opinion, and U.S. Foreign Policy.\nChicago: University of Chicago Press.\nEstlund, D. (1997). Beyond Fairness and Deliberation: The Epistemic Dimension of Democratic\nAuthority. In Bohman, J. and W. Rehg (eds.), Deliberative Democracy: Essays on Reason and\nPolitics. Cambridge: MIT Press (173–204).\n\n 2011 Swiss Political Science Association\n\nSwiss Political Science Review (2011) Vol. 17(2): 128–163\n\n16626370, 2011, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/j.1662-6370.2011.02015.x by Cochrane Israel, Wiley Online Library on [09/12/2022]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n158\n\n159\n\nFörster, J. and N. Liberman (2007). Knowledge Activation. In Kruglanski, A. and E.T. Higgins\n(eds.), Social Psychology. Handbook of Basic Principles. New York: Guilford Press (201–231).\nFournier, P., R. Nadeau, A. Blais, E. Gidengil and N. Nevitte (2004). Time-of-Voting Decision and\nSusceptibility to Campaign Eﬀects. Electoral Studies 23(4): 661–681.\nFröhlich, R. and B. Rüdiger (2006). Framing Political Public Relations: Measuring Success of Political Communication Strategies in Germany. Public Relations Review 32: 18–25.\nGerhards, J. (1997). Diskursive versus liberale Öﬀentlichkeit. Eine empirische Auseinandersetzung mit\nJürgen Habermas. Kölner Zeitschrift für Soziologie und Sozialpsychologie 49(1): 1–34.\nGilens, M. (2001). Political Ignorance and Collective Policy Preferences. American Political Science\nReview 95(2): 379–396.\nGitlin, T. (1980). The Whole World is Watching. Mass Media in the Making and Unmaking of the New\nLeft. Berkeley: University of California Press.\nGollin, A. (1980). Exploring the Liaison between Polling and the Press. Public Opinion Quarterly\n44(4): 445–461.\nGraber, D. (2003). The Media and Democracy: Beyond Myths and Stereotypes. Annual Review of\nPolitical Science 6: 139–160.\n—— (2006). Mass Media and American Politics. Washington: CQ Press.\nGrossenbacher, R., T. Forsberg, M. Koch and M. Brändli (2006). Politische Öﬀentlichkeitsarbeit in regionalen Medien. Kilchberg: Publicom AG.\nGurevitch, M. and J. Blumler (1977). Linkages between the Mass Media and Politics: A Model for\nthe Analysis of Political Communications Systems. In Curran, J., M. Gurevitch and J. Woollacott\n(eds.), Mass Communication and Society. London: Edward Arnold (270–290).\nGurevitch, M. and J. Blumler (1990). Political Communication Systems and Democratic Values. In\nLichtenberg, J. (ed.), Democracy and the Mass Media. New York: Cambridge University Press\n(269–289).\nHabermas, J. (1992). Faktizität und Geltung. Beiträge zur Diskurstheorie des Rechts und des demokratischen Rechtsstaats. Frankfurt am Main: Suhrkamp.\nHajnal, Z., E. Gerber and H. Louch (2002). Minorities and Direct Legislation: Evidence from\nCalifornia Ballot Proposition Elections. Journal of Politics 64(1): 154–177.\nHallin, D. (1984). The Media, the War in Vietnam, and Political Support: A Critique of the Thesis of\nan Oppositional Media. Journal of Politics 46(1): 2–24.\nHallin, D. and P. Mancini (2004). Comparing Media Systems. Three Models of Media and Politics.\nNew York: Cambridge University Press.\nHardmeier, S. (1999). Political Poll Reporting in Swiss Print Media: Analysis and Suggestions for\nQuality Improvement. International Journal of Public Opinion Research 11(3): 257–74.\n—— (2000). Meinungsumfragen im Journalismus: Nachrichtenwert, Präzision und Publikum. Medien\nund Kommunikationswissenschaft 48(3): 371–395.\nHargreaves, I. and J. Thomas (2002). New News, Old News. An ITC and BSC Research Publication.\nLondon: ITC.\nHayes, A. (2008). Press Critics Are the Fifth Estate: Media Watchdogs in America. New York:\nPraeger.\nHertig, H.P. (1982). Sind Abstimmungserfolge käuﬂich? — Elemente der Meinungsbildung bei\nEidgenössischen Abstimmungen. Annuaire Suisse de Science Politique 22: 35–57.\nHiggins, E. (1996). Knowledge Activation: Accessibility, Applicability, and Salience. In Higgins, E.\nand A. Kruglanski (eds.), Social Psychology. Handbook of Basic Principles. New York: The\nGuilford Press (133–168).\nHofstetter, C. et al. (1999). Information, Misinformation, and Political Talk Radio. Political Research\nQuarterly 52(2): 353–369.\n\n 2011 Swiss Political Science Association\n\nSwiss Political Science Review (2011) Vol. 17(2): 128–163\n\n16626370, 2011, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/j.1662-6370.2011.02015.x by Cochrane Israel, Wiley Online Library on [09/12/2022]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\nMedia Coverage of Referendum Campaigns\n\nLionel Marquis et al.\n\nHobolt Binzer, S. (2009). Europe in Question: Referendums on European Integration. Oxford: Oxford\nUniversity Press.\nJerit, J., J. Barabas and T. Bolsen (2006). Citizens, Knowledge, and the Information Environment.\nAmerican Journal of Political Science 50(2): 266–282.\nKaufmann, B., G. Kreis and A. Gross (2005). Direkte Demokratie und europäische Integration. Die\nHandlungsspielräume der Schweiz. Basel: Europainstitut.\nKelley, S. (1960). Political Campaigning. Problems in Creating an Informed Electorate. Washington:\nThe Brookings Institution.\nKing, E. and M. Schudson (1995). The Press and the Illusion of Public Opinion: The Strange Case of\nRonald Reagan’s ‘Popularity’. In Glasser, T. and C. Salmon (eds.), Public Opinion and the Communication of Consent. New York: The Guilford Press (132–155).\nKirchgässner, G. and T. Schulz (2005). Was treibt die Stimmbürger an die Urne? Eine empirische\nUntersuchung der Abstimmungsbeteiligung in der Schweiz, 1981–1999. Swiss Political Science\nReview 11(1): 1–56.\nKriesi, H. (1994). Le déﬁ à la démocratie directe posé par les transformations de l’espace public. In\nPapadopoulos, Y. (ed.), Pre´sent et avenir de la de´mocratie directe. Geneva: Georg (31–72).\n—— (1998). The Transformation of Cleavage Politics: The 1997 Stein Rokkan Lecture. European\nJournal of Political Research 33(2): 165–185.\n—— (2003). Strategische politische Kommunikation: Bedingungen und Chancen der Mobilisierung\nöﬀentlicher Meinung im internationalen Vergleich. In Esser, F. and B. Pfetsch (eds.), Politische\nKommunikation im internationalen Vergleich. Grundlagen, Anwendungen, Perspektiven. Wiesbaden:\nWestdeutscher Verlag (208–239).\n—— (2005). Direct Democratic Choice. The Swiss Experience. Lanham: Lexington Books.\nKriesi, H., B. Wernli, P. Sciarini and M. Gianni (1996). Le clivage linguistique. Proble`mes de compre´hension entre les communaute´s linguistiques en Suisse. Berne: Oﬃce fédéral de la statistique.\nKrouse, R. and G. Marcus (1984). Electoral Studies and Democratic Theory Reconsidered. Political\nBehavior 6(1): 23–39.\nKuklinski, J. and P. Quirk (2000). Reconsidering the Rational Public: Cognition, Heuristics, and Mass\nOpinion. In Lupia, A., M. McCubbins and S. Popkin (eds.), Elements of Reason. Cambridge:\nCambridge University Press (153–182).\nLandis, J.R. and G. Koch (1977). The Measurement of Observer Agreement for Categorical Data.\nBiometrics 33(1): 159–174.\nLeuthold, H., M. Hermann and S. Fabrikant (2007). Making the Political Landscape Visible:\nMapping and Analyzing Voting Patterns in an Ideological Space. Environment and Planning B:\nPlanning and Design 34: 785–807.\nLinder, W., R. Zürcher and C. Bolliger (2008). Gespaltene Schweiz – geeinte Schweiz. Gesellschaftliche\nSpaltungen und Konkordanz bei den Volksabstimmungen seit 1874. Baden: Hier und Jetzt.\nList, C. and R.E. Goodin (2001). Epistemic Democracy: Generalizing the Condorcet Jury Theorem.\nJournal of Political Philosophy 9(3): 277–306.\nLongchamp, C. (1998). Demoskopie: Seismograph oder Kompass? Ein Überblick über die Ausbreitung\nund Verwendung der politischen Umfrageforschung in der Schweiz. Referat vor der Erdöl-Vereinigung, 23.03.1998. Online: http://www.gfs.ch/publset.html [accessed: 29.04.2010].\nLuskin, R., J. Fishkin and R. Jowell (2002). Considered Opinions: Deliberative Polling in Britain.\nBritish Journal of Political Science 32(3): 455–487.\nMarquis, L. (2006). La formation de l’opinion publique en de´mocratie directe. Les réfe´rendums sur la\npolitique exte´rieure suisse (1981–1995). Zurich: Seismo.\nMarquis, L. and M. Bergman (2009). Development and Consequences of Referendum Campaigns in\nSwitzerland, 1981–1999. Swiss Political Science Review 15(1): 63–97.\n\n 2011 Swiss Political Science Association\n\nSwiss Political Science Review (2011) Vol. 17(2): 128–163\n\n16626370, 2011, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/j.1662-6370.2011.02015.x by Cochrane Israel, Wiley Online Library on [09/12/2022]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n160\n\n161\n\nMatthes, J. (2005). The Need for Orientation Towards News Media: Revising and Validating a Classic\nConcept. International Journal of Public Opinion Research 18(4): 422–444.\nMcDevitt, M. (2003). In Defense of Autonomy: A Critique of the Public Journalism Critique. Journal\nof Communication 53(1): 155–160.\nMcLeod, D., G. Kosicki and J. McLeod (2002). Resurveying the Boundaries of Political Communications Eﬀects. In Bryant, J. and D. Zillmann (eds.), Media Eﬀects. Advances in Theory and Research.\nMahwah: Lawrence Erlbaum (215–267).\nMeier, W. and M. Schanne (1994). Medien-‘‘Landschaft’’ Schweiz. Zurich: Pro Helvetia.\nMeier, W. and O. Jarren (2002). Ökonomisierung und Kommerzialisierung von Medien und Mediensystem. Bemerkungen zu einer (notwendigen) Debatte. In Haas, H. and O. Jarren (eds.), Mediensysteme im Wandel. Struktur, Organisation und Funktion der Massenmedien. Wien: Braumüller\n(201–216).\nMerritt, D. (1995). Public Journalism and Public Life. Why Telling the News Is Not Enough. Mahwah:\nLawrence Erlbaum.\nNeidhardt, F. (1994). Öﬀentlichkeit, öﬀentliche Meinung, soziale Bewegungen. In Neidhardt, F. (ed.),\nÖﬀentlichkeit, öﬀentliche Meinung, soziale Bewegungen. Kölner Zeitschrift für Soziologie und\nSozialpsychologie, Sonderheft 34 (7–41).\nNeidhart, L. (1970). Plebiszit und pluralitäre Demokratie. Eine Analyse der Funktion des schweizerischen Gesetzesreferendums. Berne: Francke.\nNeuman, W.R. (1990). The Threshold of Public Attention. Public Opinion Quarterly 54(2): 159–176.\nNovik, N. (2009). Do Campaigns Matter? The Contribution of European Union Campaigns to Broad\nPolitical Knowledge. Dissertation. Dublin: Trinity College.\nPage, B.I. (1996). Who Deliberates? Mass Media in Modern Democracy. Chicago: University of Chicago Press.\nPapadopoulos, Y. (1998). De´mocratie directe. Paris: Economica.\nPateman, C. (1970). Participation and Democratic Theory. Cambridge: Cambridge University Press.\nPatterson, T. (1994). Out of Order. New York: Vintage.\n—— (1998). Political Roles of the Journalist. In Graber, D., D. McQuail and P. Norris (eds.), The\nPolitics of News. The News of Politics. Washington D.C.: Congressional Quarterly (17–32).\nPfetsch, B. (2004). Geschlossene Gesellschaft? Akteursensembles und Akteursbewertungen in Pressekommentaren. In Eilders, C., F. Neidhardt and B. Pfetsch (eds.), Die Stimme der Medien: Pressekommentare und politische Öﬀentlichkeit in der Bundesrepublik. Wiesbaden: VS Verlag (74–105).\nPilon, D. (2009). Investigating Media as a Deliberative Space: Newspaper Opinions about Voting Systems in the 2007 Ontario Provincial Referendum. Canadian Political Science Review 3(3): 1–23.\nPrice, V. and J. Zaller (1993). Who Gets the News? Alternative Measures of News Reception and their\nImplications for Research. Public Opinion Quarterly 57(2): 133–164.\nRobinson, G. (1995). Making News and Manufacturing Consent: The Journalistic Narrative and Its\nAudience. In Glasser, T. and C. Salmon (eds.), Public Opinion and the Communication of Consent.\nNew York: The Guilford Press (348–369).\nRothmayr, C. and S. Hardmeier (2002). Government and Polling: Use and Impact of Polls in the\nPolicy-Making Process in Switzerland. International Journal of Public Opinion Research 14(2):\n123–140.\nSallot, L. and E. Johnson (2006). Investigating Relationships between Journalists and Public Relations\nPractitioners: Working Together to Set, Frame and Build the Public Agenda, 1991-2004. Public\nRelations Review 32(2): 151–159.\nSchönhagen, P. (2008). Ko-Evolution von Public Relations und Journalismus: Ein erster Beitrag zu\nihrer systematischen Aufarbeitung. Publizistik 53(1): 9–24.\nSchudson, M. (2001). The Objectivity Norm in American Journalism. Journalism 2(2): 149–170.\n\n 2011 Swiss Political Science Association\n\nSwiss Political Science Review (2011) Vol. 17(2): 128–163\n\n16626370, 2011, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/j.1662-6370.2011.02015.x by Cochrane Israel, Wiley Online Library on [09/12/2022]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\nMedia Coverage of Referendum Campaigns\n\nLionel Marquis et al.\n\nShoemaker, P. and S. Reese (1995). Mediating the Message. Theories of Inﬂuences on Mass Media\nContent. White Plains: Longman.\nSigal, L. (1973). Reporters and Oﬃcials. The Organization and Politics of Newsmaking. Lexington:\nHeath and Company.\nSinger, J. (2007). Contested Autonomy. Professional and Popular Claims on Journalistic Norms. Journalism Studies 8(1): 79–95.\nStatham, P. (2006). Political Journalism and Europeanization: Pressing Europe? European Political\nCommunication Working Paper No. 2006 ⁄ 13.\nStrömbäck, J. and D. Dimitrova (2006). Political and Media Systems Matter: A Comparison of\nElection News Coverage in Sweden and the United States. Harvard International Journal of Press ⁄\nPolitics 11(4): 131–147.\nSturgis, P. (2003). Knowledge and Collective Preferences. A Comparison of Two Approaches to Estimating the Opinions of a Better Informed Public. Sociological Methods and Research 31(4): 453–485.\nTarrow, S. (2004). Bridging the Quantitative-Qualitative Divide. In Brady, H. and D. Collier (eds.),\nRethinking Social Inquiry: Diverse Tools, Shared Standards. Lanham: Rowman &amp; Littleﬁed (171–\n179).\nTrechsel, A. and P. Sciarini (1998). Direct Democracy in Switzerland: Do Elites Matter? European\nJournal of Political Research 33(1): 99–124.\nTresch, A. (2008). Öﬀentlichkeit und Sprachenvielfalt. Medienvermittelte Kommunikation zur Europapolitik in der Deutsch- und Westschweiz. Baden-Baden: Nomos.\nTuchman, G. (1972). Objectivity as Strategic Ritual: An Examination of Newsmen’s Notions of\nObjectivity. American Journal of Sociology 77(4): 660–679.\nVatter, A. (2002). Kantonale Demokratien im Vergleich. Entstehungsgründe, Interaktionen und Wirkungen politischer Institutionen in den Schweizer Kantonen. Opladen: Leske und Budrich.\nWatts, M., D. Domke, D. Shah and D. Fan (1999). Elite Cues and Media Bias in Presidential Campaigns: Explaining Public Perceptions of a Liberal Press. Communication Research 26(2): 144–175.\nWuerth, A. (1999). Mediensystem und politische Kommunikation. In Klöti, U., P. Knoepfel,\nH. Kriesi, W. Linder and Y. Papadopoulos (eds.), Handbuch der Schweizer Politik. Zurich: NZZ\nVerlag (337–384).\nYankelovich, D. (1991). Coming to Public Judgment. Making Democracy Work in a Complex World.\nSyracuse: Syracuse University Press.\nYoung, I.M. (1990). Justice and the Politics of Diﬀerence. Princeton: Princeton University Press.\n—— (2000). Inclusion and Democracy. Oxford: Oxford University Press.\nZaller, J. and D. Chiu (1996). Government’s Little Helper: U.S. Press Coverage of Foreign Policy\nCrises, 1945–1991. Political Communication 13(4): 385–405.\n\nAppendix\nTable A1: List of all ballot measures analyzed in this study\nResult\n(share of yes\nvotes)\n\nBallot title\n571 10th amendment of retirement pension system (25.06.1995)\n572 Initiative on retirement pensions (25.06.1995)\n602 Law on labour: weekend and night work, maternity (01.12.1996)\n622 Financing of unemployment insurance (28.09.1997)\n\n 2011 Swiss Political Science Association\n\n60.7\n27.6\n33.0\n49.2\n\nSwiss Political Science Review (2011) Vol. 17(2): 128–163\n\n16626370, 2011, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/j.1662-6370.2011.02015.x by Cochrane Israel, Wiley Online Library on [09/12/2022]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\n162\n\n163\n\nTable A1: (Continued).\nResult\n(share of yes\nvotes)\n\nBallot title\n643 10th amendment of pension system without increase of retirement age\n(27.09.1998)\n654 Law on labour: night work, maternity (29.11.1998)\n684 Law on disability insurance (13.06.1999)\n685 Law on maternity insurance (13.06.1999)\n721 Initiative against increase of retirement age for women (26.11.2000)\n722 Initiative for ‘‘ﬂexible retirement age from 62 years’’ (26.11.2000)\n724 Initiative for ‘‘reduced hospital costs’’ (26.11.2000)\n732 Initiative for ‘‘cheaper drugs’’ (04.03.2001)\n752 Initiative to ‘‘secure pension system – tax energy instead of labour’’\n(02.12.2001)\n762 Reduction of work time (03.03.2002)\n781 Initiative about gold reserves to retirement pension (22.09.2002)\n782 Counter-proposal about gold reserves to retirement pension (22.09.2002)\n792 Law on unemployment insurance: unemployment beneﬁts (24.11.2002)\n802 Participation of cantons in ﬁnancing of hospital treatments (09.02.2003)\n815 Initiative ‘‘health must remain aﬀordable’’ (18.05.2003)\n816 Initiative ‘‘equal rights for disabled people’’ (18.05.2003)\n819 Initiative for ‘‘suﬃcient apprenticeship places’’ (18.05.2003)\n831 11th amendment of pension system: increase of pension age for women\n(16.05.2004)\n832 Financing of retirement pension through VAT increase (16.05.2004)\n844 Maternity insurance (26.09.2004)\n\n41.5\n63.4\n30.3\n39.0\n39.5\n46.0\n17.9\n30.9\n22.9\n25.4\n46.4\n46.4\n56.1\n77.4\n27.1\n37.7\n31.6\n32.1\n31.4\n55.5\n\nSource: Swiss Federal Chancellery.\n\nLionel Marquis received his PhD from the University of Geneva in 2002. Since 2008 he has been a lecturer and\nresearcher in political science at the University of Lausanne. His research interests comprise Swiss foreign and\nsocial policy, political behaviour and political psychology. Address for correspondence: Lionel Marquis, University\nof Lausanne, Institut d’Etudes Politiques, et Internationales, UNIL-Dorigny, Anthropole, 1015 Lausanne.\nPhone: +41 (0)21 692 31 56; Email: lionel.marquis@unil.ch\nHans-Peter Schaub is a PhD student at the University of Berne. He is working on the Swiss National Science Foundation project ‘‘The quality of democracy in the Swiss cantons’’ and writing his doctoral thesis on democracy’s\nquality in cantons with a popular assembly (Landsgemeinde) as compared to cantons with a ballot box system.\nAddress for correspondence: Hans-Peter Schaub, University of Berne, Institut für Politikwissenschaft, Lerchenweg\n36, 3000 Bern 9. Phone: +41 (0)31 631 48 49; Email: hans-peter.schaub@ipw.unibe.ch\nMarlène Gerber is a PhD student at the University of Berne. She is working on a project funded by the Swiss\nNational Science foundation that examines the potential for deliberation among EU-citizens. She writes her\ndoctoral thesis on deliberative quality in a European-wide deliberative poll on immigration. Address for correspondence: Marlène Gerber, University of Berne, Institut für Politikwissenschaft, Lerchenweg 36, 3000 Bern 9. Phone:\n+41 (0)31 631 83 37; Email: marlene.gerber@ipw.unibe.ch\n\n 2011 Swiss Political Science Association\n\nSwiss Political Science Review (2011) Vol. 17(2): 128–163\n\n16626370, 2011, 2, Downloaded from https://onlinelibrary.wiley.com/doi/10.1111/j.1662-6370.2011.02015.x by Cochrane Israel, Wiley Online Library on [09/12/2022]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n\nMedia Coverage of Referendum Campaigns\n\n</td>
    </tr>
    <tr>
      <th>48</th>
      <td>poli</td>
      <td>We Can Detect Your Bias: Predicting the Political Ideology of News Articles Ramy Baly1, Giovanni Da San Martino2, James Glass1, Preslav Nakov2 1MIT Computer Science and Artiﬁcial Intelligence Laboratory 3Qatar Computing Research Institute, HBKU fbaly,glassg@mit.edu fgmartino,pnakov g@hbku.edu.qa Abstract We explore the task of predicting the lead- ing political ideology or bias of news articles. First, we collect and release a large dataset of 34,737 articles that were manually annotated for political ideology –left, center, or right–, which is well-balanced across both topics and media. We further use a challenging exper- imental setup where the test examples come from media that were not seen during train- ing, which prevents the model from learning to detect the source of the target news arti- cle instead of predicting its political ideology. From a modeling perspective, we propose an adversarial media adaptation, as well as a spe- cially adapted triplet loss. We further add background information about the source, and we show that it is quite helpful for improv- ing article-level prediction. Our experimental results show very sizable improvements over using state-of-the-art pre-trained Transformers in this challenging setup. 1 Introduction In any piece of news, there is a chance that the viewpoint of its authors and of the media organiza- tion they work for, would be reﬂected in the way the story is being told. The emergence of the Web and of social media has lead to the proliferation of information sources, whose leading political ide- ology or bias may not be explicit. Yet, systematic exposure to such bias may foster intolerance as well as ideological segregation, and ultimately it could affect voting behavior, depending on the de- gree and the direction of the media bias, and on the voters’ reliance on such media (DellaVigna and Ka- plan, 2007; Iyengar and Hahn, 2009; Saez-Trumper et al., 2013; Graber and Dunaway, 2017). Thus, making the general public aware, e.g., by track- ing and exposing bias in the news is important for a healthy public debate given the important role media play in a democratic society.Media bias can come in many different forms, e.g., by omission, by over-reporting on a topic, by cherry-picking the facts, or by using propaganda techniques such as appealing to emotions, preju- dices, fears, etc. (Da San Martino et al., 2019, 2020a,b) Bias can occur with respect to a spe- ciﬁc topic, e.g., COVID-19, immigration, climate change, gun control, etc. (Darwish et al., 2020; Stefanov et al., 2020) It could also be more system- atic, as part of a political ideology, which in the Western political system is typically deﬁned as left vs. center vs. right political leaning. Predicting the bias of individual news articles can be useful in a number of scenarios. For news media, it could be an important element of internal quality assurance as well as of internal or external monitoring for regulatory compliance. For news aggregator applications, such as Google News, it could enable balanced search, similarly to what is found on AllSides.1For journalists, it could enable news exploration from a left/center/right angle. It could also be an important building block in a system that detects bias at the level of entire news media (Baly et al., 2018, 2019, 2020), such as the need to offer explainability, i.e., if a website is classiﬁed as left-leaning, the system should be able to pinpoint speciﬁc articles that support this decision. In this paper, we focus on predicting the bias of news articles as left-, center-, or right-leaning. Previous work has focused on doing so at the level of news media (Baly et al., 2020) or social me- dia users (Darwish et al., 2020), but rarely at the article level (Kulkarni et al., 2018). The scarce article-level research has typically used distant su- pervision, assuming that all articles from a given medium should share its overall bias, which is not always the case. Here, we revisit this assumption. 1http://allsides.com/arXiv:2010.05338v1  [cs.CL]  11 Oct 2020 Our contributions can be summarized as follows: •We create a new dataset for predicting the po- litical ideology of news articles. The dataset is annotated at the article level and covers a wide variety of topics, providing balanced left/center/right perspectives for each topic. •We develop a framework that discourages the learning algorithm from modeling the source instead of focusing on detecting bias in the article. We validate this framework in an ex- perimental setup where the test articles come from media that were not seen at training time. We show that adversarial media adaptation is quite helpful in that respect, and we further propose to use a triplet loss, which shows siz- able improvements over state-of-the-art pre- trained Transformers. •We further incorporate media-level representa- tion to provide background information about the source, and we show that this information is quite helpful for improving the article-level prediction even further. The rest of this paper is organized as follows: We discuss related work in Section 2. Then, we introduce our dataset in Section 3, we describe our models for predicting the political ideology of a news article in Section 4, and we present our experiments and we discuss the results in Section 5. Finally, we conclude with possible directions for future work in Section 6. 2 Related Work Most existing datasets for predicting the political ideology at the news article level were created by crawling the RSS feeds of news websites with known political bias (Kulkarni et al., 2018), and then projecting the bias label from a website to all articles crawled from it, which is a form of distant supervision. The crawling could be also done us- ing text search APIs rather than RSS feeds (Horne et al., 2019; Gruppi et al., 2020). The media-level annotation of political leaning is typically obtained from specialized online plat- forms, such as News Guard,2AllSides,3and Media Bias/Fact Check,4where highly qualiﬁed journal- ists use carefully designed guidelines to make the judgments. 2http://www.newsguardtech.com 3http://allsides.com/ 4http://mediabiasfactcheck.comAs manual annotation at the article level is very time-consuming, requires domain expertise, and it could be also subjective, such annotations are rarely available at the article level. As a result, automating systems for political bias detection have opted for using distant supervision as an easy way to obtain large datasets, which are needed to train contemporary deep learning models. Distant supervision is a popular technique for annotating datasets for related text classiﬁcation tasks, such as detecting hyper-partisanship (Horne et al., 2018; Potthast et al., 2018) and propa- ganda/satire/hoaxes (Rashkin et al., 2017). For example, Kiesel et al. (2019) created a large cor- pus for detecting hyper-partisanship (i.e., articles with extreme left/right bias) consisting of 754,000 articles, annotated via distant supervision, and ad- ditional 1,273 manually annotated articles, part of which was used as a test set for the SemEval-2019 task 4 on Hyper-partisan News Detection. The win- ning system was an ensemble of character-level CNNs (Jiang et al., 2019). Interestingly, all top- performing systems in the task achieved their best results when training on the manually annotated articles only and ignoring the articles that were la- beled using distant supervision, which illustrates the dangers of relying on distant supervision. Barr ´on-Cedeno et al. (2019) extensively dis- cussed the limitations of distant supervision in a text classiﬁcation task about article-level propa- ganda detection, in a setup that is similar to what we deal with in this paper: the learning systems may learn to model the source of the article instead of solving the task they are actually trained for. Indeed, they have shown that the error rate may drastically increase if such systems are tested on articles from sources that were never seen during training, and that this effect is positively correlated with the representation power of the learning model. They analyzed a number of representations and ma- chine learning models, showing which ones tend to overﬁt more, but, unlike our work here, they fell short of recommending a practical solution. Budak et al. (2016) measured the bias at the article level using crowd-sourcing. This is risky as public awareness of media bias is limited (Ele- jalde et al., 2018). Moreover, the annotation setup does not scale. Finally, their dataset is not freely available, and their approach of randomly crawling articles does not ensure that topics and events are covered from different political perspectives. Lin et al. (2006) built a dataset annotated with the ideology of 594 articles related to the Israeli- Palestinian conﬂict published on bitterlemons. org. The articles were written by two editors and 200 guests, which minimizes the risk of modeling the author style. However, the dataset is too small to train modern deep learning approaches. Kulkarni et al. (2018) built a dataset using distant supervision and labels from AllSides. Distant su- pervision is ﬁne for the purpose of training, but they also used it for testing, which can be problematic. Moreover, their training and test sets contain arti- cles from the same media, and thus models could easily learn to predict the article’s source rather than its bias. In their models, they used both the text and the URL contents of the articles. Overall, political bias has been studied at the level of news outlet (Dinkov et al., 2019; Baly et al., 2018, 2020; Zhang et al., 2019), user (Darwish et al., 2020), article (Potthast et al., 2018; Saleh et al., 2019), and sentence (Sim et al., 2013; Saez- Trumper et al., 2013). In particular, Baly et al. (2018) developed a system to predict the political bias and the factuality of news media. In a follow- up work, Baly et al. (2019) showed that bias and factuality of reporting should be predicted jointly. A ﬁner-grained analysis is performed in (Horne et al., 2018), where a model was trained on 10K sentences from a dataset of reviews (Pang and Lee, 2004), and used to discriminate objective versus non-objective sentences in news articles. Lin et al. (2006) presented a sentence-level classiﬁer, where the labels were projected from the document level. 3 Dataset In this section, we describe the dataset that we cre- ated and that we used in our experiments. While most of the platforms that analyze the political leaning of news media provide in-depth analysis of particular aspects of the media, AllSides stands out as it provides annotations of political ideology for individual articles, which ensures high-quality data for both training and testing, which is in contrast with distant supervision approaches used in most previous research, as we have seen above. In All- Sides, these annotations are made as a result of a rigorous process that involves blind bias surveys, editorial reviews, third-party analysis, independent reviews, and community feedback.5 5http://www.allsides.com/media-bias/ media-bias-rating-methodsFurthermore, AllSides uses the annotated arti- cles to enable its Balanced Search , which shows news coverage on a given topic from media with different political bias. In other words, for each trending event or topic (e.g., impeachment orcoro- navirus pandemic ), the platform pushes news ar- ticles from all sides of the political spectrum, as shown in Figure 1. We took advantage of this and downloaded all articles along with their political ideology annotations ( left,center , orright ), their assigned topic(s), the media in which they were published, their author(s), and their publication date. Thus, our dataset contains articles that were manually selected and annotated, and that are rep- resentative of the real political scenery. Note that thecenter class covers articles that are biased to- wards a centrist political ideology, and not articles that lack political bias (e.g., sports andtechnology ), which commonly exist in news corpora that were built by scraping RSS feeds. We collected a total of 34,737 articles published by 73 news media and covering 109 topics.6In this dataset, a total of 1,080 individual articles (3.11%) have a political ideology label that is different from their source’s. This suggests that, while the distant supervision assumption generally holds, we would still ﬁnd many articles that defy it. Table 1 shows some statistics about the dataset. Political Ideology Count Percentage Left 12,003 34.6% Center 9,743 28.1% Right 12,991 37.3% Table 1: Statistics about our dataset. Figure 2 illustrates the distribution of the differ- ent political bias labels within each of the most frequent topics. We can see that our dataset is able to represent topics or events from different political perspectives. This is yet another advantage, as it enables a more challenging task for machine learn- ing models to detect the linguistic and the semantic nuances of different political ideologies in news articles, as opposed to cases where certain topics might be coincidentally collocated with certain la- bels, in which case the models would be actually learning to detect the topics instead of predicting the political ideology of the target news article. 6In some cases, an article could be assigned to multiple topics, e.g., it could go simultaneously into coronavirus ,public health , and healthcare . Figure 1: AllSides: balanced search on the topic of reopening after the coronavirus lockdown . Figure 2: Political ideology for the most frequent top- ics:elections ,immigration ,coronavirus , and politics . It is worth noting that since most article labels are aligned with their source labels, it is likely that machine learning classiﬁers would end up model- ing the source instead of the political ideology of the individual articles. For example, a model would be learning the writing style of each medium, and then it would associate it with a particular ideology. Therefore, we pre-processed the articles in a way that eliminates explicit markers such as the name of the authors, or the name of the medium that usually appears as a preamble to the article’s content, or in the content itself. Furthermore, in order to ensure that we are actually modeling the political ideol- ogy as it is expressed in the language of the news, we created evaluation splits in two different ways: (i) randomly, which is what is typically done (for comparison only), and ( ii) based on media, where all articles by the same medium appear in either the training, the validation, or the testing dataset.The latter form of splitting would help us indi- cate what a trained classiﬁer has actually learned. For instance, if it modeled the source, then it would not be able to perform well on the test set, since all its articles would belong to sources that were never seen during training. In order to ensure fair one-to- one comparisons between experiments, we created these two different sets of splits, while making sure that they share the same test set, as follows: •Media-based Split: We sampled 1,200 arti- cles from 12 news media (100 per medium) and used them as the testset, and we excluded the remaining 5,470 articles from these media. Then, we used the articles from the remaining 61 media to create the training and the vali- dation sets, where all articles from the same medium would appear in the same set: train- ing, development, or testing. This ensures that the model is ﬁne-tuned and tested on articles whose sources were not seen during training. •Random Split: Here, the testset is the same as in the media-based split. The 5,470 articles that we excluded from the 12 media are now added to the articles from the 61 remaining media. Then, we split this collection of arti- cles (using stratiﬁed random sampling) into training andvalidation sets. This ensures that the model is ﬁne-tuned and evaluated only on articles whose sources were observed during training. Table 2 shows statistics about both splits, includ- ing the size of each set and the number of media and topics they cover. We release the dataset, along with the evaluation splits, and the code,7which can be used to extend the dataset as more news articles are added to AllSides. 7http://github.com/ramybaly/ Article-Bias-Prediction Train Valid. Test Media-basedCount 22,969 5,098 1,200 Media 46 15 12 Topics 108 105 93 RandomCount 26,828 6,709 1,200 Media 73 73 12 Topics 108 107 93 Table 2: Statistics about our dataset and its two splits: media-based andrandom . 4 Methodology 4.1 Classiﬁers The task of predicting the political ideology of news articles is typically formulated as a classi- ﬁcation problem, where the textual content of the articles is encoded into a vector representation that is used to train a classiﬁer to predict one of C classes (in our case, C= 3:left,center , and right ). In our experiments, we use two deep learning archi- tectures: ( i)Long Short-Term Memory networks (LSTMs), which are Recurrent Neural Networks (RNNs), which use gating mechanisms to selec- tively pass information across time and to model long-term dependencies (Hochreiter and Schmid- huber, 1997), and ( ii)Bidirectional Encoder Rep- resentations from Transformers (BERT), with a complex architecture yielding high-quality contex- tualized embeddings, which have been successful in several Natural Language Processing tasks (De- vlin et al., 2019). 4.2 Removing Media Bias Ultimately, our goal is to develop a model that can predict the political ideology of a news article. Our dataset, along with some others, has a special prop- erty that might stand in the way of achieving this goal. Most articles published by a given source have the same ideological leaning. This might con- fuse the model and cause it to erroneously associate the output classes with features that characterize en- tire media outlets (such as detecting speciﬁc writing patterns, or stylistic markers in text). Consequently, the model would fail when applied to articles that were published in media that were unseen during training. The experiments in Section 5 conﬁrm this. Thus, we apply two techniques to de-bias the mod- els, i.e., to prevent them from learning the style of a speciﬁc news medium rather than predicting the political ideology of the target news article.4.2.1 Adversarial Adaptation (AA) This model was originally proposed by Ganin et al. (2016) for unsupervised domain adaptation in im- age classiﬁcation. Their objective was to adapt a model trained on labelled images from a source domain to a novel target domain, where the images have no labels for the task at hand. This is done by adding an adversarial domain classiﬁer with a gradient reversal layer to predict the examples’ domains. The label predictor’s is minimized for the labelled examples (from the source domain), and the adversarial domain classiﬁer’s loss is max- imized for all examples in the dataset. As a result, the encoder can extract representation that is ( i) dis- criminative for the main task and also ( ii) invariant across domains (due to the gradient reversal layer). The overall loss is minimized as follows: X i=1:N di=0Li y(f;y) X i=1:NLi d(f;d); (1) whereNis the number of training examples, Li y(;)is the label predictor’s loss, the condi- tiondi= 0 means that only examples from the source domain are used to calculate the label pre- dictor’s loss,Li d(;)is the domain classiﬁer’s loss, controls the trade-off between both losses, and ff;y;dgare the parameters of the encoder, the label predictor, and the domain classiﬁer, respec- tively. Further details about the formulation of this method is available in (Ganin et al., 2016). We adapt this architecture as follows. Instead of adomain classiﬁer , we implement a media clas- siﬁer , which, given an article, tries to predict the medium it comes from. As a result, the encoder should extract representation that is discriminative for the main task of predicting political ideology, while being invariant for the different media. This approach was originally proposed as an unsuper- vised domain adaptation, since labelled examples were available for one domain only, whereas in our case, all articles from different media were labelled for their political ideology. Therefore, we jointly minimize the losses of both the label predictor and themedia classiﬁer over the entire dataset. The new objective function to minimize is as follows: X i=1:NLi y(f;y) X i=1:NLi m(f;m);(2) whereLi m(;)is the loss of the media classiﬁer , andmis its set of parameters. 4.2.2 Triplet Loss Pre-training (TLP) In this approach, we pre-train the encoder using a triplet loss (Schroff et al., 2015). The model is trained on a set of triplets, each composed of an anchor, a positive, and a negative example. The objective in Eq. 3 ensures that the positive example is always closer to the anchor than the negative example is, where a,pandnare the encodings of the anchor, of the positive, and of the negative examples, respectively, and D(;)is the Euclidean distance: L= max (D(a;p) D(a;n) +;0):(3) Figure 3 shows an example of such a triplet. The positive example shares the same ideology as the anchor’s, but they are published by different media. The negative example has a different ideology than the anchor’s, but they are published by the same medium. In this way, the encoder will be cluster- ing examples with similar ideologies close to each other, regardless of their source. Once the encoder has been pre-trained, its parameters, along with the softmax classiﬁer’s, are ﬁne-tuned on the main task by minimizing the cross-entropy loss when predicting the political ideology of articles. Figure 3: An example triplet used for de-biasing. 4.3 Media-level Representation Finally, we explore the beneﬁts of incorporating information describing the target medium, which can serve as a complementary representation for the article. While this seems to be counter-intuitive to what we have been proposing in Subsection 4.2, we believe that medium-level representation can be valuable when combined with an accurate represen- tation of the article. Intuitively, having an accurate understanding of the natural language in the article, together with a glimpse into the medium it is pub- lished in, should provide a more complete picture of its underlying political ideology.Baly et al. (2020) proposed a comprehensive set of representation to characterize news media from different angles: how a medium portrays itself, who is its audience, and what is written about it. Their results indicate that exploring the Twitter bios of a medium’s followers offers a good insight into its political leaning. To a lesser extent, the content of aWikipedia page describing a medium can also help unravel its political leaning. Therefore, we concatenated these representations to the encoded articles, at the output of the encoder and right be- fore the SOFTMAX layer, so that both the article encoder and the classiﬁcation layer that is based on the article and the external media representations are trained jointly and end-to-end. Similarly to (Baly et al., 2020), we retrieved the proﬁles of up to a 1,000 Twitter followers for each medium, we encoded their bios using the Sentence-BERT model (Reimers and Gurevych, 2019), and we then averaged these encodings to obtain a single representation for that medium. As for the Wikipedia representation, we automatically retrieved the content of the page describing each medium, whenever applicable. Then, we used the pre-trained base BERT model to encode this content by averaging the word representations ex- tracted from BERT’s second-to-last layer, which is common practice, since the last layer may be biased towards the pre-training objectives of BERT. 5 Experiments and Results We evaluated both the LSTM and the BERT mod- els, assessing the impact of ( i) de-biasing and (ii) incorporating media-level representation. 5.1 Experimental Setup We ﬁne-tuned the hyper-parameters of both models on the validation set using a guided grid search trial while ﬁxing the seeds of the random weights initialization. For LSTM, we varied the length of the input (128–1,024 tokens), the number of layers (1–3), the size of the LSTM cell (200–400), the dropout rate (0–0.8), the learning rate ( 1e 3to 1e 5), the gradient clipping value (0–5), and the batch size (8–256). The best results were obtained with a 512-token input, a 2-layer LSTM of size 256, a dropout rate of 0.7, a learning rate of 1e 3, gradient clipping at 0.5, and a batch size of 32. This model has around 1.1M trainable parameters, and was trained with 300-dimensional GloVe input word embeddings (Pennington et al., 2014). For BERT, we varied the length of the input, the learning rate, and the gradient clipping value. The best results were obtained using a 512-token input, a learning rate of 2e 5, and gradient clipping at 1. This model has 110M trainable parameters. We trained our models on 4 Titan X Pascal GPUs, and the runtime for each epoch was 25 seconds for the LSTM-based models and 22 minutes for the BERT-based models. For each experiment, the model was trained only once with ﬁxed seeds used to initialize the models’ weights. For the Adversarial Adaptation (AA), we have an additional hyper-parameter (see Equation 2), which we varied from 0 to 1, where 0 means no adaptation at all. The best results were obtained with= 0:7, which means that we need to pay signiﬁcant attention to the adversarial classiﬁer’s loss in order to mitigate the media bias. For the Triplet Loss Pre-training (PLT), we sam- pled 35,017 triplets from the training set, such that the examples in each triplet discuss the same topic in order to ensure that the change in topic has mini- mal impact on the distance between the examples. To evaluate our models, we use accuracy and macro-F1score (F1averaged across all classes), which we also used as an early stopping criterion, since the classes were slightly imbalanced. More- over, given the ordinal nature of the labels, we report the Mean Absolute Error (MAE), shown in Equation (4), whereNis the number of instances, andyiand^yiare the number of correct and of predicted labels, respectively. MAE =1 NNX i=1jyi ^yij (4) 5.2 Results Baseline Results The results in Table 3 show the performance for LSTM and for BERT at predicting the political ideology of news articles for both the media-based and the random splits. We observe sizable differences in performance between the two splits. In particular, both models perform much better when they are trained and evaluated on the random split, whereas they both fail on the media- based split, where they are tested on articles from media that were not seen during training. This observation conﬁrms our initial concerns that the models would tend to learn general characteristics about news media, and then would face difﬁculties with articles coming from new unseen media.Model Split Macro F1Acc. MAE Majority 19.61 41.67 0.92 LSTMMedia-based 31.51 32.30 0.97 Random 65.50 66.17 0.52 BERTMedia-based 35.53 36.75 0.90 Random 80.19 79.83 0.33 Table 3: Baseline experiments (without de-biasing or media-level representation) for the two splits. Removing the Source Bias In order to further conﬁrm the bias towards modeling the media, we ran a side experiment of ﬁne-tuning BERT on the task of predicting the medium given the article’s content, which is a 73-way classiﬁcation problem. We used stratiﬁed random sampling to create the evaluation splits and to make sure each set contains all labels (media). The results in Table 4 conﬁrm that BERT is much stronger than the majority class baseline, despite the high number of classes, which means that predicting the medium in which a target news article was published is a fairly easy task. Model Macro F1 Acc. Majority 0.25 10.21 BERT 59.72 80.12 Table 4: Predicting the medium in which a target news article was published. In order to remove the bias towards modeling the medium, we evaluated the impact of the adversarial adaptation (AA) and the Triplet Loss Pre-training (TLP) with the media-based split. The results in Table 5 show sizeable improvements when either of these approaches is used, compared to the base- line (no de-biasing). In particular, TLP yields an improvement of 14.12 points absolute in terms of accuracy, and 12.73 points in terms of macro- F1. Model De-bias Macro F1Acc. MAE LSTMNone 31.51 32.30 0.97 AA 40.33 40.57 0.69 TLP 45.44 46.42 0.62 BERTNone 35.53 36.75 0.90 AA 43.87 46.22 0.59 TLP 48.26 51.41 0.51 Table 5: Impact of de-biasing (adversarial adaptation and triplet loss) on article-level bias detection. LSTM BERT # Representation Macro F1Acc. MAE Macro F1Acc. MAE 1Article (baseline) 31.51 32.30 0.97 35.53 36.75 0.90 2Article with TLP 45.44 46.42 0.62 48.26 51.41 0.51 3Wikipedia 41.39 41.86 0.92 41.39 41.86 0.92 4Wikipedia +Article 40.49 40.79 0.92 42.33 41.90 0.90 5Wikipedia +Article with TLP 48.25 46.47 0.69 51.16 49.75 0.32 6Twitter bios 60.30 62.69 0.42 60.30 62.69 0.42 7Twitter bios +Article 60.30 62.69 0.42 60.42 63.12 0.40 8Twitter bios +Article with TLP 62.02 70.03 0.32 64.29 72.00 0.29 Table 6: Impact of adding media-level representations to the article-level representations (with and without de- biasing). Note that the results in rows 3 and 6 are the same for both LSTM and BERT because no articles were involved, and the media-level representations were directly used to train the classiﬁer. Impact of Media-Level Representation Fi- nally, we evaluated the impact of incorporating the media-level representation (Twitter followers’ bios and Wikipedia content) in addition to teh article- level representation. Table 6 illustrates these re- sults in an incremental way. First, we evaluated the performance of the media-level representation alone at predicting the political ideology of news articles (see rows 3 and 6). We should note that these results are identical for the LSTM and the BERT columns since no article was encoded in these experiments, and the media representation was used directly to train the logistic regression classiﬁer. Then, adding the article representation from either model, without any de-biasing, had no or little impact on the performance (see rows 4 vs. 3, and 7 vs. 6). This is not surprising, since we have shown that, without de-biasing, both models learn more about the source than about the bias in the language used by the article. Therefore, the ill-encoded articles do not provide more informa- tion than what the medium representation already gives, which is why no or too little improvement was observed. When we use the triplet loss to mitigate the source bias, the resulting article representation is more accurate and meaningful, and the medium rep- resentation does offer complementary information, and eventually contributes to sizeable performance gains (see rows 5 and 8 vs. 2). The Twitter bios rep- resentation appears to be much more important than the representation from Wikipedia, which shows the importance of inspecting the media followers’ background and their point of views, which is also one of the observations in (Baly et al., 2020).Overall, comparing the best results to the base- line (rows 8 vs. 1), we can see that ( i) using the triplet loss to remove the source bias, and ( ii) in- corporating media-level representation from Twit- ter followers yields 30.51 and 28.76 absolute im- provement in terms of macro F1on the challenging media-based split. 6 Conclusion and Future Work We have explored the task of predicting the leading political ideology of news articles. In particular, we created a new large dataset for this task, which fea- tures article-level annotations and is well-balanced across topics and media. We further proposed an adversarial media adaptation approach, as well as a special triplet loss in order to prevent modeling the source instead of the political bias in the news arti- cle, which is a common pitfall for approaches deal- ing with data that exhibit high correlation between the source of a news article and its class, as is the case with our task here. Finally, our experimental results have shown very sizable improvements over using state-of-the-art pre-trained Transformers. In future work, we plan to explore topic-level bias prediction as well as going beyond left-center- right bias. We further want to develop models that would be able to detect speciﬁc fragments in an article where the bias occurs, thus enabling explain- ability. Last but not least, we plan to experiment with other languages, and to explore to what extent a model for one language is transferable to another one given that the left-center-right division is not universal and does not align perfectly across coun- tries and cultures, even when staying within the Western political world. Acknowledgments This research is part of the Tanbih project8, which aims to limit the effect of “fake news,” propaganda and media bias by making users aware of what they are reading. The project is developed in col- laboration between the Qatar Computing Research Institute, HBKU and the MIT Computer Science and Artiﬁcial Intelligence Laboratory. References Ramy Baly, Georgi Karadzhov, Dimitar Alexandrov, James Glass, and Preslav Nakov. 2018. Predict- ing factuality of reporting and bias of news media sources. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , EMNLP ’18, pages 3528–3539, Brussels, Belgium. Ramy Baly, Georgi Karadzhov, Jisun An, Haewoon Kwak, Yoan Dinkov, Ahmed Ali, James Glass, and Preslav Nakov. 2020. What was written vs. who read it: News media proﬁling using text analysis and social media context. In Proceedings of the 58th An- nual Meeting of the Association for Computational Linguistics , ACL ’20, pages 3364–3374. Ramy Baly, Georgi Karadzhov, Abdelrhman Saleh, James Glass, and Preslav Nakov. 2019. Multi-task ordinal regression for jointly predicting the trustwor- thiness and the leading political ideology of news media. In Proceedings of the 17th Annual Confer- ence of the North American Chapter of the Associ- ation for Computational Linguistics: Human Lan- guage Technologies , NAACL-HLT ’19, pages 2109– 2116, Minneapolis, MN, USA. Alberto Barr ´on-Cedeno, Israa Jaradat, Giovanni Da San Martino, and Preslav Nakov. 2019. Proppy: Organizing the news based on their propagandistic content. Information Processing &amp; Management , 56(5):1849–1864. Ceren Budak, Sharad Goel, and Justin M Rao. 2016. Fair and balanced? Quantifying media bias through crowdsourced content analysis. Public Opinion Quarterly , 80(S1):250–271. Giovanni Da San Martino, Alberto Barr ´on-Cede ˜no, Henning Wachsmuth, Rostislav Petrov, and Preslav Nakov. 2020a. SemEval-2020 task 11: Detection of propaganda techniques in news articles. In Pro- ceedings of the International Workshop on Semantic Evaluation , SemEval ’20, Barcelona, Spain. Giovanni Da San Martino, Stefano Cresci, Alberto Barr´on-Cede ˜no, Seunghak Yu, Roberto Di Pietro, and Preslav Nakov. 2020b. A survey on compu- tational propaganda detection. In Proceedings of 8http://tanbih.qcri.org/the 29th International Joint Conference on Artiﬁ- cial Intelligence and the 17th Paciﬁc Rim Interna- tional Conference on Artiﬁcial Intelligence , IJCAI- PRICAI ’20, pages 4826–4832, Yokohama, Japan. Giovanni Da San Martino, Seunghak Yu, Alberto Barron-Cedeno, Rostislav Petrov, and Preslav Nakov. 2019. Fine-grained analysis of propaganda in news articles. In Proceedings of the 2019 Con- ference on Empirical Methods in Natural Language Processing , EMNLP ’19, pages 5636–5646, Hong Kong, China. Kareem Darwish, Michael Aupetit, Peter Stefanov, and Preslav Nakov. 2020. Unsupervised user stance de- tection on Twitter. In Proceedings of the Interna- tional AAAI Conference on Web and Social Media , ICWSM ’20, pages 141–152, Atlanta, GA, USA. Stefano DellaVigna and Ethan Kaplan. 2007. The Fox News effect: Media bias and voting. The Quarterly Journal of Economics , 122(3):1187–1234. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language under- standing. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies , NAACL-HLT ’19, pages 4171–4186, Min- neapolis, MN, USA. Yoan Dinkov, Ahmed Ali, Ivan Koychev, and Preslav Nakov. 2019. Predicting the leading political ide- ology of YouTube channels using acoustic, textual, and metadata information. In Proceedings of the 20th Annual Conference of the International Speech Communication Association , INTERSPEECH ’19, pages 501–505, Graz, Austria. Erick Elejalde, Leo Ferres, and Eelco Herder. 2018. On the nature of real and perceived bias in the main- stream media. PloS one , 13(3):e0193765. Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Franc ¸ois Lavi- olette, Mario Marchand, and Victor Lempitsky. 2016. Domain-adversarial training of neural net- works. The Journal of Machine Learning Research , 17(1):2096–2030. Doris A Graber and Johanna Dunaway. 2017. Mass media and American politics . SAGE Publications. Maur ´ıcio Gruppi, Benjamin D. Horne, and Sibel Adalı. 2020. NELA-GT-2019: A large multi-labelled news dataset for the study of misinformation in news arti- cles. arXiv preprint arXiv:2003.08444 . Sepp Hochreiter and J ¨urgen Schmidhuber. 1997. Long Short-Term Memory. Neural Computation , 9(8):1735–1780. Benjamin D. Horne, William Dron, Sara Khedr, and Sibel Adali. 2018. Assessing the news landscape: A multi-module toolkit for evaluating the credibility of news. In Proceedings of the The Web Conference , WWW ’18, pages 235–238, Lyon, France. Benjamin D Horne, Jeppe Nørregaard, and Sibel Adalı. 2019. Different spirals of sameness: A study of con- tent sharing in mainstream and alternative media. In Proceedings of the International AAAI Conference on Web and Social Media , ICWSM ’19, pages 257– 266, Munich, Germany. Shanto Iyengar and Kyu S Hahn. 2009. Red media, blue media: Evidence of ideological selectivity in media use. Journal of communication , 59(1):19–39. Ye Jiang, Johann Petrak, Xingyi Song, Kalina Bontcheva, and Diana Maynard. 2019. Team Bertha von Suttner at SemEval-2019 Task 4: Hyperpartisan news detection using ELMo sentence representation convolutional network. In Proceedings of the 13th International Workshop on Semantic Evaluation , Se- mEval ’19, pages 840–844, Minneapolis, MN, USA. Johannes Kiesel, Maria Mestre, Rishabh Shukla, Em- manuel Vincent, Payam Adineh, David Corney, Benno Stein, and Martin Potthast. 2019. SemEval- 2019 Task 4: Hyperpartisan news detection. In Pro- ceedings of the 13th International Workshop on Se- mantic Evaluation , SemEval ’19, pages 829–839, Minneapolis, Minnesota, USA. Vivek Kulkarni, Junting Ye, Steven Skiena, and William Yang Wang. 2018. Multi-view models for political ideology detection of news articles. In Pro- ceedings of the Conference on Empirical Methods in Natural Language Processing , EMNLP ’18, pages 3518–3527, Brussels, Belgium. Wei-Hao Lin, Theresa Wilson, Janyce Wiebe, and Alexander Hauptmann. 2006. Which side are you on? Identifying perspectives at the document and sentence levels. In Proceedings of the Tenth Confer- ence on Computational Natural Language Learning , CoNLL ’06, pages 109–116. Bo Pang and Lillian Lee. 2004. A sentimental edu- cation: Sentiment analysis using subjectivity sum- marization based on minimum cuts. In Proceed- ings of the 42nd Annual Meeting of the Association for Computational Linguistics , ACL ’04, pages 271– 278, Barcelona, Spain. Jeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. GloVe: Global vectors for word rep- resentation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Process- ing, EMNLP ’14, pages 1532–1543, Doha, Qatar. Martin Potthast, Johannes Kiesel, Kevin Reinartz, Janek Bevendorff, and Benno Stein. 2018. A stylo- metric inquiry into hyperpartisan and fake news. In Proceedings of the 56th Annual Meeting of the As- sociation for Computational Linguistics , ACL ’18, pages 231–240, Melbourne, Australia.Hannah Rashkin, Eunsol Choi, Jin Yea Jang, Svitlana V olkova, Yejin Choi, and Paul G Allen. 2017. Truth of varying shades: Analyzing language in fake news and political fact-checking. In Proceedings of the 2017 Conference on Empirical Methods in Natu- ral Language Processing , EMNLP ’17, pages 2931– 2937, Copenhagen, Denmark. Nils Reimers and Iryna Gurevych. 2019. Sentence- BERT: Sentence embeddings using Siamese BERT- networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Nat- ural Language Processing , EMNLP-IJCNLP ’19, pages 3973–3983, Hong Kong, China. Diego Saez-Trumper, Carlos Castillo, and Mounia Lal- mas. 2013. Social media news communities: Gate- keeping, coverage, and statement bias. In Proceed- ings of the 22nd ACM International Conference on Information &amp; Knowledge Management , CIKM ’13, page 1679–1684, San Francisco, CA, USA. Abdelrhman Saleh, Ramy Baly, Alberto Barr ´on- Cede ˜no, Giovanni Da San Martino, Mitra Mo- htarami, Preslav Nakov, and James Glass. 2019. Team QCRI-MIT at SemEval-2019 Task 4: Propa- ganda analysis meets hyperpartisan news detection. InProceedings of the 13th International Workshop on Semantic Evaluation , SemEval ’19, pages 1041– 1046, Minneapolis, MN, USA. Florian Schroff, Dmitry Kalenichenko, and James Philbin. 2015. FaceNet: A uniﬁed embedding for face recognition and clustering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , CVPR ’15, pages 815–823, Boston, MA, USA. Yanchuan Sim, Brice D. L. Acree, Justin H. Gross, and Noah A. Smith. 2013. Measuring ideological pro- portions in political speeches. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing , EMNLP ’13, pages 91–101, Seattle, Washington, USA. Peter Stefanov, Kareem Darwish, Atanas Atanasov, and Preslav Nakov. 2020. Predicting the topical stance and political leaning of media using tweets. InProceedings of the 58th Annual Meeting of the Association for Computational Linguistics , ACL ’20, pages 527–537. Yifan Zhang, Giovanni Da San Martino, Alberto Barr´on-Cede ˜no, Salvatore Romeo, Jisun An, Hae- woon Kwak, Todor Staykovski, Israa Jaradat, Georgi Karadzhov, Ramy Baly, Kareem Darwish, James Glass, and Preslav Nakov. 2019. Tanbih: Get to know what you are reading. In Proceedings of the 2019 Conference on Empirical Methods in Natu- ral Language Processing and the 9th International Joint Conference on Natural Language Processing , EMNLP-IJCNLP ’19, pages 223–228, Hong Kong, China.</td>
    </tr>
    <tr>
      <th>13</th>
      <td>poli</td>
      <td>royalsocietypublishing.org/journal/rsif Research Cite this article: Coscia M, Rossi L. 2020 Distortions of political bias in crowdsourcedmisinformation flagging. J. R. Soc. Interface 17: 20200020.http://dx.doi.org/10.1098/rsif.2020.0020 Received: 8 January 2020 Accepted: 13 May 2020 Subject Category: Life Sciences –Mathematics interface Subject Areas: computational biology, biotechnology Keywords: social media, social networks, content policing,flagging, fake news, echo chambers Author for correspondence: Michele Cosciae-mail: mcos@itu.dkDistortions of political bias in crowdsourced misinformation flagging Michele Coscia and Luca Rossi IT University of Copenhagen, Kobenhavn, Denmark MC, 0000-0001-5984-5137; LR, 0000-0002-3629-2039 Many people view news on social media, yet the production of news items online has come under fire because of the common spreading of misinforma- tion. Social media platforms police their content in various ways. Primarily they rely on crowdsourced ‘flags ’: users signal to the platform that a specific news item might be misleading and, if they raise enough of them, the item will be fact-checked. However, real-world data show that the most flagged news sources are also the most popular and —supposedly —reliable ones. In this paper, we show that this phenomenon can be explained by the unrea- sonable assumptions that current content policing strategies make about how the online social media environment is shaped. The most realistic assumptionis that confirmation bias will prevent a user from flagging a news item if they share the same political bias as the news source producing it. We show, via agent-based simulations, that a model reproducing our current understandingof the social media environment will necessarily result in the most neutral and accurate sources receiving most flags. 1. Introduction Social media have a central role to play in the dissemination of news [1]. There is a general concern about the low quality and reliability of information viewed online:researchers have dedicated increasing amounts of attention to the problem of so- called fake news [2 –4]. Given the current ecosystem of news consumption and pro- duction, misinformation should be understood within the complex set of social andtechnical phenomena underlying online news propagation, such as echo chambers [5–10], platform-induced polarization [11,12] and selective exposure [13,14]. Over the years two main approaches have emerged to try to address the problem of fake news by limiting its circulation: a technical approach and an expert-based approach. The technical approach aims at building predictive models able to detect misinformation [15,16]. This is often done using one ormore features associated with the message, such as content (through natural language processing (NLP) approaches [17]), source reliability [18] or network structure [19]. While these approaches have often produced promising results,the limited availability of training data as well as the unavoidable subjectivity involved in labelling a news item as fake [20,21] constitute a major obstacle to wider development. The alternative expert-based approach consists of a fact-checker on the specific topic that investigates and evaluates each claim. While this could be the most accurate way to deal with misinformation, given the amount ofnews that circulates on social media every second, it is hard to imagine how this could scale to the point of being effective. For this reason, the dominant approach, which has recently also been adopted by Facebook, 1is based on a combination of methods that first use computationally detected crowd signals, often constituted by users flagging what they consider fake or misleading infor- mation, and then assigning selected news items to external professional fact-checkers for further investigation [22,23]. Although flagging-based systems remain, to the best of our knowledge, widely used, many authors have ques- tioned their reliability, showing how users can flag news items for reasons © 2020 The Authors. Published by the Royal Society under the terms of the Creative Commons AttributionLicense http:/ /creativecommons.org/licenses/by/4.0/, which permits unrestricted use, provided the originalauthor and source are credited.  Downloaded from https://royalsocietypublishing.org/ on 27 November 2022  other than the ones intended [24,25]. Recently, researchers proposed methods to identify reliable users and improve, in that way, the quality of the crowd signal [20,23]. Regardless of the ongoing efforts, fake news and mislead- ing information still pollute online communications and no immediate solution seems to be available. In 2018, Facebook released, through the Social Science One initiative, theFacebook URL Shares dataset [26], a preview of the larger dataset released recently. 2The dataset contains the web page addresses (URLs) shared by at least 20 unique accountson Facebook between January 2017 and June 2018. Together with the URLs, the dataset also details whether the specific link had been sent to the third-party fact-checkers thatcollaborate with Facebook. We accessed the most shared links in the Italian subset, which revealed some curious patterns and inspired the pre-sent work. We exclusively use this dataset for the motivation and validation of our analysis, leaving the use of the newer full dataset for future work. Table 1 shows the top 10 most reported domains, which are exclusively major national newspapers, news sites and a satirical website. A further analysis of the data reveals, as figure 1 shows,a positive correlation ( y¼ bxafit, with slope α= 0.2, scale β= 1.22 and p&lt; 0.0013) between a source ’sp o p u l a r i t ya n dt h e number of times a domain has been checked by Facebook ’s third-party fact-checkers. We measure the popularity of the source through Alexa ’s (https://www.alexa.com) page views per million users (PVPM). It is worth observing that all the news reported in the top 10 most reported domains have been fact-checked astrue legitimate news (with the obvious exception of the satirical website, which was fact-checked as satire).These observations create the background for the present paper. Our hypothesis is that users are polarized and that polarization is an important driver of the decision of whetherto flag or not a news item: a user will only flag it if it is not perceived truthful enough andif it has a significantly differ- ent bias from that of the user (polarity). Sharing the samebias would act against the user ’s flagging action. Thus, we introduce a model of online news flagging that we call the ‘bipolar ’model, since we assume for simplicity that there are only two poles —roughly corresponding to ‘liberal ’and ‘conservative ’in the US political system. The bipolar model of news-flagging attempts to capture the main ingredients thatwe observe in empirical research on fake news and disinforma- tion—echo chambers, confirmation bias, platform-induced polarization and selective exposure. We show how the proposedmodel provides a reasonable explanation of the patterns that we observe in Facebook data. The current crowdsourced flagging systems seem to assume a simpler flag-generating model. Despite being some- how similar to the bipolar model we propose, in this simple case the model does not account for users ’polarization, thus we will call it the ‘monopolar ’model. In the monopolar model, users do not gravitate around two poles and perceived truthfulness constitutes the only parameter. Users flagnews items only if they perceive an excessive ‘fakeness ’of the news item, depending of their degree of scepticism. We show how the monopolar model relies on unrealistic expectationsand that it is unable to reproduce the observed flag-generating patterns. Lastly, we test the robustness of the bipolar model against various configurations of the underlying network structure and the actors ’behaviour. We show, on the one hand, how the model is always able to explain the observed flaggingphenomenon and, on the other hand, that a complex social network structure is a core element of the system. 2. Methods In this section, we present the main model on which we base the results of this paper. It is possible to understand the bipolar andmonopolar models as a single model with or without users ’ polarization. However, a user ’s polarization has a significant impact on the results, and it seriously affects the social networkunderlying the flagging and propagation processes. For theseTable 1. The top 10 most ﬂagged domains among the Italian links shared on the Facebook URL Shares dataset. domain reported PVPM type 1 repubblica.it 270.00 54.00 national newspaper 2 ilfattoquotidiano.it 85.00 21.00 national newspaper 3 corriere.it 83.00 30.00 national newspaper 4 fanpage.it 49.00 5.00 national news site 5 ansa.it 47.00 12.00 national news site 6 huf ﬁngtonpost.it 40.00 7.20 national news site 7 ilmessaggero.it 34.00 2.00 national newspaper 8 ilsole24ore.com 32.00 4.00 national newspaper 9 lercio.it 29.00 3.00 satire10 tgcom24.mediaset.it 28.00 28.00 national news site110102103 10–210–11 10 102no. flags PVPM Figure 1. The relationship between the web traffic of a website ( x-axis) and the number of flags it received on Facebook ( y-axis). Traffic is expressed in PPVM, which indicates what fraction of all the page views by Alexa toolbar users go to a particular site.royalsocietypublishing.org/journal/rsif J. R. Soc. Interface 17: 202000202  Downloaded from https://royalsocietypublishing.org/ on 27 November 2022  reasons, in the paper, we will refer to them as two different models with two different names, which makes the comparisoneasier to grasp. In the following, we start by giving a general overview of the bipolar model (§2.1). In the subsequent sections, we provide themodel details, motivating each choice on the basis of real-worlddata. We conclude by showing the crucial differences betweenthe bipolar and monopolar models (§2.5). We note that our model shares some commonalities with the bounded confidence model [27]. 2.1. Model overview Figure 2 shows a general depiction of the bipolar model. In the bipo-lar model, we have two kinds of agents: news sources and users. News sources are characterized by three values: popularity, polarity and truthfulness. The popularity distributes broadly:there are a few big players with a large following while themajority of sources are followed by only a few users. The polaritydistributes quasi-normally. Most sources are neutral and there areprogressively fewer and fewer sources that are more polarized.Truthfulness is linked to polarity, with more polarized sourcestending to be less truthful. This implies that most news sourcesare truthful, and less trustworthy sources are more and morerare. Each news item has the same polarity and truthfulnessvalues as the news source publishing it. Users only have polarity. The polarity of the users distributes in the same way as that of the news sources. Most users aremoderate and extremists are progressively more rare. Usersfollow news sources, preferentially those of similar polarity(selective exposure). Users embed in a social network, preferen- tially being friends of other users of similar polarity (homophily). A user can see a news item if the item is either published by a source the user is following or reshared by one of their friends. Ineither case, the user can do one of three things: 1. reshare —if the polarity of the item is sufficiently close to their own andthe item is sufficiently truthful; 2. flag —if the polarity of the item is sufficiently different from their own orthe item is not truthful enough; 3. consume —in all other cases, meaning that the item does not propagate and nor is it flagged. We expect the bipolar model to produce mostly flags in the moderate and truthful part of the spectrum. We base this expec-tation on the following reasoning. Since most news sources aremoderate and truthful, the few very popular sources are over- whelmingly more likely to be moderate and truthful. Thus wewill see more moderate and truthful news items, which aremore likely to be reshared. This resharing activity will causethe news items published by the moderate and truthful newssources to be shared to the polarized parts of the network.Here, given that the difference between the polarization of theuser and the polarization of the source plays a role in flaggingeven relatively truthful items, moderate and truthful newsitems are likely to be flagged. Polarized and untruthful items, on the other hand, are unli- kely to be reshared. Because of the polarization homophily thatcharacterizes the network structure, they are unlikely to reachthe more moderate parts of the network. If polarized itemsare not shared, they cannot be flagged. A neutral item is morelikely to be shared, and thus could reach a polarized user, whowould flag it. Thus, most flags will hit moderate and truthfulnews items, rendering the whole flagging mechanism unsuitablefor discovering untruthful items. 2.2. Agents In this section, we detail how we build the main agents in ourmodel: the news sources and the users. As mentioned previously, news sources have a certain popu- larity. The popularity of a news source is the number of usersfollowing it. We generate the source popularity distribution asa power law. This means that the vast majority of news sourceshave a single follower, while the most popular sources have thousands of followers. This is supported by real-world data. Figure 3 ashows the complement cumulative distribution of the number of followersof Facebook pages. These data come from CrowdTangle. 4As we can see, the distribution has a long tail: two out of three Face-book pages have 10 000 followers or fewer. The most popularpages are followed by more than 60 million users. As for the user and source polarities ( p uand pi), we assume that they distribute quasi-normally. We create a normal distributionwith average equal to zero and standard deviation equal to 1.Then we divide it by its maximum absolute value to ensure thatthe distribution fully lies between −1 and 1. In this way we ensure that most users are moderates; more extreme users/sourcesare progressively more rare, at both ends of the spectrum. This is also supported by the literature [28] and by real-world data. Figure 3 bshows the distribution of political leaning in the USA across time [29], collected online. 5These data were collectedsourcespopularity polarity truth userspolarit ypublish reshare degreefrom friend flagfi,u =ti |pi – pu| consumefi,u = 1 –fi,u fi,u + 1 fi,u &lt; r fi,u &gt; f Figure 2. The overview of the bipolar model. From left to right, we show: the characteristics of the agents (source ’s polarities, popularity and truthfulness; and user ’s polarity); the model ’s structures (the bipartite source –user follower network and the unipartite user –user social network); and the agents ’actions (source publishing and users resharing, consuming and flagging news items).royalsocietypublishing.org/journal/rsif J. R. Soc. Interface 17: 202000203  Downloaded from https://royalsocietypublishing.org/ on 27 November 2022  by surveying a representative sample of the US electorate via phone and face-to-face interviews. While not perfectly normally distributed, the data show that the majority of Americans either feel they are moderate or do notknow to which side they lean. ‘Moderate ’or‘don’t know ’is always the mode of the distribution, and their combinationis always the plurality option. Finally, sources have a degree of truthfulness t i. Here, we make the assumption that this is correlated with the newssource ’s polarity. The more a source is polarized, the less it is interested in the actual truth. A polarized source wants tobring readers onto their side, and their ideology clouds theirbest judgement of truthfulness. This reasonable assumption isalso supported by the literature [30]. Mathematically , this means that t i=1−|pi|+ϵ, with −0.05≤ ϵ≤0.05 being extracted uniformly at random, ensuring then that ti remains between 0 and 1 by capping it to these values. 2.3. Structures There are two structures in the model: the user –source bipartite network and the user –user social network. 2.3.1. User –source network The user –source network connects users to the news sources they are following. This is the primary channel through which usersare exposed to news items. We fix the degree distribution of the sources to be a power law, as we detailed in the previous section. The degree distribution ofthe user depends on the other rules of the model. There is a certainnumber of users with degree zero in this network. These users donot follow any news source and only react to what is shared bytheir circle of friends. We think this is reasonably realistic. We connect users to sources to maximize polarity homophily. The assumption is that users will follow news organizationssharing their polarity. This assumption is supported by theliterature [31,32]. For each source with a given polarity and popularity, we pick the required number of individuals with polarity values in aninterval around the source polarity. For instance, if a sourcehas popularity of 24 and polarity of 0.5, we will pick the 24users whose polarity is closest to 0.5 and we will connect themto the source. 2.3.2. Social network Users connect to each other in a social network. The social net-work is the channel through which users are exposed to newsitems from sources they are not following.We aim at creating a social network with realistic character- istics. For this reason, we generate it via an Lancichinetti – Fortunato –Radicchi (LFR) benchmark 6[33]. The LFR benchmark ensures that the social network has a community structure, abroad degree distribution, and communities are overlapping,i.e. they can share nodes. All these characteristics are typical ofreal-world social networks. We fix the number of nodes to≈16 000, while the number of communities is variable and not fixed by the LFR ’s parameters. We need an additional feature in the social network: polarity homophily. People are more likely to be friends with like-mindedindividuals. This is supported by studies of politics on socialmedia [34]. We ensure homophily by iterating over all communitiesgenerated by the LFR benchmark and assigning to users grouped inthe same community a portion of the polarity distribution. For instance, if a community includes 12 nodes, we take 12 con- secutive values in the polarity distribution and we assign themto the users. This procedure generates extremely high polarityassortativity. The Pearson correlation of the polarity values at thetwo endpoints of each edge is ≈0.89. 2.4. Actions A news source publishes to all the users following it an item i carrying the source ’s polarity piand truthfulness ti. Every time a user sees an item i, it calculates how acceptable the item is, using the function fi,u. An item is acceptable if it is (i) truthful and (ii) it is not far from the user in the polarity spectrum — experiments [35] show how this is a reasonable mechanics:users tend to trust more sources with a similar polarity to theirown. Mathematically, (i) means that f i,uis directly proportional toti; while (ii) means that fi,uis inversely proportional to the difference between piand pu fi,u¼ti jpi/C0puj: The acceptability function fi,uhas two issues: first, its domain spans from 0 (if ti=0 )t o+ ∞(ifpi=pu). This can be solved by the standard transformation x/(x+ 1), which is always between 0 and 1 if x≥0. Second, for the discussion of our parameters and results, it is more convenient to estimate a degree of ‘unacceptability ’, which is the opposite of the acceptability fi,u. This can be achieved by the standard transformation 1 −x. Putting the two transformations together, the unacceptability fi,uof item ifor user uis fi,u¼1/C0fi,u fi,uþ1:10–410–310–210–11 1 10 102103p (followers ≥ x) followers (×10 k)0100200300400500600700 EL L SL M DK SC C ECcount polarit y(b)(a) Figure 3. (a) The cumulative distribution of source popularity on Facebook in our dataset: the probability ( y-axis) of a page to have a given number of followers or more ( x-axis). ( b) The polarity distribution in the USA from 1994 (light) to 2016 (dark). Biannual observation, except for missing years 2006, 2010 and 2014. EL, extremely liberal; L, liberal; SL, slightly liberal; M, moderate; DK, don ’t know; SC, slightly conservative; C, conservative; EC, extremely conservative.royalsocietypublishing.org/journal/rsif J. R. Soc. Interface 17: 202000204  Downloaded from https://royalsocietypublishing.org/ on 27 November 2022  Users have a finite tolerance for how unacceptable a news item can be. If the item exceeds this threshold, meaning fi,u.f, the user will flag the item. On the other hand, if the news item has low to zero unacceptability, meaning fi,u,r, the user will reshare it to their friends. If r/C20fi,u/C20f, the user will neither flag nor reshare the item. The parameters ϕand ρregulate which and how many news items are flagged, and thus we need to tune them to generaterealistic results —as we do in the Results section. 2.5. Monopolar model The monopolar model is the result of removing everything related to polarity from the bipolar model. The sharing and flag-ging criteria are the same as in the bipolar model —testing fi,u against the ρand ϕparameters, with the difference being in how fi,uis calculated. The unacceptability of a news item is now simply the opposite of its truthfulness, i.e. fi,u¼1/C0ti. Moreover, in the monopolar model users connect to random news sources and there is no polarity homophily in thesocial network. The monopolar model attempts to reproduce the assumption of real-world crowdsourced flagging systems: only the least truthfularticles are flagged. However, we argue that it is not a good rep-resentation of reality because truthfulness assessment is not anobjective process: it is a subjective judgement and it includes pre-existing polarization of both sources and users. The bipolar modelcan capture such polarization while the monopolar model cannot. 2.6. Example To understand what happens in the bipolar and monopolarmodels, consider figure 4 as a toy example. Table 2 a,bcalculates fi,ufor all user –source pairs in the bipolar and monopolar models, respectively. Table 3 a,bcounts the number of flags received by each source for different combinations of the ρand ϕparameters in the bipolar and monopolar models, respectively. A few interesting differences between the bipolar and monopolarmodels appear. In the monopolar model, only the direct audience of a source can flag its news items and, if one member of the direct audienceflags, so will all of them. This is because fi,uis equal for all nodes, thus either fi,u.fand the entire audience will flag the item (and no one will reshare it) or fi,u,rand the entire network —not just the audience —will reshare the item, and no one will ever flag it. This is not true for the bipolar model. S1 (figure 4) can be either flagged by its entire audience ( ϕ= 0.14); by part of its audi- ence ( ϕ= 0.3); or by nodes who are not in its audience at all (users U5 and U6 for ϕ= 0.44; or user U7 for ϕ= 0.6). On the other hand, in our examples, S2 is never flagged by its audience (U7). WhenS2 is flagged, it is always because it percolated to a user for which fi,u.f, via a chain of users for which fi,u,r, because fi,uis not constant across users any longer. 3. Results 3.1. Parameter tuning Before looking at the results of the model, we need to identify the range of parameter values that can support robust andU5S2 S1 U3 U1 U7 U2 U4 U6pi = 0.5 ti = 0.55 pu = 0.8 pu = 0.6 pu = 0.4 pu = 0.2 pu = –0.2 pu = –0.45 pu = 0pi = –0.5 ti = 0.45 Figure 4. Two simple structures with sources (squares) and users (circles). Edges connect sources to the users following them and users to their friends. Each s ource has an associated tiand pivalue and each user has an associated puvalue next to their respective nodes. Table 2. The fi,uvalue for each user –source pair from ﬁgure 4 in the ( a) bipolar and ( b) monopolar models. (a) bipolar ’sfi,u (b) monopolar ’sfi,u user S1 S2 user S1 S2 U1 0.35 0.74 U1 0.45 0.55 U2 0.15 0.71 U2 0.45 0.55 U3 0.15 0.66 U3 0.45 0.55 U4 0.35 0.61 U4 0.45 0.55U5 0.48 0.52 U5 0.45 0.55 U6 0.56 0.40 U6 0.45 0.55 U7 0.62 0.10 U7 0.45 0.55 Table 3. The number of ﬂags each source in ﬁgure 4 gets in the ( a) bipolar and ( b) monopolar models, for varying values of ρandϕ. (a) bipolar ( b) monopolar ρϕ S1 S2 ρϕ S1 S2 0.67 0.7 0 2 0.67 0.7 0 0 0.57 0.6 1 1 0.57 0.6 0 0 0.49 0.54 1 1 0.49 0.54 0 1 0.36 0.44 2 0 0.36 0.44 4 10.2 0.3 2 1 0.2 0.3 4 1 0.1 0.6 0 0 0.1 0.6 0 0 0.1 0.5 0 0 0.1 0.5 0 1 0.1 0.14 4 0 0.1 0.14 4 1royalsocietypublishing.org/journal/rsif J. R. Soc. Interface 17: 202000205  Downloaded from https://royalsocietypublishing.org/ on 27 November 2022  realistic results. The most important of the two parameters isϕ, because it determines the number of flags generated in the system. Figure 5 ashows the total number of flags generated per value of ϕ. As expected, the higher the ϕ, the fewer the flags, as the user finds more news items acceptable. The sharp drop means that, for ϕ&gt; 0.6, we do not have a sufficient number of flags to support our observation of the model ’s be- haviour. Thus, hereafter, we will only investigate the behaviour of the model for ϕ≤0.6. ρis linked to ϕ; specifically , its value is capped by ϕ. Aworld with ρ≥ϕis unreasonable, because it would be a scenario where a user feels enough indignation by an item that theywill flag it, but then they will also reshare it to their social network. Thus, we only test scenarios in which ρ&lt;ϕ. Another important question is what combination of ϕand ρvalues generates flags that can reproduce the observed relation between source popularity and the number of flagswe see in figure 1. To do so, we perform a grid search, testing many combinations of ϕ–ρvalues. Our quality criterion is the absolute difference in the slope of the power fit betweenpopularity and the number of flags. The lower the difference, the better the model is able to approximate reality. Figure 5 bshows such a relationship. We can see that there is an area of high performance at all levels of ϕ. 3.2. Bipolar model Figure 6 shows the distribution of the polarity of the flagged news items, for different values of ϕand setting ρ= 0.08, an interval including the widest spectrum of goodness of fit asshown in figure 5 b. We run the model 50 times and take the average of the results, to smooth out random fluctuations. We can see that our hypothesis is supported: in a polarized environment the vast majorityofflagged news items are neutral. This happens for ϕ≤0.3, which, as we saw in figure 5 b,i st h e most realistic scenario. For ϕ≥0.4, our hypothesis would not be supported, but, as we can see in figure 5 b, this is the area in red, where the model is a bad fit for the observations anyway —since here we are looking at ρ= 0.08 results. Figure 7 shows the distribution of truthfulness of the flagged items. These distributions show that, by flagging following their individual polarization, users in the bipolarmodel end up flagging the most truthful item they can —if ϕis high enough, items with t i∼1 cannot be flagged almost regardless of the polarity difference.The two observations put together mean that, in the bipolar model, the vast majority of flags come from extremists who are exposed to popular neutral and truthful news. The extremists do not follow the neutral and truthful news sources, but getin contact with neutral and truthful viewpoints because of their social network. The bipolar model results —in accordance with the obser- vation from figure 1 —suggest that more popular items are s h a r e dm o r ea n dt h u sf l a g g e dm o r e .O n ec o u l db et e m p t e dt o identify and remove fake news items by taking the ones receivingmore than their fair shares of flags given their popularity. How- ever, such a simple system would not work in reality. Figure 1 is based on data coming after Facebook ’s machine learning pre- processor, the aim of which is to minimize false positives. 7 Thus, even after controlling for a number of factors —source popularity, reputation, etc. —most reported flags still end up attached to high-popularity, high-reputability sources. 3.3. Monopolar model In the monopolar model, we remove all aspects related topolarity, thus we cannot show the polarity distribution of the flags. Moreover, as we have shown in §2.6, the effect ofρand ϕis marginal. Thus we only show in figure 8 the truth- fulness distribution of the flags, for only ϕ= 0.1 and ρ= 0.08, noting that all other parameter combinations result in apractically identical distribution. The monopolar results show the flag truthfulness distribution as the ideal result. The distribution shows a dispro-portionate number of flags going to low truthfulness news items, as they should —the drop for the lowest truthfulness value is due to the fact that there are few items at that lowlevel of truthfulness, and that they are not reshared. Is this ideal result realistic? If we use the same criterion as we used for the bipolar model to evaluate the quality of themonopolar model, the answer is no. The absolute slope difference in the popularity –flag regression between obser- vation and the monopolar model is ≈0.798 for all ϕ–ρ combinations. This is a significantly worse performance than the worst-performing versions of the bipolar model — figure 5 bshows that no bipolar version goes beyond a slope difference of 0.5. Thus we can conclude that the monopolar model is not a realistic representation of reality, even if we would expect it tocorrectly flag the untruthful news items. The bipolar model is a better approximation, and results in flagging truthful news items. 0.1 0.2 0.3 0.4 0.5 0.6 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 0.45r 00.050.100.150.200.250.300.350.400.450.50 abs slo pe difference 02.0 × 1054.0 × 1056.0 × 1058.0 × 1051.0 × 1061.2 × 1061.4 × 1061.6 × 1061.8 × 1062.0 × 106 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9no. flags ff(b) (a) Figure 5. (a) The number of flags ( y-axis) in the bipolar model for different values of ϕ(x-axis). ( b) The slope difference (colour; red = high, green = low) between the real world and the bipolar fit between the source popularity and the number of flags received, per combination of ϕandρvalues ( x–yaxis).royalsocietypublishing.org/journal/rsif J. R. Soc. Interface 17: 202000206  Downloaded from https://royalsocietypublishing.org/ on 27 November 2022  3.4. Robustness Our bipolar model makes a number of simplifying assumptions that we need to test. First, we are showing results for a model in which all news sources have the same degree of activity ,meaning that each source will publish exactly one news item. This is not realistic: data from Facebook pages show that there is a huge degree of activity heterogeneity (figure 9 a). There is a mild positive correlation between the popular- ity of a page and its degree of activity (log-log Pearson correlation of ≈0.12; figure 9 b). For this reason, we use the real-world distribution of page popularity and we lock it in with its real-world activity level. This is the weighted bipolar model, in which each synthetic news source is the model ’s equivalent of a real page, with its popularity and activity. A second simplifying assumption of the bipolar model is that the reshareability and flaggability parameters ρand ϕarethe same for every individual in the social network. However, people might have different trigger levels. Thus we create thevariable bipolar model, where each user has its own ρ uand ϕu. These values are distributed normally, with their average /C22r¼0:08 (and standard deviation 0.01) and /C22fdepending on which average value of ϕwe are interested in studying (with the standard deviation set to one-eighth of /C22f). Figure 10 shows the result of the weighted and variable variants against the original bipolar model. In figure 10 a, we report the dispersion (standard deviation) of the polariz- ation values of the flags. A low dispersion means that flagscluster in the neutral portion of the polarity spectrum, mean- ing that most flags signal neutral news items. In figure 10 b, we report the average truthfulness of flagged items. We can see that taking into account the pages ’activities increases the dispersion by a negligible amount and only020406080100120140no. flags polarity010203040506070no. flags polarity 0510152025 –1.0–0.9–0.8–0.7–0.6–0.5–0.4–0.3–0.2–0.100.10.20.30.40.50.60.70.80.91.0 –1.0–0.9–0.8–0.7–0.6–0.5–0.4–0.3–0.2–0.100.10.20.30.40.50.60.70.80.91.0no. flags polarity012345678no. flags polarity 00.20.40.60.81.01.21.4 –1.0–0.9–0.8–0.7–0.6–0.5–0.4–0.3–0.2–0.100.10.20.30.40.50.60.70.80.91.0–1.0–0.9–0.8–0.7–0.6–0.5–0.4–0.3–0.2–0.100.10.20.30.40.50.60.70.80.91.0–1.0–0.9–0.8–0.7–0.6–0.5–0.4–0.3–0.2–0.100.10.20.30.40.50.60.70.80.91.0 –1.0–0.9–0.8–0.7–0.6–0.5–0.4–0.3–0.2–0.100.10.20.30.40.50.60.70.80.91.0no. flags polarity00.010.020.030.040.050.060.070.080.090.10no. flags polarity(e)( f)(b) (a) (c) (d) Figure 6. Flag count per polarity of items at different flaggability thresholds ϕfor the bipolar model. Reshareability parameter ρ= 0.08. Average of 50 runs. (a)ϕ= 0.1, ( b)ϕ= 0.2, ( c)ϕ= 0.3, ( d)ϕ= 0.4, ( e)ϕ= 0.5 and ( f)ϕ= 0.6.royalsocietypublishing.org/journal/rsif J. R. Soc. Interface 17: 202000207  Downloaded from https://royalsocietypublishing.org/ on 27 November 2022  for high values of ϕ. This happens because there could be some extremely active fringe pages spamming fake content, which increases the likelihood of extreme flags. There is no difference in the average truthfulness of flagged items. Having variable ϕandρvalues, instead, actually decreases dispersion, making the problem worse —although only for larger values of ϕ. In this configuration, a very tolerant society with high (average) ϕwould end up flagging mostly neutral reporting —as witnessed by the higher average truthfulness of the reported items. This is because lower-than-average ρu users will be even less likely to reshare the most extreme news items. So far we have kept the reshareability parameter constant atρ= 0.08. If we change ρ(figure 11) the dispersion of a flag ’s polarity (figure 11 a) and its average truthfulness value (figure 11 b) do not significantly change. The changes are050100150200250 00.10.20.30.40.50.60.70.80.91.000.10.20.30.40.50.60.70.80.91.0 00.10.20.30.40.50.60.70.80.91.000.10.20.30.40.50.60.70.80.91.0 0.00.10.20.30.40.50.60.70.80.91.000.10.20.30.40.50.60.70.80.91.0no. flags truthfulness020406080100120140no. flags truthfulness 05101520253035404550no. flags truthfulness0246810121416no. flags truthfulness 00.51.01.52.02.53.0no. flags truthfulness00.020.040.060.080.100.120.140.160.180.20no. flags truthfulness(e)( f)(b) (a) (c) (d) Figure 7. Flag count per truthfulness of items at different flaggability thresholds ϕfor the bipolar model. Reshareability parameter ρ= 0.08. Average of 50 runs. (a)ϕ= 0.1, ( b)ϕ= 0.2, ( c)ϕ= 0.3, ( d)ϕ= 0.4, ( e)ϕ= 0.5 and ( f)ϕ= 0.6. 00.51.01.52.02.53.03.54.0 00.10.20.30.40.50.60.70.80.91.0no. flags truthfulness Figure 8. Flag count per truthfulness of items for the monopolar model for ϕ= 0.6. Average of 50 runs.royalsocietypublishing.org/journal/rsif J. R. Soc. Interface 17: 202000208  Downloaded from https://royalsocietypublishing.org/ on 27 November 2022  10–410–310–210–11 1 10 102103p (shares ≥x) shares01234567 00.5 1.0 1.5 2.0 2.5 3.0 3.5log (followers) log (shares) 110102 no. sources(b) (a) Figure 9. (a) The cumulative distribution of source activity in Facebook in our dataset: the probability ( y-axis) of a news source sharing a given number of items or more ( x-axis). ( b) The relationship between activity ( x-axis) and popularity ( y-axis) in our Facebook dataset. 00.10.20.30.40.50.6s(pi)bipolar weighted variable 00.10.20.30.40.50.60.70.80.9 0.1 0.2 0.3 0.4 0.5 0.6m(ti) f0.1 0.2 0.3 0.4 0.5 0.6 fbipolar weighted variable(b) (a) Figure 10. Dispersion of polarization ( a) and average truthfulness ( b) of the flagged items in the bipolar model and its weighted and variable variants.s(pi) 0.1 0.2 0.3 0.4 0.5 0.6m(ti) f0.1 0.2 0.3 0.4 0.5 0.6 f(b) (a) 00.10.20.30.40.50.6 r = 0.03 r = 0.04 r = 0.05 r = 0.06 r = 0.07 r = 0.08 00.10.20.30.40.50.60.70.80.9 r = 0.03 r = 0.04 r = 0.05 r = 0.06 r = 0.07 r = 0.08 Figure 11. Dispersion of polarization ( a) and average truthfulness ( b) of the flagged items for different values of reshareability ρ.s(pi) m(ti) 0.1 0.2 0.3 0.4 0.5 0.6 f0.1 0.2 0.3 0.4 0.5 0.6 f(b) (a) 00.10.20.30.40.50.60.7 bipolar no-homophily no-community 00.10.20.30.40.50.60.70.80.91.0 bipolar no-homophily no-community Figure 12. Dispersion of polarization ( a) and average truthfulness ( b) of the flagged items in the bipolar and alternative models.royalsocietypublishing.org/journal/rsif J. R. Soc. Interface 17: 202000209  Downloaded from https://royalsocietypublishing.org/ on 27 November 2022  due to the fact that ρsimply affects the number of flags: a higher ρmeans that users are more likely to share news items. More shares imply more news items percolating through the social network and thus more flags. The bipolar model contains many elements besides the ρ and ϕparameters. For instance, it imposes that the social net- work has several communities and that social relationshipsare driven by homophily. These two elements are based on existing literature, yet we should test their impact on the model. First, keeping everything else constant, the no-homophily variant allows users to connect to friends ignoring their polarity value. In other words, polarity is randomly distribu- ted in the network. Second, keeping everything else constant,the no-community variant uses an Erdo ̋s–Rényi random graph as the social network instead of an LFR benchmark. The Erdo ̋s–Rényi graph extracts connections between nodes uniformly at random and thus it has, by definition, no com- munity structure. Figure 12 shows the impact on flag polarity dispersion (figure 12 a) and average truthfulness (figure 12 b). The no-homophily variant of the bipolar model has a significantly higher dispersion in the flag polarity distribution, and lowertruthfulness average, and the difference is stable (though stronger for values of ρabove 0.3). This means that polarity homophily is playing a key role in ensuring that flags are pre-dominantly assigned to neutral news items: if we remove it, the accuracy in spotting fake news increases. In contrast, removing the community structure from the net- work will result in a slightly smaller dispersion of flag ’s polarity and higher average flag truthfulness. The lack of communities might cause truthful items to spread more easily, and thus beflagged, increasing the average flag truthfulness. 4. Discussion In this paper, we show how the assumption of traditionalcrowdsourced content policing systems is unreasonable. Expecting users to flag content carries the problematic assump- tion that a user will genuinely attempt to estimate the veracityof a news item to the best of their capacity. Even if that was a reasonable expectation to have, a user ’s estimation of veracity will be made within their individual view of the world andvariable polarization. This will result in assessments that will give an easier pass to biased content if they share such bias. This hypothesis is supported by our bipolar agent-basedmodel. The model shows that even contexts that are extremely tolerant towards different opinions, represented by our flagg- ability parameter ϕ, would still mostly flag neutral content, and produce results that fit well with observed real-world data. Moreover, by testing the robustness of our model, we show how our results hold both for the amount of heterogen-eity of source activity and for individual differences in both tolerance and propagation attitudes. Removing polarization from the model, and thus testing what we defined as the monopolar model, attempts to repro- duce the assumptions that would make a classical content policy system work. The monopolar model, while seeminglybased on reasonable assumptions, is not largely supported by established literature in the area of online behaviour and social interaction, differently from the bipolar model.Moreover, it is not able to deliver on its promises in terms of ability to represent real-world data.Our paper has a number of weaknesses and possible future directions. First, our main results are based on a simu- lated agent-based model. The results hold as long as the assumptions and the dynamics of the models are an accurateapproximation of reality. We provided evidence to motivate the bipolar model ’s assumptions, but there could still be fac- tors unaccounted for, such as the role of originality [36] or ofspreaders ’effort [37] in making content go viral. Second, many aspects of the model were fixed and should be investi- gated. For instance, there is a strong polarity homophilybetween users and news sources, and in user –user connec- tions in the social network. We should investigate whether such strong homophily is really supported in real-world scen-arios. Third, the model has an essentially static structure. The users will never start/stop following news sources, nor befriend/unfriend fellow users. Such actions are commonin real-world social systems and should be taken into account. Fourth the model only assumes news stories worth interacting with. This is clearly different from the realitywhere, in a context of overabundant information, most stories are barely read and collect few reshares or flags. Including those news stories in the model could certainly affect theoverall visibility of other items. Finally, the model does not take into account reward and cost functions for both users and news sources. What are the repercussions for a newssource of having its content flagged? Should news sources attempt to become mainstream and gather following? Such reward/cost mechanisms are likely to greatly influence ouroutcomes. We plan to address the last two points in future expansions of our model. Ethics. No individual-level data have been accessed in the develop- ment of this paper. The paper ’s experiments rely on synthetic simulations. Motivating data provided by the Social Science Research Council fulfil the ethical criteria required by Social Science One. Data accessibility. The archive containing the data and code necessary for the replication of our results can be found at http://www.michelec-oscia.com/wp-content/uploads/2020/03/20200304_ffff.zip Authors ’contributions. L.R. collected the data. M.C. performed the exper- iments. M.C. and L.R. jointly designed the study, analysed the data, prepared the figures, and wrote and approved the manuscript. Competing interests. We declare we have no competing interest. Funding. No funding has been received for this article. Acknowledgements. This study was supported in part by a dataset from the Social Science Research Council within the Social Data Initiative. CrowdTangle data access has been provided by Facebook in collabor- ation with Social Science One. The authors also thank Fabio Gigliettoand the LaRiCA, University of Urbino Carlo Bo, for data access, and Clara Vandeweerdt for insightful comments. Endnotes 1https://www.facebook.com/facebookmedia/blog/working-to- stop-misinformation-and-false-news (April 2017, date of access 3 March 2020). 2https://socialscience.one/blog/unprecedented-facebook-urls-data- set-now-available-research-through-social-science-one (February 2020, date of access 3 March 2020). 3From a least-squares fit in a log-log space. Alternative hypotheses such as linear relationship or exponential relationship are discarded, with p-values approximately 0.98 and 0.34, respectively. 4https://www.crowdtangle.com/ 5https://electionstudies.org/resources/anes-guide/top-tables/?id= 29 (date of access 11 November 2019). 6https://sites.google.com/site/andrealancichinetti/files 7https://about.fb.com/news/2018/06/increasing-our-efforts-to- fight-false-news/ (date of access 7 January 2020).royalsocietypublishing.org/journal/rsif J. R. Soc. Interface 17: 2020002010  Downloaded from https://royalsocietypublishing.org/ on 27 November 2022  References 1. Newman N, Fletcher R, Kalogeropoulos A, Nielsen R. 2019 Reuters institute digital news report 2019 , vol. 2019. Oxford, UK: Reuters Institute for the Study of Journalism. 2. Allcott H, Gentzkow M. 2017 Social media and fake news in the 2016 election. J. Econ. Perspect. 31, 211 –36. (doi:10.1257/jep.31.2.211) 3. Lazer DMJ et al. 2018 The science of fake news. Science 359, 1094 –1096. (doi:10.1126/science. aao2998) 4. Vosoughi S, Roy D, Aral S. 2018 The spread of true and false news online. Science 359, 1146 –1151. (doi:10.1126/science.aap9559) 5. Adamic LA, Glance N. 2005 The political blogosphere and the 2004 US election: divided they blog. In Proc. of the 3rd Int. Workshop on Link Discovery, Chicago, IL, 21 –24 August 2005 , pp. 36 –43. New York, NY: ACM. 6. Garrett RK. 2009 Echo chambers online? Politically motivated selective exposure among internet news users. J. Comput.-Mediated Commun. 14, 265 –285. (doi:10.1111/j.1083-6101.2009.01440.x) 7. Nikolov D, Oliveira DFM, Flammini A, Menczer F. 2015 Measuring online social bubbles. PeerJ Comput. Sci. 1, e38. (doi:10.7717/peerj-cs.38) 8. Quattrociocchi W, Scala A, Sunstein CR. 2016 Echo chambers on Facebook. See https://papers.ssrn.com/ sol3/papers.cfm?abstract_id=2795110. 9. Flaxman S, Goel S, Rao JM. 2016 Filter bubbles, echo chambers, and online news consumption. Public Opin. Q. 80, 298 –320. (doi:10.1093/poq/ nfw006) 10. Dubois E, Blank G. 2018 The echo chamber is overstated: the moderating effect of politicalinterest and diverse media. Inf. Commun. Soc. 21, 729 –745. (doi:10.1080/1369118X.2018.1428656) 11. Del Vicario M, Vivaldo G, Bessi A, Zollo F, Scala A, Caldarelli G, Quattrociocchi W. 2016 Echo chambers: emotional contagion and group polarization on Facebook. Sci. Rep. 6, 37825. (doi:10.1038/ srep37825) 12. Garimella K, De Francisci Morales G, Gionis A, Mathioudakis M. 2018 Political discourse on socialmedia: echo chambers, gatekeepers, and the price of bipartisanship. In Proc. of the 2018 World Wide Web Conference, Lyon, France, 23 –27 April 2018 , pp. 913 –922. Geneva, Switzerland: International World Wide Web Conferences Steering Committee. 13. An J, Quercia D, Crowcroft J. 2013 Fragmented social media: a look into selective exposure to political news. In Proc. of the 22nd Int. Conf. on World Wide Web, Rio de Janeiro, Brazil, 13 –17 May 2013 , pp. 51 –52. New York, NY: ACM.14. Bakshy E, Messing S, Adamic LA. 2015 Exposure to ideologically diverse news and opinion on Facebook. Science 348, 1130 –1132. (doi:10.1126/science. aaa1160) 15. Conroy NJ, Rubin VL, Chen Y. 2015 Automatic deception detection: methods for finding fake news. Proc. Assoc. Inf. Sci. Technol. 52,1–4. (doi:10.1002/ pra2.2015.145052010082) 16. Shu K, Sliva A, Wang S, Tang J, Liu H. 2017 Fake news detection on social media: a data miningperspective. ACM SIGKDD Explor. Newsl. 19,2 2 –36. (doi:10.1145/3137597.3137600) 17. Wei W, Wan X. 2017 Learning to identify ambiguous and misleading news headlines. In Proc. of the 26th Int. Joint Conf. on Artificial Intelligence, Melbourne, Australia, 19 –25 August 2017 , pp. 4172 –4178. Palo Alto, CA: AAAI Press. 18. Li Y, Li Q, Gao J, Su L, Zhao B, Fan W, Han J. 2015 On the discovery of evolving truth. In Proc. of the 21th ACM SIGKDD Int. Conf. on Knowledge Discovery and Data Mining, Sydney, Australia, 10 –13 August 2015 , pp. 675 –684. New York, NY: ACM. 19. Wu L, Liu H. 2018 Tracing fake-news footprints: characterizing social media messages by how they propagate. In Proc. of the 11th ACM Int. Conf. on Web Search and Data Mining, Los Angeles, CA, 5 –9 February 2018 , pp. 637 –645. New York, NY: ACM. 20. Tschiatschek S, Singla A, Gomez Rodriguez M, Merchant A, Krause A. 2018 Fake news detection in social networks via crowd signals. In Companion Proc. of the Web Conf. 2018, Lyon, France, 23 –27 April 2018 , pp. 517 –524. Geneva, Switzerland: International World Wide Web Conferences Steering Committee. 21. Giglietto F, Iannelli L, Valeriani A, Rossi L. 2019 ‘Fake news ’is the invention of a liar: how false information circulates within the hybrid news system. Curr. Sociol. 67, 625 –642. 22. Myslinski LJ. 2013 Social media fact checking method and system, 4 June 2013. US Patent 8,458,046. 23. Kim J, Tabibian B, Oh A, Schölkopf B, Gomez- Rodriguez M. 2018 Leveraging the crowd to detect and reduce the spread of fake news and misinformation. In Proc. of the 11th ACM Int. Conf. on Web Search and Data Mining, Los Angeles, 5 –9 February 2018 , pp. 324 –332. New York, NY: ACM. 24. Crawford K, Gillespie T. 2016 What is a flag for? Social media reporting tools and the vocabulary of complaint. New Media Soc. 18, 410 –428. (doi:10. 1177/1461444814543163) 25. Gillespie T. 2018 Custodians of the Internet: platforms, content moderation, and the hidden decisions that shape social media . New Haven, CT: Yale University Press.26. Messing S, State B, Nayak C, King G, Persily N. 2018 Facebook URL Shares. See https://doi.org/10.7910/ DVN/EIAACS. 27. Mathias J-D, Huet S, Deffuant G. 2016 Bounded confidence model with fixed uncertainties and extremists: the opinions can keep fluctuating indefinitely. J. Artif. Soc. Soc. Simul. 19, 6. (doi:10. 18564/jasss.2967) 28. Giglietto F, Iannelli L, Rossi L, Valeriani A, Righetti N, Carabini F, Marino G, Usai S, Zurovac E. 2018Mapping italian news media political coverage in the lead-up to 2018 general election. See https:// papers.ssrn.com/sol3/papers.cfm?abstract_id=3179930. 29. American National Election Studies. 2008 The ANES guide to public opinion and electoral behavior. Seehttps://electionstudies.org/resources/anes-guide/ top-tables/?id=29. 30. Lewandowsky S, Ecker UKH, Cook J. 2017 Beyond misinformation: understanding and coping with the ‘post-truth ’era. J. Appl. Res. Memory Cogn. 6, 353 –369. (doi:10.1016/j.jarmac.2017.07.008) 31. Iyengar S, Hahn KS, Krosnick JA, Walker J. 2008 Selective exposure to campaign communication: the role of anticipated agreement and issue publicmembership. J. Politics 70, 186 –200. (doi:10.1017/ S0022381607080139) 32. Stroud NJ. 2008 Media use and political predispositions: revisiting the concept of selective exposure. Pol. Behav. 30, 341 –366. (doi:10.1007/ s11109-007-9050-9) 33. Lancichinetti A, Fortunato S, Radicchi F. 2008 Benchmark graphs for testing community detection algorithms. Phys. Rev. E 78, 046110. (doi:10.1103/ PhysRevE.78.046110) 34. Conover MD, Ratkiewicz J, Francisco M, Gonçalves B, Menczer F, Flammini A. 2011 Political polarizationon twitter. In Proc. 5th Int. AAAI Conf. on Weblogs and Social Media, Barcelona, Spain, 17 –21 July 2011. Palo Alto: AAAI Press. 35. Swire B, Berinsky AJ, Lewandowsky S, Ecker UKH. 2017 Processing political misinformation: comprehending the Trump phenomenon. R. Soc. open sci. 4, 160802. (doi:10.1098/rsos.160802) 36. Coscia M. 2017 Popularity spikes hurt future chances for viral propagation of protomemes. Commun. ACM 61,7 0 –77. (doi:10.1145/3158227) 37. Pennacchioli D, Rossetti G, Pappalardo L, Pedreschi D, Giannotti F, Coscia M. 2013 The threedimensions of social prominence. In Proc. Int. Conf. on Social Informatics, Kyoto, Japan, 25 –27 November 2013 , pp. 319 – 332. New York, NY: Springer.royalsocietypublishing.org/journal/rsif J. R. Soc. Interface 17: 2020002011  Downloaded from https://royalsocietypublishing.org/ on 27 November 2022</td>
    </tr>
    <tr>
      <th>9</th>
      <td>poli</td>
      <td>© 2006 Sigma Xi, The Scientific Research Society. Reproduction  with permission only. Contact perms@amsci.org.A reprint from American Scientist the magazine of Sigma Xi, The Scientific Research Society This reprint is provided for personal and noncommercial use. For any other use, please send a request to Permissions,  American Scientist, P .O. Box 13975, Research Triangle Park, NC, 27709, U.S.A., or by electronic mail to perms@amsci. org. ©Sigma Xi, The Scientific Research Society and other rightsholders 542     American Scientist, Volume 94© 2006 Sigma Xi, The Scientific Research Society. Reproduction  with permission only. Contact perms@amsci.org.The summer of 1994 was our first season to - gether in the Chernobyl Exclusion Zone, a  region within a 30-kilometer radius of the Cher - nobyl Nuclear Power Plant. We were there to  investigate the long-term biological effects of  ionizing radiation following the catastrophic ex - plosion and fire at reactor number four on April  26, 1986, which released plumes of radionu - clides that spread across Europe. We were only  2 kilometers from the defunct power plant, and  the area was still so radioactive that our Geiger  counters were perpetually abuzz. Although the  “Zone” was now nearly deserted—more than  135,000 people had been evacuated from the  region—we were amazed by the diversity of  mammals living in the shadow of the ruined  reactor only eight years after the meltdown. The  odd juxtaposition was eerily reminiscent of one  of the creepier Twilight Zone  episodes. We were in an area known as the “red for - est,” so named from the predominant hue of  the trees, which had been discolored in death  by the radiation. All the pine trees were dead;  only birches remained. During our excursion  through the woods, we trapped some of the local  mice for examination in a makeshift laboratory .  We were surprised to find that although each  mouse registered unprecedented levels of ra - diation in its bones and muscles, all the animals  seemed physically normal, and many of the fe - males were carrying normal-looking embryos.  This was true for pretty much every creature we  examined—highly radioactive, but physically  normal. It was the first of many revelations.  We have now spent 12 years trying to sort  out the effects of a radioactive environment on  the local wildlife. We have performed a variety  of experiments in the Zone. In one of our earli - est studies, we found that the resident mouse  population did not have any obvious chromo - somal damage. We wondered whether the ab - sence of injury could be explained by some sort  of adaptive change, perhaps a more efficient  DNA -repair mechanism, after many prior gen - erations of exposure to radiation. But when we  transplanted wild mice from uncontaminated regions into cages in the red forest and then  examined their chromosomes, they were like - wise unaffected by the radiation. In at least this  respect, the mice seemed to have a natural “im - munity” to harm from radiation. We repeated  the cage experiments with Big Blue transgenic  mice—which carry a gene that glows “blue” if it  undergoes a mutation—and radiosensitive mice  to look for evidence of chromosome breakage,  genetic aberrations and changes in gene expres - sion. The genetic impacts proved to be subtle  and not likely to threaten the rodent’s repro - ductive success or longevity . We also compared  the genetic variations of populations inside the  Zone with those from relatively uncontaminat - ed areas, and we found no evidence of increased  mutation rates from exposure to radioactivity . It turns out that the nascent field of radioecol - ogy is much more complicated than we had ex - pected. Radioactive fallout from the Chernobyl  accident was not deposited uniformly around  the reactor. Distinct “excursions,” known as the  northern and western traces, carried the ash  in plumes across the countryside and through  the city of Pripyat, a mere 3 kilometers from the  power plant. This produced a mosaic of radio - active habitats that are separated by relatively  unaffected areas. Such heterogeneity makes it  difficult to evaluate the effects on animal popu - lations because animals from “clean” habitats  might migrate into the contaminated areas. The  complexity of the habitats is exacerbated by  the presence of non-radioactive pollutants. Vin - cent Bahryaktor, vice president of the Ukrainian  Academy of Sciences, has said that “Northern  Ukraine is the cleanest part of our country; it has  only radiation.” Unfortunately , this isn’t quite  true. Decades of uncontrolled waste manage - ment have contaminated the region with heavy  metals, petrochemicals and pesticides. Radiation doses have declined precipitously  since the accident—less than 3 percent of the  initial radioactivity remains. Nevertheless, the  Chernobyl Exclusion Zone still offers a unique  outdoor laboratory to examine the fate and the  effects of a radioactive environment. The aban -Growing Up with Chernobyl  Working in a radioactive zone, two scientists learn tough lessons   about politics, bias and the challenges of doing good science Ronald K. Chesser and Robert J. Baker Ronald K. Chesser is a profes - sor of biological sciences and  director of the Center for Envi - ronmental Radiation Studies at  T exas T ech University. Much  of his current research is in  reverse-engineering radioac - tive releases from nuclear ac - cidents. He continues to work  at Chernobyl and is currently  examining radioactive con - tamination and human health  issues surrounding nuclear  facilities near Baghdad, Iraq.  Robert J. Baker is Horn Profes - sor of Biological Sciences and  director of the Natural Sciences  Research Laboratory at T exas  T ech. His research program  evalulates molecular variations  in organisms exposed to Cher - nobyl radiation. He is one of  the world’s leading authorities  on the genetic variation and  phylogenetics of bat species. Ad - dress: Department of Biological  Sciences, T exas T ech University,  Lubbock, T exas 79409–3131.  Internet: ron.chesser@ttu.edu,   robert.baker@ttu.edu 2006    November–December      543 www.americanscientist.org© 2006 Sigma Xi, The Scientific Research Society. Reproduction  with permission only. Contact perms@amsci.org.doned city of Pripyat is now largely a forest with  buildings poking above the treetops. After the  initial decline of the animal populations, which  were decimated by radioactive fallout, wildlife  is now thriving. The region has become a refuge  for released populations of Przewalski’s horse  and European bison. The population densities of  Russian wild boar are 10 to 15 times greater in the  Zone than in adjacent areas inhabited by people.  Endangered black storks and white-tailed eagles  are also more common in the Zone. The “Exclu - sion Zone” has effectively become a preserve. We were completely taken aback by what  we saw that first summer in Chernobyl, and  we continue to be challenged by what we en - counter in that strangely beautiful environ - ment. Our endeavors have led to some of the  happiest and bleakest moments in our profes - sional lives. We now recognize that we were  terribly naive about radioecology and the poli - tics of scientific research when we first started  this work. But we’ve gained some wisdom  along the way, and here we’d like to share  what we’ve learned from our experiences in  the form of brief lessons. Lesson 1:   Beautiful  theories  are often  destroyed  by ugly  facts. It may be a cliché, but it seems that nearly  everyone must learn this lesson at some point  in their scientific careers. In our case, the beau - tiful theory involved little rodents, voles of  the genus Microtus . We found a great deal of  genetic variation when we first examined the  voles within the Exclusion Zone, and because  the genetic differences were linked to different  sites within the Zone, we naturally assumed  that the variations were caused by diverse  exposures to radiation. To our chagrin, a chro - mosomal analysis revealed that we were ac - tually studying the natural variation of four  species of Microtus , not a single species, as we  had believed. It was evolutionary time, not  mutagenic radiation, that accounted for the  genetic differences we observed. What had promised to be a quick exposé of  the radiation effects from the Chernobyl fallout  proved to be a lesson in taxonomy . It also re - vealed a prejudice we had about the potential ef - fects of radiation. W e caught the error early in our  investigations, but we were still disappointed.  Figure 1. Destruction of the Chernobyl nuclear Power Plant by an explosion in reactor IV on April 26, 1986 released plumes of radioactive  particles across the countryside. The ionizing radiation resulted in a few dozen deaths within a few months, along with cancers implicated in  thousands of premature deaths. More than 135,000 people were evacuated from the zone around the plant, leaving behind the local wildlife.  The long-term effects of a radioactive environment on these inhabitants are still under investigation. The authors discuss some of the difficul - ties of assessing these effects after more than 12 years of research in the region surrounding the defunct nuclear complex.Igor Kostin/Sygma/Corbis 544     American Scientist, Volume 94© 2006 Sigma Xi, The Scientific Research Society. Reproduction  with permission only. Contact perms@amsci.org.W e hadn’t traveled halfway across the globe and  hiked through radiation-contaminated forests  to conduct studies in species classification, but  that was exactly what we had to do if we were to  move onto the meaningful phase of our work. Lesson 2:  Real  progress  often  requires  a  change  in direction. Before we worked at Chernobyl, our expertise  was limited to evolutionary genetics. Our un -derstanding of radiation dose rates, especially  in rodents, was almost zero. When we entered  Chernobyl, most animal studies assumed that  individuals from the same location would  have similar dose rates and that the rates were  proportional to the animal’s distance from the  source, in this case a damaged reactor. Neither  of these assumptions proved to be correct.  Mice living in the same habitat vary consider - ably in how much radiation they are exposed  to externally from the soils and the vegetation,  and internally from things they have ingested.  Our analyses showed that we must examine  the internal and external doses for each indi - vidual, rather than relying on population aver - ages or the animal’s proximity to the reactor. So we immersed ourselves in the study of  radioisotopes. The predominant radionuclides  remaining in Chernobyl are strontium-90  and cesium-137. These isotopes emit differ - ent amounts of energy, through particles and  photons, so the radiation an animal receives  depends on its relative exposure to these ra - dioisotopes. Also, it turns out that cesium ac - cumulates in muscle and other soft tissues,  whereas strontium is deposited in teeth and  bones. We studied the energies of the particles  and photons emitted by these radioisotopes,  and we learned how to estimate the probabili - ties that these energies will be absorbed by  air, soil and biological tissues. These are non - Figure 2. Two radioactive plumes—the northern and Western T races—skirted the edges of Pripyat, a city in northern Ukraine inhabited by 50,000  people at the time of the accident. It’s been estimated that 4,000 people died of cancer because of direct exposure to the radiation. The death toll  would have been much greater had the prevailing winds directed the plumes through the city. Distribution of the isotope cesium-137 serves as a  marker of radioactive contamination in the Chernobyl exclusion Zone  (map) . Most of the authors’ studies on the local wildlife took place in the  “Red Forest,” named after pine trees that were killed and discolored by the radiation. ( satellite image courtesy of Google/T erraMetrics.)10 kilometers BELARUSRed Forest Chernobyl nuclear power plant Pripyat City Ukraine/Belarus border Exclusion Zone boundaryremediated 1,000 500 200 100 50 20 10 5 1 &lt;1curies per square kilometer UKRAINE Chernobyl reactor IVChernobyl reactor IVPripyat, UkrainePripyat, Ukraine Western TraceNorthern Trace 2006    November–December      545 www.americanscientist.org© 2006 Sigma Xi, The Scientific Research Society. Reproduction  with permission only. Contact perms@amsci.org.trivial tasks, and even today we are still refin - ing some of the mathematical formulations to  improve our estimates. We failed to anticipate that such intricacies  would force us to become familiar with other  branches of science. Indeed, it’s been said that  science advances so quickly that its practitio - ners must run as hard as they can—like the Red  Queen in Lewis Carroll’s Through the Looking- Glass —just to stay in the same place. We learned  that sometimes you also have to change your  course just to stay on top of things. Lesson 3:   Don’t  forget  about  history. Animal populations have natural variations  in both gene frequency and physical traits as  a normal product of their evolutionary histo - ries. Populations experience growths, declines,  dispersals and local extinctions that may be  completely independent of the event that is  being investigated—here, an explosive release  of lethal radiation. The trick is to come up  with a way to distinguish the natural variation  from that caused by the agent of interest. This  is difficult because we don’t have samples of  populations in the contaminated zone before  the radiation was released.  One approach is to assume that nearby popu - lations of the same species that have not been  exposed to appreciable amounts of radioactivity  will display levels of variations that are similar to  those that would have been found in the original  population within the contaminated zone before  the accident. We use this approach, but not with - out recognizing its limitations. The “reference”  samples are merely pseudo-controls because we  cannot be sure that we have accounted for all the  factors that might have influenced genetic varia - tion. For example, geographical features—rivers,  forests and farmlands—that differ between the  regions may affect gene frequencies in unexpect - ed ways. Ecotoxicologists must consider these  historical influences if we are to identify the true  effects of a contaminating agent. Lesson 4:   It’s  always  wise  to maintain  some  humility. There’s a broad range of opinions on the biolog - ical consequences of being exposed to the con - taminated environment near Chernobyl. Vari - ous studies on wheat, mice, birds and humans  have concluded that mutation rates are greatly  elevated and that the evolutionary fitness of the  organisms is reduced. Other studies have failed  to find any increase in the rate of genetic muta - tions or any evidence that the survival of the  animals living near Chernobyl differs from that  of those living in clean environments.  We have spent a considerable amount of time  trying to understand how applications of the  scientific method can produce results that are  so contradictory . We could offer a few explana - tions, including the possibility that some studies simply lack the data to justify the authors’ con - clusions, but it might be best to offer an example  from our own experiences of tasting humble  pie. (We should add that human error may not  explain all the differences between the studies.) The April 25, 1996 issue of Nature  featured on  its cover our article concluding that voles living  in the Chernobyl environment had an elevated  rate of genetic mutation. Our experimental de - sign included double-blind analyses of DNA  sequences, the long strings of nucleotides that  make up the genetic code. We had determined  the genetic sequences manually , a process which  involves laborious alignments of genes and even  a few judgment calls. Nevertheless, we were  confident in our results.  Figure 3. Pripyat, now abandoned, lies a mere 3 kilometers from the Chernobyl nucle- ar Complex, which is visible on the horizon. Brenda Rodgers of T exas T ech University  and author Chesser measured the radiation levels of every building in Pripyat, and the  city is now the basis of a digital, three-dimensional reconstruction that models the flow  of radiation in urban environments. (Photograph courtesy of the authors.) Figure 4. The Red Forest was bathed in the ionizing radiation of the Western Trace,  which passed through the region in the wake of the accident at reactor IV . Pine trees  were killed by the plume, but birches survived. Wildlife now thrives in the area.  (Photograph courtesy of the authors.) 546     American Scientist, Volume 94© 2006 Sigma Xi, The Scientific Research Society. Reproduction  with permission only. Contact perms@amsci.org.Soon after the paper was published, we ac - quired an automated sequencer that was more  accurate than the manual methods used to se - quence DNA. We had archived the tissues from  all the animals used in our Nature  study , so we  decided to re-sequence the genes to compare  the methods. To our horror, the automated se - quencer failed to replicate the result we reported  in Nature . The more accurate method failed to  find an elevated mutation rate, even though we  repeated the sequencing several times. We agonized over our options for a few  weeks while we were on an expedition to an - other radioactive site in Chelyabinsk, Russia.  After we returned, we knew we had to retract  our report. Not all of our seven coauthors  agreed. Some thought that we should allow  the paper to fade from public scrutiny . Another  suggested that our future work would eluci - date the true mutation rate. However, Nature  is  a publication with a high profile, and our origi - nal report may well attract more attention than  potential reports in other publications. In the  end, we all agreed that we had an obligation to  the scientific community to come clean, and we  published a brief retraction in the November 6, 1997 issue of Nature . It was an important lesson  in admitting error and coming to terms with  our mistakes. Lesson 5:   Scientists  must  have  a single  agenda:  the truth. One day a graduate student brought her labo - ratory notebook to one of our group meetings.  She looked at us, almost in tears, and blurted,  “I couldn’t find any differences between the  experimentals and the controls. What did I do  wrong?” We were grateful for her honesty . Our  retraction of the Nature  paper had shown us  that problems in quality control can arise even  with the best intentions. Our scrutiny of the  published literature reveals that many scientists  are less than careful about such matters. Professors, graduate students and techni - cians all have preconceived ideas of where  their data may lead them. We recognized that  we weren’t immune to such prejudices either,  so we had to find a way to prevent our bi - ases from influencing our results. The best ap - proach is a blind analysis, with no knowledge  of whether the samples come from the experi - mental or the control groups. (We now have  inhalation of airborne particles ingestion of food and particles beta particles gamma raysairborne particles egestion and urinationsoil and vegetationexternal exposure internal exposure Figure 5. Mice brought into the Red Forest from uncontaminated regions serve  as experimental models to ascertain the effects of a radioactive environment. The  animals are exposed to radioactive particles, primarily cesium-137 and strontium- 90 (purple  and yellow  dots),  in the contaminated forest from internal and external  sources. some of the ionizing radiation (gamma rays and beta particles) is not  absorbed by the animal, and some of the particles are excreted. each mouse wears  a collar (photograph),  which contains dosimeters that absorb radiation at the same  rate as soft biological tissues. The collar provides an accurate measure of the radia - tion dose the animal receives from outside sources. such experiments reveal that  the mice appear unaffected by the residual radiation in the Chernobyl environ - ment. (Photograph courtesy of the authors.) 2006    November–December      547 www.americanscientist.org© 2006 Sigma Xi, The Scientific Research Society. Reproduction  with permission only. Contact perms@amsci.org.a secretary codify all the sample tubes.) After  we complete the analysis, we then decode and  classify our data.  Right or wrong, at least we know that the  results were not affected by our personal prej - udices. A scientist should always be ready to  change his mind. Once a scientist resists alterna - tive explanations, he is no longer a useful deci - sion maker. Blind studies are the antithesis of  blind ambition.  Lesson 6:   Incredible  results  require  incredible  evidence. Some reports on the biological impacts of the  fallout from Chernobyl seem to lie outside the  bounds of reasonable expectations. One study  reported that barn swallows, collected from  northern Ukraine, were experiencing partial al - binism and increased germline mutation rates,  with a concomitant loss of evolutionary fitness.  Unfortunately , the authors did not identify ex - actly where the birds were collected, they did  not evaluate the radiation doses to the birds and  they provided little information on the level of  soil contamination. In contrast, our own inves - tigations of swallows collected in a 10-kilometer  zone surrounding the reactor reveals that the  internal radiation doses are negligible—less  than 10 microsieverts per day . That dose rate  is less than one-tenth of a typical chest x ray ,  or about the same for three hours of flight at  35,000 feet. It would be astonishing if that dose were responsible for the elevated mutation rates  reported by the authors.  It’s also the case that local genetic variations  are common in natural animal populations. It  would be surprising to find populations with - out geographic variations. The authors of the  barn-swallow article inferred that the variation  among the birds was the result of radiation-in - duced mutations, but they did not provide any  evidence for their assertion. They did not elimi - nate the possibility that the geographic varia - tions were natural, or that the mutations might  have been caused by something else. Moreover,  some of the same barn-swallow variants have  been found in regions not affected by Chernob - yl, suggesting that they may not be uncommon.  In our opinion, their incredible conclusions were  supported only by circumstantial evidence. Lesson 7:   A good  idea doesn’t  always  attract  funding. Chernobyl is known the world over as the  worst nuclear power plant disaster in history .  We naively assumed that this name recognition,  and a good scientific plan, would quickly yield  financial support for our research. On one of  our funding forays to Washington we present - ed what we thought was a well-designed plan  on the scientific merits of our proposed research  at Chernobyl. Afterward, one of the politicians  said, “Okay , that was fine ... now, how do I sell  this to my fellow congressmen?” We exchanged  Figure 6. Wildlife flourishes in the Chernobyl exclusion Zone. Many species are more plentiful in the exclusion Zone than they are in neigh - boring habitats (clockwise  from  upper  left: red fox, nesting  Northern  shrikes,  elk and Russian  wild  boar).  (Photographs courtesy of the authors.) 548     American Scientist, Volume 94© 2006 Sigma Xi, The Scientific Research Society. Reproduction  with permission only. Contact perms@amsci.org.glances with each other that said, “didn’t we  just answer that question?” Later we realized that we hadn’t provided  him with anything useful for his agenda. Good  science is a beautiful thing, but it must fit within  an existing framework of policy and strategic  plans or it will be very difficult to finance. Our  great scientific proposal floated away like a he - lium-filled balloon with no string tying it down  to political reality .  Policy makers want concrete conclusions and  results, not probability estimates on the relative  dangers of radiation exposure, which turns out  to be less hazardous than generally believed. Many public servants do not share the scientist’s  enthusiasm for the scientific process. They make  laws that distinguish right from wrong, while we  spend 95 percent of our time trying to explain 5  percent of the variation of a phenomenon. Ultimately, we were successful in securing  funds for several years of research, and we have  since built the International Radioecology Labo - ratory in Slavutych, Ukraine. But it hasn’t been  easy, and most prospects for future funding  look bleak. Lesson 8:   Be prepared  to be unpopular  and uncomfortable. In this year’s 20th-anniversary reports on the  Chernobyl accident, news outlets various - ly claimed that the disaster had resulted in  93,500, 70,000, 4,000, hundreds or just 31 hu - man deaths. We couldn’t find a single story  that tried to explain the enormous difficulties  of determining an accurate number for the  excess cancer deaths caused by the radiation  fallout from Chernobyl. The press did not at - tempt to explain the differences in opinions  between scientists or the contradictory results  of research on animals exposed to radiation.  Instead, advocacy groups pointed their fingers  at scientists and asked why they were trying  to cover up the “real” impact of radiation on  people and the environment.  Scientists often find themselves in unpopular  and uncomfortable positions. That’s just part  of the job when you have to report the truth.  A scientist’s conclusions help to guide public  policy , write regulations and develop new tech - nologies. The results of good science are simply  Figure 7. some people have returned to the Chernobyl exclusion Zone, hoping to  reclaim the land and livelihoods they lost to the accident in 1986. scientists continue  to monitor the environment. Viktor Drachev/AFP/Getty Images 548     American Scientist, Volume 94Research on the effects of radiation should be held to  the highest scientific standards. In our opinion, the  standards have been lax. Many of the studies showing  an elevated rate of mutations among the animals in the  Chernobyl environment need to be replicated, often with  improved experimental designs. Funding agencies should  encourage independent laboratories to undertake simulta - neous studies of the same samples.  We would be negligent in our mission to improve the re - search at Chernobyl if we did not also provide some positive  recommendations for future investigations. Here we offer a  short list of the minimum requirements necessary for a valid  study of the consequences of environmental radioactivity . 1. Investigators should archive tissue, DNA and any other  material that would permit other scientists to replicate their  results. Archiving would also allow scientists to employ other  experimental designs to corroborate, refute or enhance the  results. To that end, we have archived over 3,000 specimens  from mammals native to Chernobyl and the surrounding  areas at the Museum of Texas Tech University . We have also  archived tissue from laboratory mice with recombinant DNA  (Big Blue), and mice exposed to low-dose radiation. These  samples allow scientists to confirm our results, and the tissue can be used to identify biomarkers that can resolve the effects  of radiation. Scientists interested in obtaining tissue or bor - rowing samples from the Chernobyl region may contact us by  e-mail: robert.baker@ttu.edu 2. Samples should be analyzed using double-blind ex - perimental methods. This removes any suspicion that an  investigator’s bias may be affecting the results. 3. Radiation levels and expected dose rates from external  sources in an animal’s normal activity area should be identi - fied for all known radionuclides. There is too much variation  in radiation levels to assign dose rates for an animal based on  a few measures from soil samples.  4. Investigators need to acquire accurate measures of an  animal’s dose rate from inhaled or ingested radionuclides.  Dose rates from internal sources are often higher than those  from the outside. If dose rates are unknown, then measures of  radioactivity in tissues will suffice. 5. The provenance of a sample must be provided. Other  scientists should have the opportunity to collect samples from  the same location. 6. Scientists should report positive and negative results.  The omission of negative results gives the impression that the  effects of radiation are ubiquitous. Setting Standards for Radioecology 2006    November–December      549 www.americanscientist.org© 2006 Sigma Xi, The Scientific Research Society. Reproduction  with permission only. Contact perms@amsci.org.too important to be swayed by emotional ap - peals. Unfortunately, poor science often gets  great publicity , especially if it stirs controversy  or implies that governments are recklessly en - dangering the lives of their citizens. In the long  run, poor science will beget poor policy . Lessons for science Twenty years have passed since the accident at  Chernobyl, yet the extent to which people, ani - mals and the environment have been harmed  is hotly debated. The conflicting reports on the  genetic and biological impacts of environmental  radiation make it difficult for even the most  seasoned scientist to make sense of all the data.  Remarkably , there is still no accurate account  of the number of deaths or birth defects caused  by the disaster. Investigations into the effects of  ionizing radiation on living organisms should  be based on sound scientific principles for the  simple reason that risk assessments, regulatory  statutes and the effectiveness of remediation  measures are often based on these reports. The  public and scientific communities need to recog - nize that ecological studies on Chernobyl that do  not include accurate information on animal ex - posure do not qualify as radiological research. The proper null hypothesis should be that  the effects of the Chernobyl environment on  an organism do not differ from effects outside  the environment. Falsification of the null hy - pothesis has profound implications for society .  If there is an elevated mutation rate and loss of  health, then appropriate measures should be  taken to protect ourselves. No one would argue  with that. But we must be mindful that the costs  of over-regulation can be extreme. Zbigniew  Jaworowski, former chairman of the United  Nations Scientific Committee on the Effects of  Atomic Radiation, has estimated that enforcing  the radiation-safety regulations in the U.S. costs  about three billion dollars for each life saved  from accidental exposure. By comparison, the  measles vaccine costs $99 per life saved. We believe the problem requires a coordinat - ed effort that enforces standards of data gather - ing and assessment. This effort would establish  protocols for collegial verification of results and  preserve samples for future studies. Without  such coordination, we will fritter away meager  resources on disconnected and unrepeatable  studies. We should endeavor now, while the sig - natures of released radiation remain on the land - scape and in the affected organisms, to solve the  issues of environmental health and safety . Bibliography Baker, R. J., R. A. Van Den Bussche, A. J. Wright, L. E.  Wiggins, M. J. Hamilton, E. P . Reat, M. H. Smith, M. D.  Lomakin and R. Chesser. 1996. High levels of genetic  change in rodents of Chernobyl. Nature  380:707–708. Baker, R. J., R. A. Van Den Bussche, A. J. Wright, L. E.  Wiggins, M. J. Hamilton, E. P. Reat, M. H. Smith,  M. D. Lomakin and R. Chesser. 1997. Retraction. High levels of genetic change in rodents of Cher - nobyl. Nature  390:100. Baker, R. J., and R. K. Chesser. 2000. The Chornobyl  nuclear disaster and subsequent creation of a wild - life preserve. Environmental Toxicology and Chemistry  19:1231–1232 . Baker, R. J., A. M. Bickham, M. Bondarkov, S. P . Gas - chak, C. W. Matson, B. E. Rodgers, J. K. Wickliffe and  R. K. Chesser. 2001. Consequences of polluted en - vironments on population structure: The bank vole  (Clethrionomys glareolus ) at Chornobyl. Ecotoxicology   10:211–216. Chesser, R. K., and R. J. Baker. 1996 . Life continues at  Chernobyl. La Recherche  286:30–31. Chesser, R. K., D. W. Sugg, A. J. DeWoody, C. H. Jagoe,  M. H. Smith, M. D. Lomakin, R. A. Van Den Bussche,  C. E. Dallas, K. Holloman, F. W. Whicker, S. P . Gas - chak, I. V . Chizhevsky, V . V . Lyabik, E. G. Buntova  and R. J. Baker. 2000. Concentrations and dose rate  estimates of 134,  137cesium and 90strontium in small  mammals at Chornobyl, Ukraine. Environmental Toxi - cology and Chemistry 19:305–312. Chesser, R. K., M. Bondarkov , R. J. Baker, J. K. Wickliffe  and B. E. Rodgers. 2004. Reconstruction of radioactive  plume characteristics along Chernobyl’s western trace.  Journal of Environmental Radioactivity  71:147–157. Chesser, R. K., B. E. Rodgers, J. K. Wickliffe, S. Gaschak,  I. Chizhevsky, C. J. Phillips and R. J. Baker. 2001. Ac - cumulation of 137cesium and 90strontium from abiotic  and biotic sources in rodents at Chornobyl, Ukraine.  Environmental Toxicology and Chemistry  20:1927–1935. Dubrova, Y. E., V . N. Nesterov, N. G. Krouchinsky, V . A.  Ostapenko, R. Neumann, D. L. Nell and A. J. Jeffreys.  1996. Human minisatellite mutation rate after the  Chernobyl accident. Nature  380:683–686. Ellegren, H., G. Lindgren, C. R. Primmer and A.P.  Møller. 1997. Fitness loss and germline mutations  in barn swallows breeding in Chernobyl. Nature   389:593–596. Goncharova, R. I., and N. I. Ryabokon. 1995. Dynamics  of cytogenetic injuries in natural populations of bank  vole in the republic of Belarus. Radiation Protection  Dosimetry  62:37–40. Jacob, P .,  et al . 1998. Thyroid cancer risk to children cal - culated. Nature  392:31–32.  Jaworowski, Z. 1995. Beneficial radiation. Nukleonika   40:3–12. Kovalchuk, O., Y. E. Dubrova, A. Arkhipov, B. Hohn  and I. Kovalchuk. 2000. Wheat mutation rate after  Chernobyl. Nature  407:583. Medvedev, Z. A. 1994. Chernobyl: Eight years after.  Trends in Ecology &amp; Evolution  9:369–371. Meeks, H. N., J. K. Wickliffe, S. R. Hoofer, R. K. Chesser,  B. E. Rodgers and R. J. Baker. Accepted. Evidence  that variation in the mitochondrial control region  is not related to radiation exposure. Environmental  Toxicology and Chemistry. Rodgers, B. E., R. K. Chesser, J. K. Wickliffe, C. J. Phillips  and R. J. Baker. 2001. Sub-chronic exposure of BALB  and C57BL strains of Mus musculus to the radioactive  environment of the Chornobyl exclusion zone. Envi - ronmental Toxicology and Chemistry  20:2830–2835. Rodgers, B. E., J. K. Wickliffe, C. J. Phillips, R. K. Chesser  and R. J. Baker. 2001. Experimental exposure of naïve  bank voles, Clethrionomys glareolus , to the Chornobyl,  Ukraine, environment: A test of radioresistance. Envi - ronmental Toxicology and Chemistry  20:1936–1941. Wickliffe, J. K., B. E. Rodgers, R. K. Chesser, C. J. Phil - lips, S. P . Gaschak and R. J. Baker. In press. Mito - chondrial DNA heteroplasmy in laboratory mice ex - perimentally enclosed in the radioactive Chornobyl  environment. Radiation Research .For relevant Web links,  consult this issue of  American Scientist  Online : http://www.american   scientist.org/   IssueTOC/issue/902</td>
    </tr>
    <tr>
      <th>4</th>
      <td>poli</td>
      <td>A Question of Balance — 1Running head: A QUESTION OF BALANCE A Question of Balance:Are Google News search results politically biased?By Eric UlkenUSC Annenberg School for CommunicationMay 5, 2005 A Question of Balance — 2 AbstractThis study examines search results from the popular online news portal Google News inan effort to determine whether they are politically biased.  By analyzing the content ofthird-party articles returned in a search on a political candidate (“George W. Bush,” forexample), it is possible to assess the level of bias in the search results.  Articles returnedin searches on the two leading presidential candidates in the weeks before the 2004election were collected, and a random sample of the highest-ranking results was analyzedfor favorability to each candidate.  Results from the same searches on Yahoo News wereused as a benchmark for comparison.  The data show that articles returned in GoogleNews searches are significantly more likely to have a political bias than those returned insearches on Yahoo News, but there is no evidence of an overall conservative bias insearch results on Google News, as has been suggested. A Question of Balance — 3 A Question of Balance: Are Google News search results politically biased?As online news has grown in popularity, a number of sites have sprung up tocatalog the wealth of news content available on the Internet. These so-called news portalsinclude Google News, Yahoo News, Topix.net and MSNBC’s Newsbot.  Through anautomated process known as “spidering” or “crawling,” these aggregators index thecontent of selected news sources and allow users to browse and search recent newsstories, usually linking to the source of the article for the full text.Google News, launched in 2001 but still in the “beta-testing” phase, has becomeone of the Internet’s most popular news portals, drawing about 5.9 million visitors amonth (Gaither, 2005). It indexes the top stories on some 4,500 English-language newssites, updating its index roughly every 15 minutes (Google, 2004).  Google’s innovativemethod of identifying top stories based on how frequently they appear on sites in itsindex – and doing so entirely without human intervention – has made the site a target ofcriticism since its inception.  The efficiency with which Google News is able toautomatically determine relative importance of stories and present a “front page” with topstories in different subject areas has been seen by some as the first ominous sign thatcomputers will eventually make human editors obsolete. At the same time, users haveridiculed flaws in Google News’s algorithms that cause it to occasionally attach a photo A Question of Balance — 4to an unrelated article or elevate a relatively minor story to a prominent spot on its frontpage.Google News front page Yahoo News front page Google does not share the list of sources it crawls, but searches often revealresults from relatively obscure online-only news sites – including some that are bestdescribed as weblogs – leading to questions about Google News’s criteria for inclusionand the notion that there might be some political imbalance in the sites it crawls.  Googlehas taken a lot of criticism recently for the quality of news content in its index.  Earlierthis year, it removed from its index several sites, including the white supremacist journalNational Vanguard, after users complained that hate speech was turning up in searches.Practices at Google News have come under additional scrutiny since March, whenAgence France-Presse filed a lawsuit alleging that Google infringed its copyright bydisplaying AFP material on Google News pages.  The Associated Press has alsoexpressed “concern” about Google’s use of its material without payment (Gaither, 2005).Janice Castro, director of graduate programs at Northwestern University’s MedillSchool of Journalism and one of the founders of the Online News Association, toldCNET News.com the problem with Google News is that it gives users no way of A Question of Balance — 5evaluating the quality of news sources (Olsen, 2005).  “The best is mixed up with thingsthat are far from the best,” Castro said.Web journalism pioneer J.D. Lasica was among the first to suggest a conservativebias in Google News (2004).  “What’s going on?” he wrote in an article for OnlineJournalism Review as the 2004 presidential race was heating up.  “Have Google’s searchresults been hijacked by Fox News?”  Lasica cited several stories from “second-tier”online-only news and commentary sites in a search on the words “John Kerry.”  Theheadlines returned included the following: “John Kerry Said ‘Bring It On,’ Now Whines To Bush To Stop The Ads” “The Imploding John Kerry” “Swift Boat Veterans for Truth Expose John Kerry’s Lies” “John Kerry is Definitely ‘Unfit for Command’”Political bias in news coverage has been the topic of numerous academic studies,most of which have themselves been subjected to charges of bias.  Bias is practicallyimpossible to quantify absolutely, but it can be measured in relative terms.  One of themost widely cited and controversial recent studies attempts to do just that.  Grosecloseand Milyo (2003) assess bias among major news outlets – including The New York Times,USA Today and Fox News’ “Special Report” – by looking at how often they cite certainpolitically active “think tanks” and comparing this with how frequently members ofCongress cite the same sources in floor speeches.  The authors’ assumption is thatconservatively biased news organizations, say, will cite a certain think tank with the samefrequency as a conservative member of Congress.  The study’s conclusion is that themainstream media have an overwhelming liberal bias and the most unbiased news source A Question of Balance — 6is Fox News’ “Special Report.”  This finding was met with as much outrage as praisewhen the study first made the rounds of politically oriented weblogs (Dallas, 2004;Tabarrok, 2004) – proof that bias is always relative to the observer.This study attempts to scientifically test Lasica’s casual observation of bias inarticles linked from Google News.  Unlike the Groseclose/Milyo study, it does not havethe benefit of an independent benchmark for comparison.  Instead, it compares GoogleNews with a more established competitor, Yahoo News, in an attempt to determinerelative bias.  The study looks at balance within stories as an indicator of bias.  Abalanced article will presumably have roughly as many favorable references to the searchterm (i.e., the candidate) as it has unfavorable ones.  Given what Lasica and others havereported about Google News search results, the goal of this study is to prove or disprovequantitatively the assertion that Google News displays a conservative bias.The research question and hypothesis are as follows:RQ1: Are Google News search results politically biased?H1: Results of Google News searches on the two major-party presidentialcandidates will reveal a conservative bias.The research hypothesis is tested by means of a quantitative text analysis ofarticles returned in Google News and Yahoo News search results.MethodThis study analyzes articles returned in searches on the full names of the twomajor-party presidential candidates (“George W. Bush” and “John Kerry”) in the weeks A Question of Balance — 7leading up to the 2004 election in order to determine a bias score for each article and,ultimately, to quantify the overall bias of the search results.Data acquisitionSince the news search engines update their indexes frequently over the course of aday, the results for a particular search term can change from one minute to the next.  Adata acquisition scheme was devised that respects the dynamic nature of the searchresults.  A computer program was written to retrieve the first 10 articles returned byGoogle News and Yahoo News for each search term (“George W. Bush” and “JohnKerry”) at four-hour intervals and save them.Google News search results Yahoo News search results The program was run for the period of Oct. 17-30, 2004, the two weeks precedingthe Nov. 2 presidential election, resulting in a total of 80 “snapshots.”  Each snapshotcontained four sets of search results: “George W. Bush” on Google News, “George W.Bush” on Yahoo News, “John Kerry” on Google News and “John Kerry” on YahooNews.  The program also downloaded the full text of the first 10 articles returned in eachresult list. A Question of Balance — 8Sampling schemeTaking the top 10 articles in each list would yield 3,200 texts.  A moremanageable sample of 100 was selected for analysis.  In order to generate arepresentative sample, a two-stage sampling process was devised that divided the datacollection period into five sequential periods of equal length and then randomly selectedone snapshot from each period.  The stratified selection provided for a sample that wasspread fairly evenly over the two weeks, so that a single news event would be unlikely todominate the sample.  The random selection stage ensured that the final five snapshotsrepresented a variety of dayparts and days of the week.The following snapshots were selected: Monday, Oct. 18, 2004, midnight Thursday, Oct. 21, 2004, 8 a.m. Monday, Oct. 25, 2004, midnight Thursday, Oct. 28, 2004, 4 p.m. Saturday, Oct. 30, 2004, 8 a.m.For each snapshot, the first five articles from each of the four result lists wereselected for analysis, ensuring an equal number of Bush and Kerry results and an equalnumber of Google News and Yahoo News results.  In a couple of cases, the complete textof an article was behind a paid subscription wall.  Where possible, a shortened freeversion was used; otherwise the article was skipped and the next highest-ranked articlewas used instead. A Question of Balance — 9Units of observationThe articles were subdivided by sentence – with a sentence representing a singleunit of observation.  The decision was made to use sentences, rather than propositions, asthe units of observation for two reasons: The texts can be parsed into sentences with minimal work, an importantconsideration given the volume of the data. Propositions – that is, groups of words expressing a distinct idea, whether as aphrase, a sentence or multiple sentences – must be manually identified andparsed by coders.  This introduces the problem of unitizing reliability.Because a number of the texts were extremely long, only the first 25 sentences ofeach article were coded.  It was assumed that an article’s overall bias would be apparentwithin the first 25 sentences.  The results section offers data that support this assumption.Overall, 1,587 sentences were coded.Coding schemeEach sentence could be coded in one of five ways:1. Unfavorable to Kerry2. Favorable to Kerry3. Neutral4. Unfavorable to Bush5. Favorable to BushA coding manual was created to guide coders in the process of evaluating units.(See Appendix A for the full coding manual.)  Because the coding scheme involves A Question of Balance — 10assessing latent meanings of sentences – and the terms “favorable” and “unfavorable” areimprecise – the most important purpose of the coding manual is to define these terms andspecify how to apply them.  The basic coding rules used are as follows: A unit (sentence) can only be coded as favorable or unfavorable if it containsan unambiguous message that, taken independently of other units, is favorableto one candidate or the other. Otherwise it must be coded neutral. If a unit contains both favorable and unfavorable references to the samecandidate, or it contains only favorable or unfavorable references to bothcandidates, it should be coded neutral. If a unit contains one or more favorable references to one candidate and one ormore unfavorable references to the other candidate, only the first reference inthe sentence should be considered in coding. If there is uncertainty about how different people might interpret a unit, itshould be coded neutral.For purposes of assessing favorability, direct quotations and other attributedstatements are treated no differently from statements made by the article’s author, sincethe choice of one particular quote over another can also represent bias.  For example, thesentence, “Democrats accused Bush of misleading the nation about the justification forwar in Iraq,” would be coded as unfavorable to the president.The coding scheme attempts to make the rating process as objective as possible.However, coders’ personal biases could affect how they evaluate elements of the texts.While it is true that such coder subjectivity may skew balance scores of individual A Question of Balance — 11articles in one direction or another, it is still possible to compare average scores forGoogle News and Yahoo News and thus assess bias of one site relative to the other.Coding procedureTwo coders were used in this study.  A primary coder – the author of the study –analyzed all 100 texts.  A second coder – also a graduate student in communication withexperience in content analysis – analyzed 28 of the texts, selected at random, for thepurpose of assessing the validity of the coding scheme and the reliability of the primarycoder.  Articles were assigned to coders in random order using a computer-based codingsystem.  The coders had no knowledge of whether a particular article came via GoogleNews or Yahoo News (or which search term returned it).  Coders were given only theheadline and the name of the source organization for each article.  The coders weretrained and the coding scheme was initially tested using sample texts from articles notincluded in the actual sample.In addition to coding each sentence, coders were asked to assess the overallfavorability of the article in the form of two variables, each with five possible values: Overall favorability to Bush: highly favorable, favorable, neutral, unfavorable,highly unfavorable Overall favorability to Kerry: highly favorable, favorable, neutral,unfavorable, highly unfavorableThese overall favorability scores were intended only for the purpose of validatingthe unit-by-unit coding scheme and ultimately were discarded. A Question of Balance — 12Intercoder reliabilityIntercoder reliability is assessed on two levels: by sentence (the unit ofobservation) and by article (the unit of analysis).At the sentence level, Cohen’s kappa, a measure of agreement between coders onnominative variables, is computed as 0.72 – just above the 70% threshold considered anacceptable level of agreement.  When intercoder reliability is tested at the article level,the agreement between the two coders is closer.  Because the favorability scorescomputed for each article are ratio measurements, the two coders’ scores are fit againsteach other, with the r-square statistic used to express intercoder reliability.  Values of r-square for the Kerry favorability and Bush favorability variables are 0.94 and 0.90,respectively – indicating a relatively high level of agreement between the coders.When computing reliability using such a small number of measurements (only 28articles were coded by both coders), a single outlying value can greatly affect theoutcome. It was necessary to omit one such outlier from consideration in computingagreement on the Kerry favorability score. The offending score was for an article thatcontained only one sentence, which one coder deemed favorable to Kerry and the othercoder recorded as neutral. As a result, the article took a Kerry favorability score of 1 fromone coder and 0 from the other. This greatly affected the reliability calculation, causing r-square to plummet to 0.57.  The article was excluded from the data analysis, which iswhy the data show one article fewer for Yahoo News than for Google News.ResultsUsing the values for each sentence, two scores are calculated for each article, A Question of Balance — 13measuring the degree of the article’s overall favorability to each candidate. Bush andKerry favorability scores for each article are computed using the following formula:favorability score = (sum of favorable units) – (sum of unfavorable units)(total units coded)Favorability scores can thus take values of –1 (completely unfavorable) to 1(completely favorable), with 0 being neutral. For instance, a Kerry favorability score of–0.3 for an article would indicate that, on balance, 30% the content of an article isunfavorable to John Kerry (the actual proportion of unfavorable units might be 35% butoffset by 5% of units coded as favorable to Kerry). Because even the most biased articlescontain a lot of neutral (or irrelevant) content, the scores tend be closer to 0 than to eitherextreme.Two scatterplots – one for Google News and the other for Yahoo News – providea basic summary of the data. They show the two candidates’ favorability scores for eacharticle, plotted against each other. This facilitates comparison of the overall favorabilityof the two portals’ search results.Favorability plots by search engine -1-0.75-0.5-0.2500.250.50.751Kerry favorability-1-0.75-0.5-0.250.25.5.751Bush favorabilityGoogle News-1-0.75-0.5-0.2500.250.50.751Kerry favorability-1-0.75-0.5-0.250.25.5.751Bush favorabilityYahoo News A Question of Balance — 14Each data point represents an article, and its placement on the chart represents itsfavorability to the two candidates: Upper left quadrant: Article is favorable to Kerry and unfavorable to Bush Upper right quadrant: Article is favorable to both Lower right quadrant: Article is favorable to Bush and unfavorable to Kerry Lower left quadrant: Article is unfavorable to bothIn other words, articles in the upper right and lower left are more balanced thanthose in the upper left and lower right. Articles closer to the center are more neutral. Thecircular boundary is a density ellipse drawn around 90% of the data points, which makesit easier to see patterns in the data.  One fact that is not apparent in the scatterplots is thata large number of data points are at the coordinates (0, 0).  This is because many of thearticles – 22% for Google News and 45% for Yahoo News – exhibited no bias at all,either because they discussed both candidates with complete neutrality or because theywere not relevant to either candidate.  A glance at the two plots reveals what can be seenempirically in the search results from the two sites:  Articles returned in the searchesusing Google News are more likely to be biased in favor of one candidate and against theother, while those that turn up in the Yahoo News searches are generally more balanced.In order to illustrate article bias in one dimension, a measurement that takes intoaccount favorability ratings for both candidates is needed.  Two related scores are devisedfor this purpose.  The first, the article balance score, shows the degree to which articlesfavor one candidate over the other.  It is computed using a simple formula:balance score = kerry favorability – bush favorability A Question of Balance — 15An article’s balance score takes a value between –1 and 1, with positive numbersindicating greater favorability to Kerry and negative numbers indicating greaterfavorability to Bush.  Articles with balance scores of 0 are equally favorable (orunfavorable) to both candidates.Article balance scores by search engineBalance score-1-0.500.51 GoogleYahooSearch engineBy taking the average balance scores for articles returned by Google News andYahoo News, each search engine’s overall bias can be determined.  A balance score thatfavors Bush is presumed to show a conservative bias, while one that favors Kerry wouldindicate a liberal bias.  The average balance scores for both Google News and YahooNews are not significantly different from 0, indicating an absence of overall bias in thesearch results for both sites.  Thus, while the data do show bias in many of the articlesreturned by Google News, there is no evidence of an overall conservative (or liberal)slant to the site’s search results, as has been alleged.The second measurement, the article bias score, is simply the absolute value ofthe balance score.  It takes a value between 0 and 1 and represents the proportion of anarticle that is biased, regardless of the direction of bias.  For example, an article in which A Question of Balance — 16half the sentences are coded as favorable to Kerry and the other half as unfavorable toBush would have a bias score of 1, meaning 100% of the article is biased.  In fact, biasscores tend to be closer to 0, though one article returned by Google News had a bias scoreof 0.92.  The mean article bias score for each search engine describes the degree to whichthe average article returned by that search engine is likely to be biased.Article bias scores by search engineBias score00.20.40.60.81 GoogleYahooSearch engineAs seen in the plot above, the articles returned by Google News have a highermean bias score than those returned by Yahoo News (0.23 compared with 0.13, astatistically significant difference).  This means that a search on Google News is likely toturn up articles that are more biased than those returned by its competitor.Besides being coded for favorability, articles were also classified by whether theycame from an independent, online-only source (such as Salon.com) or a website affiliatedwith a traditional news source.  A traditional news source is defined as a wire service,newspaper, magazine, TV station, radio station, broadcast network or cable network.(Content from one of these sources that is syndicated on a news aggregator such asYahoo News is also classified as traditional.)  Of the articles returned by Google News,40% were from non-traditional news sources, while only 24% of the Yahoo News results A Question of Balance — 17came from non-traditional sources.  (See Appendix B for a list of the sources of all thearticles coded.)  Notably, almost all of the bias in Google News’s search results can beattributed to its use of non-traditional sources.  In other words, when articles from non-traditional sources are left out of the calculation, the average bias scores for Google Newsand Yahoo News are virtually identical.Finally, articles that exceeded the 25-sentence length limit for coding tended to bescored as slightly more biased than shorter articles, on average.  A possible explanationfor this is that most of the articles from traditional sources – those less likely to exhibitbias – were shorter than 25 sentences.  If this arbitrary limit were hindering the codingscheme’s ability to ascertain bias in longer texts, one would expect to find a loweraverage bias score for longer articles.DiscussionThe data show that articles returned in Google News searches are more likely tohave a bias toward a particular candidate than those returned in searches on Yahoo News,but there is no evidence of an overall conservative bias in search results on Google News,as has been suggested.  Both Google News and Yahoo News searches returned articlesthat were, on the whole, equally favorable to both George W. Bush and John Kerry.  Thisis what one would expect to see of balanced search results at a time when public opinionis evenly divided between the two candidates.Accordingly, the research hypothesis, H1, is rejected.For both candidates, a slight tendency toward negativism (that is, moreunfavorable content than favorable) can be seen in articles returned by the two news A Question of Balance — 18portals. This can be explained in two ways: By the time the data were collected – in the two weeks before the 2004general election – the race for the presidency had turned increasingly negative. The news media by their nature generally place greater emphasis on negativestories than positive ones.With the abundance of well-respected, credible sources on the Internet, why doesGoogle News return so many articles from biased sources?  An explanation offered byNathan Stoll, Google’s associate product manager for Google News, has to do with thesearch terms themselves (2004):  A search for “John Kerry” will first return entries inwhich the entire search term appears in the headline.  Even though Google Newsexamines the full text of articles when looking for a search term, it puts extra weight onthe headline when it ranks the results.  So, stories with headlines such as “John Kerry liesabout his record” will receive a higher rank than stories with headlines such as “Kerrycampaigns in Ohio” (omitting his first name).  Traditional media – which tend to be lessbiased than many alternative, online-only news sources – generally identify people inheadlines by their last names only.  As a result, articles from these news organizationsmay often be outranked in Google News search results by those from sites that do notfollow this practice.  Given this peculiarity, the use of full names in the searches analyzedhere could be seen as a weakness in the study, but in fact it emulates the behavior of anaverage user.  Unaware of the distinction, a user presumably is more likely to search for afull name than just a last name, resulting in a disproportionate number of results fromnon-traditional news sources.  Searches on Yahoo News do not appear to exhibit thistendency as frequently. A Question of Balance — 19It is important to understand that this study is not an indictment of Google News’spractice of automatically ranking the top stories on its front page and section fronts.While Google News’s ranking methods may be flawed, as some have charged, this studyis concerned only with the site’s search results.  It should also be noted that Google Newsdoes not distinguish between factual and opinion pieces in its search results (Stoll, 2004).Thus, an editorial may appear along with straight-news stories, even though the formerrepresents a particular point of view while the latter are supposed to be reasonablybalanced recitations of fact.  It is not clear that average users can make the distinction,especially given the many online-only sources that often peddle a confusing mixture offact and opinion.  Accordingly, this study makes no attempt to separate news fromeditorial content.The main flaws in the study are with the coding scheme.  Better coder training, amore detailed coding manual and a more precise definition of “favorability” wouldalmost certainly have improved intercoder reliability, which, while not low enough to callinto question the results, is below expectations.  Additionally, using sentences as units ofobservation makes for some ambiguity in the coding process.  If one sentence containsmultiple distinct propositions, or a single proposition stretches across multiple sentences,some of this granularity is lost in the current coding method.If users are looking for current factual information about a political candidate, thisstudy concludes that they are more likely to find it by searching Yahoo News.  If, on theother hand, users want a wide range of alternative viewpoints, then Google News may betheir best bet. A Question of Balance — 20ReferencesDallas, Jim. (2004) “From the Department of ‘Huh?’” Burnt Orange Report [weblog].Retrieved April 12, 2005, fromhttp://www.burntorangereport.com/archives/001234.htmlGaither, Chris. (2005, April 11). “Web Giants Go With Different Angles in Competitionfor News Audience.” Los Angeles Times, p. C-1.Google. (2004). About Google News. Retrieved Dec. 9, 2004, fromhttp://news.google.com/intl/en_us/about_google_news.htmlGroseclose, Tim, and Milyo, Jeff. (2003). A Measure of Media Bias [working paper].Retrieved Dec. 9, 2004, fromhttp://www.stanford.edu/~wacziarg/mediapapers/GrosecloseMilyo.pdfKrippendorff, Klaus. (1980). Content analysis: An Introduction to its Methodology.Beverly Hills, CA: Sage.Lasica, J.D. (2004, Sept. 24). Balancing act: how news portals serve up political stories.Online Journalism Review. Retrieved Dec. 9, 2004, fromhttp://ojr.org/ojr/technology/1095977436.phpLee, Martin A., and Solomon, Norman. (1990). Unreliable Sources: A Guide to DetectingBias in News Media. New York: Carol Publishing.Olsen, Stefanie, and Hansen, Evan. (2005, March 25). “All the news that robots pick.”CNET News.com. Retrieved April 28, 2005, from http://news.com.com/2100-1038_3-5635161.htmlStoll, Nathan. (2004, Nov. 13). [personal communication with Google News associateproduct manager]Tabarrok, Alex. (2004). “Surprise! Fox News Is Fair and Balanced!” MarginalRevolution [weblog]. Retrieved April 12, 2005, fromhttp://www.marginalrevolution.com/marginalrevolution/2003/09/surprise_fox_is.htmlWeber, Robert Philip. (1990). Basic Content Analysis. Newbury Park, CA: Sage. A Question of Balance — 21Appendix A: Coding instructionsProcedureWhen you are assigned an article to code, you will evaluate it in two ways: unit-by-unit and overall. The units of observation are sentences. When you code unit-by-unit,you must consider only the individual unit you are coding. When you code the articleoverall, you can consider aspects of the article, such as the headline, that can't be takeninto account in a unit-by-unit analysis. If there are any technical or proceduralirregularities in the coding, please make a note of it in the comments field.When you have completed the coding process for an article, double-check yourresponse (since you can't go back) and hit the "Submit responses" button. Your responseswill be recorded, and you'll be given the opportunity to continue on to another article.GuidelinesWe are looking for favorable and unfavorable references to John Kerry in theresults of a search on his name, and the same for George W. Bush. You will be codingthe text of articles returned in search queries Yahoo News and Google News.To keep coders from spending an inordinate amount of time on any one story,stories longer than 25 units (sentences) will be truncated. Most stories are shorter thanthis anyway.Here are the basic rules for coding individual units: A Question of Balance — 22 A unit can only be coded as favorable or unfavorable if it contains anunambiguous message that, taken independently of other units, is favorable toone candidate or the other. Otherwise it must be coded neutral. If a unit contains both favorable and unfavorable references to the samecandidate, or it contains only favorable or unfavorable references to bothcandidates, it should be coded neutral. If a unit contains one or more favorable references to one candidate and one ormore unfavorable references to the other candidate, only the first reference inthe sentence should be considered in coding. If there is uncertainty about how different people might interpret a unit, itshould be coded neutral.On the coding form, mark each sentence as favorable to Bush, favorable to Kerry,unfavorable to Bush, unfavorable to Kerry or neutral. Please observe the followingdefinitions when considering what are favorable and unfavorable references.The following may be considered favorable or unfavorable references: Direct references to the candidate (by name or other obvious identifier -- e.g.,"my opponent", "the senator") Quotes from candidates (or their surrogates) about themselves or each other References to actions or statements by the Bush or Kerry campaigns News directly related to candidates' issues or policies where it is clear that thenews is damaging or helpful to a particular candidate The following should be left marked as "netural": A Question of Balance — 23 Mentions of the actions of parties, aides, colleagues, etc., unless they aredirectly related to the campaign General ideological assertions and political observations that are subjectiveand can't be considered positive or negative for either candidate (e.g., "biggovernment is bad" or "social security is broken") Any citation of poll results (since it is difficult to weight poll results fairly) Any mention that cannot be clearly determined to be favorable or unfavorableto a particular candidateWhat constitutes favorable and unfavorable? If a reference cannot be clearly construed as favorable or unfavorable (e.g.,"John Kerry has a rich wife" could be interpreted either way), it should beignored Instances where a favorable adjective is used to describe a neutral orunfavorable action (e.g., "...efficient in his criticism of Bush") do not count asa favorable mention. Same with unfavorable adjectives. Historical references can be coded as favorable or unfavorable only if there isa clear relationship to the candidate and it can be clearly discerned as beingfavorable or unfavorable (e.g., "Truman didn't apologize for war mistakes, soBush shouldn't have to either" could be coded as favorable to Bush) If a candidate's actual or alleged associate or ally is portrayed negatively (orpositively), the ally's relationship to the candidate counts as a singleunfavorable (favorable) reference. (e.g., "Arafat is a murderer. Arafat is a A Question of Balance — 24thug. Arafat is derailing the peace process. Arafat endorses Kerry." Only thelast sentence is coded as unfavorable to Kerry.) A Question of Balance — 25Appendix B: Sources of articles returnedYahoo News returned more articles from traditional media sources (in boldface)than Google News did.  A traditional news source is defined as a wire service,newspaper, magazine, TV station, radio station, broadcast network or cable network,accessed either directly or through a news aggregator.Google NewsVOANewstruthoutChicago MaroonFt. Worth Star-TelegramDaytona Beach News-JournalNME.comUselessKnowledge.comUnconfirmedSources.comThe Jewish PressPRNewswireAxisofLogicThe (Carlisle, Pa.) SentinelSioux Falls Argus Leader quoted on Lucianne.comBiloxi (Miss.) Sun-HeraldDenver Post quoted on Lucianne.comBloombergCBC News (Canada)VOANewsJerusalem PostInternational Herald TribuneNew York PostUselessKnowledge.comXinhuaAP via San Jose Mercury NewsAP via Duluth News TribuneSalon.comBloombergThe AustralianINDOlinkLawrence (Kan.) Journal-WorldCNNAP via canada.comBusiness-Standard.com (India)MichNews.comRushLimbaugh.comTVM (Maldives) via MaldivesInfoPRNewswire via Yahoo NewsMichNews.comwestcoastmusicReadaBet.comThisDay (Nigeria) via AllAfrica.comAP via Canada.comThe Washington DispatchRushLimbaugh.comUselessKnowledge.coms5000.comWashington Times via The Conservative VoiceTVM (Maldives) via MaldivesInfoScranton (Pa.) Times TribuneAP via WHEC-TV (Rochester, N.Y.)(60% traditional media sources)Yahoo NewsChannelNewsAsia.comTheWGALChannel.com (Harrisburg, Pa.)Whitehouse.govTheWGALChannel.com (Harrisburg, Pa.)APPRNewswire via Yahoo NewsPRNewswireWhitehouse.govWhitehouse.govGuardian Unlimited (U.K.)The Southern IllinoisanAP via Daily Herald (Arlington Heights, Ill.)The Smoking GunAFP via Yahoo NewsWhitehouse.govBloombergAFPWCPO.com (Cincinnati)New Zealand HeraldAPAP via Yahoo NewsWPXI.com (Pittsburgh)AP via Duluth News TribuneIndia DailyAFP via Yahoo NewsBloombergINDOlinkKnight RidderBloombergReuters via Australian Broadcasting Corp.AP via Canada.comWhiteHouse.govWorldNetDaily.comWhitehouse.govAP via PhillyBurbs.com (N.J.)PRNewswire via Yahoo NewsKyodo News via Yahoo AsiaAP via WNEP-TV (Scranton, Pa.)AP via Canada.comKYW Newsradio 1060 (Philadelphia)The (Youngstown, Ohio) VindicatorThe Times of IndiaAP via Canada.comIndieWireIndia OnlineAP via WHEC-TV (Rochester, N.Y.)AFP via Khaleej Times (U.A.E.)Editor and Publisher via Yahoo NewsAP via Yahoo News(76% traditional media sources)</td>
    </tr>
    <tr>
      <th>90</th>
      <td>news</td>
      <td>University of Nebraska - Lincoln\n\nDigitalCommons@University of Nebraska - Lincoln\nFaculty Publications, College of Journalism &amp; Mass\nCommunications\n\nJournalism and Mass Communications, College of\n\n2016\n\nTeaching Fairness in Journalism: A Challenging\nTask\nJoseph Weber\nUniversity of Nebraska–Lincoln, josephweber@unl.edu\n\nFollow this and additional works at: http://digitalcommons.unl.edu/journalismfacpub\nPart of the Journalism Studies Commons\nWeber, Joseph, "Teaching Fairness in Journalism: A Challenging Task" (2016). Faculty Publications, College of Journalism &amp; Mass\nCommunications. 89.\nhttp://digitalcommons.unl.edu/journalismfacpub/89\n\nThis Article is brought to you for free and open access by the Journalism and Mass Communications, College of at DigitalCommons@University of\nNebraska - Lincoln. It has been accepted for inclusion in Faculty Publications, College of Journalism &amp; Mass Communications by an authorized\nadministrator of DigitalCommons@University of Nebraska - Lincoln.\n\nPublished in Journalism &amp; Mass Communication Educator (2015) 12pp.\ndoi: 10.1177/1077695815590014\nCopyright © 2015 AEJMC; published by SAGE Publications.\nUsed by permission.\nPublished online August 19, 2015.\n\ndigitalcommons.unl.edu\ndigitalcommons.unl.edu\n\nTeaching Fairness in Journalism:\nA Challenging Task\nJoseph Weber\nAssociate Professor, College of Journalism and Mass Communications,\nUniversity of Nebraska–Lincoln, 307 Andersen Hall, Lincoln, NE 68588-0474, USA\nemail josephweber@unl.edu\nAbstract\nObjectivity has long been contentious in American journalism. Many practitioners call it essential to a news organization’s credibility. Critics, however, hold objectivity is impossible and\nurge reporters simply to reveal their biases. For educators, teaching objectivity is challenging. Some, seeking a middle ground, instead urge fairness and balance, or counsel “impartiality.” Even such approaches are challenging. This article explores the difficulties, based on\na study where students were lectured on fairness, balance, objectivity, and bias. They wrote\nnews stories before and after the lessons. Evaluators found no substantial improvement in\nfairness and increased bias, however, pointing up the difficulties involved.\nKeywords: bias, fairness, objectivity, balance, media education, teaching fairness, impartiality\n\nIntroduction\nObjectivity has fallen into disfavor in many quarters of journalism in recent years.\nBiased coverage of the news seems to be in ascendance, as viewpoint-based websites such as The Washington Post’s PostEverything site multiply (Kushner, 2014) and\nopinion-driven television enterprises such as Fox News, on the right, and MSNBC,\non the left, compete for attention along ideological lines. Some prominent media\noutlets appear to offer less truly fair and balanced coverage as they cater to splintering audiences, seemingly wagering that audiences prefer to have their biases reinforced rather than challenged by journalism that aspires to impartiality. As the\nPew Research Journalism Project study showed in October, 2014, “When it comes\n1\n\n2    J o s e p h W e b e r i n J o u r n a l i s m &amp; M a s s C o m m u n i c a t i o n E d u c a t o r ( 2 0 1 5 )\n\nto getting news about politics and government, liberals and conservatives inhabit\ndifferent worlds. There is little overlap in the news sources they turn to and trust”\n(Mitchell, Gottfried, Kiley, &amp; Matsa, 2014).\nThe battle over objectivity hit a recent high point in fall 2013. Former New York\nTimes executive editor Bill Keller and former Guardian columnist Glenn Greenwald\nclashed then in Keller’s op-ed column about the desirability of what Keller called\n“aggressive but impartial” journalism. Keller held that journalists who set aside\ntheir opinions “to follow the facts—as a judge in court is supposed to set aside\nprejudices to follow the law and the evidence … can often produce results that are\nmore substantial and more credible” than today’s activist bloggers or earlier opinion-driven pamphleteers and muckrakers. In making his case, Keller gave voice to\nwhat has long been an article of faith for mainstream journalists.\nCountering this, Greenwald acknowledged that some “superb reporting” emerged\nfrom the traditional approach but said it “has also produced a lot of atrocious journalism and some toxic habits that are weakening the profession.” Greenwald, who\nshared a 2014 Pulitzer Prize for reporting the leaks of former National Security\nAgency contractor Edward Snowden, complained that a “journalist who is petrified of appearing to express any opinions will often steer clear of declarative sentences about what is true ….” Furthermore, Greenwald argued, “Human beings are\nnot objectivity-driven machines. We all intrinsically perceive and process the world\nthrough subjective prisms. What is the value in pretending otherwise?” (Keller, 2013)\nAlthough the rise of blogging, websites, and fragmented TV audiences gives\nfresh currency to the debate, the argument predates Greenwald and Keller. “Objectivity is considered doomed to failure and dismissed as an unattainable standard.\nThis discredit has become radical as some scholars have gone so far as to question\nobjectivity as a desirable norm,” Sandrine Boudana (2011), now at Tel Aviv University, wrote in Media, Culture &amp; Society. Earlier, in 2004, journalist Geneva Overholser contended in Nieman Reports that “… ‘objectivity’ as a touchstone has grown\nworse than useless…. To the extent that objectivity still holds sway, it often produces a report bound in rigid orthodoxy, a deplorably narrow product of conventional thinking.” Calling the objective approach an “ineffective and even harmful\nguide,” she held that a “forthright jettisoning of the ‘objectivity’ credo, and a welcoming of the diverse media landscape springing up around us, could have freeing effects.” (Overholser, 2004) A year before, in 2003, Brent Cunningham (2003)\nobserved in Columbia Journalism Review that “few would argue that complete objectivity is possible, yet we bristle when someone suggests we aren’t being objective—\nor fair, or balanced—as if everyone agrees on what they all mean.”\nFaced with such disputes, journalism educators wrestle with how best to teach\ntheir students to practice the craft. Should they hew to the ideal of objectivity or\nshould they yield to the current clamor for viewpoint-oriented and ideologically\ndriven news coverage? Is it possible for students to be taught to set aside their biases and report evenhandedly? If that is desirable, how might one do that? This\nstudy, involving students in a pair of advanced-reporting classes, explored a potential methodology for assessing student biases and examined one avenue for addressing them.\n\nTeaching Fairness in Journalism: A Challenging Task\n\n3\n\nLiterature Review\nFor American journalists, the challenge to report and record the news objectively\nhas a long and tumultuous history. Suggesting he would deliver impartial and thorough reporting, editor James Gordon Bennett in 1835 announced that his then-new\nNew York Herald would “record facts on every public and proper subject, stripped\nof verbiage and coloring” (Mindich, 1998). But his critics thought him incapable of\nfairness, much less objectivity or impartiality. Walt Whitman, a newspaperman as\nwell as a poet, called his competitor\na reptile marking his path with slime wherever he goes, and breathing mildew at\neverything fresh or fragrant; a midnight ghoul, preying on rottenness and repulsive filth; a creature, hated by his nearest intimates, and bearing the consciousness\nthereof upon his distorted features, and upon his despicable soul; one whom good\nmen avoid as a blot to his nature— whom all despise, and whom no one blesses—\nall this is James Gordon Bennett.\n\nSpeaking generally of mid-19th-century journalism, Whitman also said, “Scurrility—the truth may as well be told—is a sin of the American newspaper press.”\n(Reynolds, 1995)\nStill, some journalists strode toward objectivity in the middle and late 1800s, and,\nearly in the 20th century, they enshrined it as a central journalistic value. When a\ngroup of New York editors in 1848 established the Associated Press (AP), they created a news service that by the end of the century was delivering dispatches that sociologist and media historian Michael Schudson said “were markedly more free from\neditorial comment than most reporting for single newspapers.” Schudson speculated\nthat the AP steered a middle course because it served papers with widely varying allegiances. Sensationalism prevailed in many newspapers, nonetheless, at least until\n1896, when the New York Times began rising to prominence because it hewed to an\n“information” model of news delivery rather than the “story” model others used,\nthe sociologist reported (Schudson, 1978). After World War I, devotion to objectivity\ntook hold, such that in 1923, the American Society of Newspaper Editors adopted the\nCanons of Journalism at its first convention, mandating that “news reports should be\nfree from opinion or bias of any kind” (Schudson, 2003). Even as journalists unionized, and fears arose that impartial coverage of business and labor issues would suffer, the American Newspaper Guild, in 1934, adopted a code of ethics that called for\naccurate and unbiased reporting, guided “only by fact and fairness.” By the end of\nWorld War II, Schudson and Tifft (2005) reported that objectivity was “universally\nacknowledged to be the spine of the journalist’s moral code.”\nTo be objective, advocates said, journalists needed to understand their biases\nand, despite them, adopt a scientific fact-based approach, argued press critic Walter Lippmann. In 1920, he and a colleague, Charles Merz, an associate editor for\nthe New York World, lambasted coverage of the Russian Revolution in the rival New\nYork Times for falling short. “In the large, the news about Russia is a case of seeing\nnot what was, but what men wished to see,” they wrote. Lippmann wanted the individual journalist “to remain clear and free of his irrational, his unexamined, his\nunacknowledged prejudgments in observing, understanding and presenting the\n\n4    J o s e p h W e b e r i n J o u r n a l i s m &amp; M a s s C o m m u n i c a t i o n E d u c a t o r ( 2 0 1 5 )\n\nnews.” Journalism, Lippmann railed, was being practiced by “untrained accidental witnesses,” when journalists instead should hew to the “the scientific spirit”\nand should aspire to “a common intellectual method and a common area of valid\nfact.” (Dean, 2015)\nOne can hear echoes of Lippmann’s complaints today, as critics bemoan political polarization in which facts seem to take a backseat to rancorous argument.\nLippmann, cofounder of The New Republic and a propagandist for Washington in\nWorld War I, argued that a more “scientific” approach would have gotten readers\ncloser to the truth about the Bolsheviks whose regime the Times repeatedly misreported as near collapse, as Schudson recounts. In his book, Liberty and the News,\nLippmann complained that\nwhere all news comes at second-hand, where all the testimony is uncertain, men cease\nto respond to truths, and respond simply to opinions. The environment in which they\nact is not the realities themselves, but the pseudo-environment of reports, rumors,\nand guesses. (Schudson, 2007)\n\nHowever, Overholser looked to a more recent conflict, the Iraq War, to argue\nthat the rise of viewpoint-oriented media is healthy, so long as they are transparent and accountable. She bemoaned the “cowardly, credulous and provincial coverage leading up to the Iraq War,” and suggested that “forthrightly partisan media”\ncoverage would have been preferable (Overholser, 2004). Similarly, Cunningham\n(2003) cited examples of flawed pre-war coverage, noting that they\n. . . provide a window into a particular failure of the press: allowing the principle of\nobjectivity to make us passive recipients of news, rather than aggressive analyzers\nand explainers of it. We all learned about objectivity in school or at our first job. Along\nwith its twin sentries “fairness” and “balance,” it defined journalistic standards.\n\nToday, some professionals and educators concede that objectivity may be impossible, but they still insist on fairness and balance. Indeed, Overholser argued\nthat media dedicated to fairness and balance could appeal to those seeking “guidance through an ever more bewildering media forest.” By its nature, journalism involves choices of what to cover and how to cover it, as well as choices in questions\nto pursue. The challenge is to report so thoroughly that all reasonable views are\naired. Subjectivity and bias are impossible to avoid, in this view, but can be minimized. Even journalists at some magazines—where points of view are encouraged and expected by readers— hew to this approach. As the BusinessWeek Code\nof Ethics declared in 2009, “Because we do analytic journalism and commentaries,\nwe do not strive for perfect objectivity. But we must always strive to be fair” (The\nBusinessWeek, 2009).\nEven while acknowledging problems with the ideal of objectivity, many journalism textbooks maintain that it remains central to the journalistic mission. Melvin\nMencher’s News Reporting and Writing, a commonly used text, declares,\nIf readers want to weep or laugh, write angry letters to their senators or send money\nto the Red Cross for tornado victims, that is their business. The reporter is content to\nlay out the facts. Objective journalism is the reporting of the visible and the verifiable.\n\nTeaching Fairness in Journalism: A Challenging Task\n\n5\n\nFurthermore, it reports,\nUnfair and unbalanced journalism might be described as a failure in objectivity. When\njournalists talk about objectivity, they mean that the news story is free of the reporter’s opinion or feelings, that it contains facts and that the account is written by an impartial and independent observer. (Mencher, 2011)\n\nSimilarly, Reporting for the Media, by John R. Bender, Lucinda D. Davenport, Michael W. Drager, and Fred Fedler, urges journalists to remain objective. “Journalists have opinions and biases as do other people,” the authors write.\nBut reporters strive to be as impartial or objective as possible. They are neutral observers, not advocates or participants. They provide the details of the stories they\nreport, not their own opinions about the facts and events. Journalists express their\nopinions only in editorials and commentaries, which are clearly labeled. (Bender,\nDavenport, Drager, &amp; Fedler, 2012)\n\nTo an extent, the textbook authors provide guidance on how students can avoid\ninjecting or revealing bias in their work. “Stories are objective when they can be\nchecked against some kind of record—the text of a speech, the minutes of a meeting, a police report, a purchase voucher, a payroll, or vital statistics,” the Mencher\ntext says, for instance. More to the point, the Bender text says,\nOne way reporters keep their opinions out of stories is by avoiding loaded words,\nsuch as “demagogue,” “extremist,” “radical,” and “zealot.” Such words are often unnecessary and inaccurate. . . . Reporters can eliminate the opinions in some sentences\nsimply by deleting a single adjective or adverb: “alert witness,” “famous author,” “gala\nreception,” “thoughtful reply.”\n\nA third text, Writing and Reporting the News: A Coaching Method, by Carole Rich\n(2013), counsels,\nIf the story involves conflict, you should always get comments from both or all sides\nof an issue. Avoid one-source stories. Also, make sure you attribute your sources; including information you use from websites, other news organizations and quotes or\nstatements from people you interview.\n\nNews Reporting and Writing, by The Missouri Group (Brian S. Brooks, George\nKennedy, Daryl R. Moen, and Don Ranly), offers students a substantial discussion\nabout accuracy, fairness, and bias, as well as objectivity. The text concludes,\nThough there’s debate about just how objective a reporter can possibly be, journalists and scholars all agree about one thing: Reporting the news is not the same as expressing an opinion. The primary goal of a news story is to inform . . . . By contrast,\nthe primary goal of opinion writers and speakers is to persuade.\n\nFurthermore, The Missouri Group text urges accuracy and fairness as “paramount” values. “Fairness requires, above all, that you make every effort to avoid\nfollowing your own biases in your reporting and writing,” the text advises. It gives\n\n6    J o s e p h W e b e r i n J o u r n a l i s m &amp; M a s s C o m m u n i c a t i o n E d u c a t o r ( 2 0 1 5 )\n\nstudents guidance that they can apply to their work to assure accuracy and fairness and to avoid bias. In news stories, for instance, it counsels students to “Work\nto leave personal bias out the story” and to “Use neutral language.” Regarding fairness, it advises that they “Provide context for facts,” “Give all relevant sides of a\nstory,” and “Strive for balance.” Even in commentaries, the text counsels, writers\nmust support their personal bias with facts and reasons, as well as acknowledge\nand rebut other viewpoints, and must use civil language, not “highly charged language” (Brooks, Kennedy, Moen, &amp; Ranly, 2011).\nStill, there is little direct guidance available in the texts or in the academic literature on how an instructor can best teach students to avoid bias and assure fairness in\ntheir work. Neither is there direct guidance on how best to assess bias. This study, in\npart, sought to examine techniques for both teaching fairness and for evaluating bias.\n\nResearch Question and Method\nThe central question in this study was the following:\nResearch Question 1: How can journalism students be taught to avoid bias and\nto build fairness into their work?\nStudents in advanced-reporting classes at the University of Nebraska–Lincoln\nin the spring semester of 2014 read certain materials and then discussed those\nreadings and the topic generally in a lecture session. They also watched a relevant video in class, followed by further discussion. Beforehand, the students reviewed a set of facts and quotes—including fictitious material—about a controversial topic and wrote a news story based on that data. Then, to test whether\nstudents had absorbed the message, they were given a second set of facts and\nquotes after the lecture session and wrote a second story. The hypothesis was\nthat if they took the message in the readings and lecture to heart, they would\nshow less bias and more fairness in their second stories. To assure an impartial\nevaluation, two independent reviewers read the students’ stories and assessed\nthem for fairness and bias.\nThe pedagogical elements included the following:\n• Students read Keller’s op-ed column, “Is Glenn Greenwald the Future of\nNews?” This piece included exchanges between Keller, an advocate of impartial journalism, and Greenwald, an advocate of what Keller called a “more\nactivist, more partisan kind of journalism.” The October 2013 exchange reveals two very different approaches—Keller’s (2013) advocacy of “aggressive\nbut impartial reporting” countered by Greenwald’s view that every journalistic choice is “highly subjective” and all journalism is “a form of activism.”\n• They read the draft of a BusinessWeek feature story about a small town in\nTexas, Waxahachie, in which the writer takes a condescending tone toward\nresidents’ beliefs in creationism and their political and social views in the\n\nTeaching Fairness in Journalism: A Challenging Task\n\n7\n\ncontext of the expected arrival of physicists who would build and operate\na giant “superconducting super collider,” a particle accelerator designed to\nanswer such questions as how the universe came to be.\n• In class, they viewed a video of a July 2013 interview on Fox News with author\nReza Aslan (2013), a religious scholar who wrote Zealot: The Life and Times\nof Jesus of Nazareth. The interview was widely derided as unfair as the questioner repeatedly hammered away on the theme of why a Muslim would\nwrite about Christianity rather than exploring the themes and topics the\nwriter developed in his book. The author repeatedly answered that he was\na scholar who, like many others, had made an academic career in studying\ndifferent religions and whose personal religious views were irrelevant and\nbesides the point of the book.\n• They heard a detailed lecture and took part in a discussion of how impartiality is generally the preferred approach for journalists writing for newspapers. The lecture addressed the issue of how one can use one’s biases in reporting and research (a sense of indignation at injustice, for instance, can be\nhelpful). But the lecture also discussed how one must discard such biases in\nwriting the news. Furthermore, the lecture dealt with fairness, urging that the\nstudents comb their work to make sure they are fair to all parties involved.\nThese lessons were bracketed by the two writing assignments:\n• In the first, the facts and quotes—some fictitious—involved the consequences\nof and reactions to the legalization of marijuana in Colorado. The news developments included plans by a conservative legislator to set a standard\nfor blood tests to determine whether users could be judged guilty of driving under the influence of marijuana. There were comments by marijuana\ncritics and rebuttals by defenders.\n• In the second, the facts and quotes—again including fictitious elements—dealt\nwith abortion. The developments include a US$2 million settlement to be\npaid to the family of a woman who died in a botched abortion. The comments included criticisms by anti-abortion activists and comments by defenders of abortion.\n\nData Collection\nThe instructors collected and graded the 36 resulting papers as they normally\nwould. If explicit opinion statements appeared in the texts, they pointed them out.\nThe grading dealt with all the normal issues of student news accounts, such as completeness, journalistic writing style, proper uses of quotes and anecdotes, and so on.\nTwo faculty members, Associate Professor Bernard R. McCoy and visiting instructor John Baker, then evaluated the student work for fairness and bias. To assure that this was a blinded approach, student names were trimmed from the work\nand the papers did not include grades. The reviewers did not know which assignment preceded the lecture and which followed.\n\n8    J o s e p h W e b e r i n J o u r n a l i s m &amp; M a s s C o m m u n i c a t i o n E d u c a t o r ( 2 0 1 5 )\n\nFigure 1. Reader assessments.\n\nThe reviewers applied two scales to use in evaluating the work, one for bias and\none for fairness. They rated each story on scales of 1 to 5, with 1 indicating low ratings on both bias and fairness (thus, the best rating would be 5 on the fairness scale\nand 1 on the bias scale). They applied separate scales because it is possible (though\nundesirable) for a writer to show bias in a story but still offer a fair account with\nall appropriate viewpoints represented.\n\nAnalysis and Assessment of the Findings\nOne might expect measurable improvement in fairness and the avoidance of bias\nbetween the first story, which dealt with marijuana, and the second, which dealt\nwith abortion. As it turned out, however, there was no substantial improvement\nand, indeed, bias appeared to worsen.\nFigure 1 presents the average and median results. Story 1, with assessments\nreflected in the bar on the left, came before the lecture and discussion (pre-intervention), and Story 2, with assessments reflected in the bar on the left, came after\n(post-intervention).\n\nStatistical Analysis\nThe results suggest that the average degree of fairness rose slightly, but that the\naverage degree of bias also rose slightly. Furthermore, the median amount of fairness declined and the median degree of bias rose. If one looks at the numerical results and renders the changes in percentage terms, one sees that the average degree\nof fairness rose from 2.78 to 2.86, or 2.9%, while the average degree of bias rose from\n2.92 to 3.28, or 12.3%. With the medians, the difference appeared more dramatic:\nfairness declined 16.7%, from 3 to 2.5, while bias rose 33.3%, moving from 3 to 4.\nEven if one allows for overstatement in the medians, the results suggest a rise\nin perceived bias and no appreciable improvement in fairness.\n\nTeaching Fairness in Journalism: A Challenging Task\n\n9\n\nFigure 2. Story 1—Marijuana reader assessments.\n\nDifferences Between Reviewers\nWhen one breaks down the averages between the reviewers, variances emerge.\nOne sees differences between them on each story, and in general one sees differences in their perceptions of bias and fairness. In the marijuana story, the first story,\nReviewer 1 saw substantially more bias, on average, and modestly less fairness,\non average than Reviewer 2. In the second story, about abortion, Reviewer 1 similarly saw substantially more bias than did Reviewer 2. See results below in Figures\n2 and 3, with averages reflected by the bars on the left and medians on the right.\n\nConclusion\nAn unexpected result can be as illuminating as an expected one. In this case, one\nmight expect improvement in the avoidance of bias and in fairness after a lecture\nabout the topics. The results at best were equivocal, however, with no substantial\nimprovement in fairness and an increase in perceived bias. But one can draw conclusions, nonetheless, that can be helpful in teaching:\n• Writing interesting copy in a disinterested manner is a learned skill that takes\ntime, practice, and a teacher’s oversight over time to develop. Journalism\nstudents may need repeated critiques over a full semester or longer to develop a journalist’s mindset and approach to news stories. A single lesson—\neven when it includes substantial reading assignments, a video and a lecture/discussion session—may be inadequate. Sensitizing students to bias\nand fairness may simply take more effort.\n\n10    J o s e p h W e b e r i n J o u r n a l i s m &amp; M a s s C o m m u n i c a t i o n E d u c a t o r ( 2 0 1 5 )\n\nFigure 3. Story 2—Abortion reader assessments.\n\n• It may also be that it is impossible to expunge bias, that nontraditional journalists such as Greenwald are correct. Bias may be inescapable and efforts\nto limit it may be doomed, so the best course may be for journalists to be\ncandid about their attitudes. Veterans in the media business who are familiar with both viewpoint- oriented journalism and “straight news” approaches may not accept this, but must at least understand the argument.\n• The Keller–Greenwald debate document may need to be supplemented by\nothers. One reviewer of this article noted that students tend to find Greenwald more persuasive than Keller. While this article’s author did not find\nthe same skew toward Greenwald, additional material would be helpful,\nnonetheless. One potential additional document for study is “Objectivity\nand Impartiality for Digital News,” by Richard Sambrook of Cardiff University. Sambrook ties “real risks to public understanding” to the growth\nof subjective or advocacy news, linking this further to the significant level\nof distrust in media among the public. His data-based argument goes beyond any mere assertion that the pursuit of objectivity is valuable. (Sambrook, 2015)\n• It may also be that the topics for the stories here yielded poor examples. Abortion may be more inflammatory than marijuana. Furthermore, the facts of\nthe abortion story were especially difficult (involving a woman’s death),\nand thus may give rise to an emotional treatment that could be seen as biased. If the order of the stories had been reversed, the conclusions may\nwell have been different.\n• Perception of bias and fairness may be so subjective that assessments inevitably will be flawed. There was measurable difference between the reviewers\non the issue. To mitigate this, enlisting more reviewers—perhaps as many\nas a halfdozen— could yield more reliable measures, or at least make clear\nwhether one has outliers. So, too, could providing a specific rubric that\n\nTeaching Fairness in Journalism: A Challenging Task\n\n11\n\nreviewers could apply. The tests The Missouri Group suggests, including\nproviding context and all sides of a story, striving for balance, and using\nneutral language, may aid in building such a rubric. These tests, along with\nrequiring students to omit opinion statements, are the kinds of tests that\neditors in news organizations may well already apply, perhaps implicitly,\nas they review material for publication.\n\nFinal Thoughts\nFor journalism educators, dealing with bias and fairness is important. Indeed, it is\na central matter for journalists whose job traditionally has been to deliver news in\nan evenhanded and straightforward manner. Furthermore, students need instruction in how to achieve that approach, in how to develop habits of mind where they\nmay be guided by personal judgments but not impaired by them, and where they\nlearn to listen to and reflect varying viewpoints in their work.\nThis inquiry makes it clear that the task is not simple. Even reviewing student\nwork for bias is challenging, with different reviewers potentially bringing their own\nbiases to bear. Assessing bias and objectivity could easily throw one into a hall of\nmirrors, where bias meets bias and objectivity becomes impossible to measure, no\nmatter how many reviewers one employs. A greater number of reviewers may simply multiply the opportunity for bias.\nNonetheless, more studies on this topic may shed still more light on the questions involved. In future inquiries, those engaged in studies might screen reviewers\nto determine where their biases lay. It is possible that journalism teachers, including those who have worked professionally as editors or journalists, will assess student work through very different prisms. Thus, their views could skew their judgments and those potential skews should be weighed.\nAs for classroom practice, if teachers are to encourage fairness, the results of this\ninquiry suggest that instilling a fair-minded approach in students takes time, effort,\nand substantial criticism. The task appears to take far more than a few weeks and\na single dedicated lesson; indeed, a semester may be inadequate.\nAcknowledgments — The author received no financial support for the research, authorship,\nand publication of this article and declared no potential conflicts of interest with respect to\nits research, authorship, or publication.\n\nReferences\nAslan, R. (2013). Reza Aslan to Fox News anchor: “I wrote ‘Zealot’ because I am an expert.” Retrieved from https://www.youtube.com/watch?v=vsc64Q4KqkE\nBender, J. R., Davenport, L. D., Drager, M. W., &amp; Fedler, F. (2012). Reporting for the media.\nNew York, NY: Oxford University Press.\nBoudana, S. (2011). A definition of journalistic objectivity as a performance. Media, Culture\n&amp; Society, 33, 385-398. doi:10.1177/0163443710394899\n\n12    J o s e p h W e b e r i n J o u r n a l i s m &amp; M a s s C o m m u n i c a t i o n E d u c a t o r ( 2 0 1 5 )\nBrooks, B. S., Kennedy, G., Moen, D. R., &amp; Ranly, D. (2011). News reporting and writing. Boston, MA: Bedford/St. Martin’s.\nThe BusinessWeek code of ethics. (2009). Bloomberg Businessweek. Retrieved from http://www.\nbusinessweek.com/ethics.htm\nCunningham, B. (2003). Re-thinking objectivity. Columbia Journalism Review. Retrieved from\nhttp://www.cjr.org/feature/rethinking_objectivity.php?page=all\nDean, W. (2015). The lost meaning of “objectivity.” American Press Institute. Retrieved\nfrom http://www.americanpressinstitute.org/journalism-essentials/bias-objectivity/\nlost-meaning-objectivity/\nKeller, B. (2013, October 27). Is Glenn Greenwald the future of news? The New York Times.\nRetrieved from http://www.nytimes.com/2013/10/28/opinion/a-conversation-in-lieu-ofa-column.html?_r=0\nKushner, A. B. (Ed.). (2014, May 27). PostEverything. The Washington Post. Retrieved from http://www.washingtonpost.com/posteverything/wp/2014/05/27/\nwelcome-toposteverything/\nMencher, M. (2011). Melvin Mencher’s news reporting and writing (12th edition). New York,\nNY: McGraw-Hill.\nMindich, D. T. Z. (1998). Just the facts: How objectivity came to define American journalism. New\nYork: New York University Press.\nMitchell, A., Gottfried, J., Kiley, J., &amp; Matsa, K. E. (2014). Political polarization &amp; media habits.\nPew Research Journalism Project. Retrieved from http://www.journalism.org/2014/10/21/\npolitical-polarization-media-habits\nOverholser, G. (2004). The inadequacy of objectivity as a touchstone. Nieman Reports. Retrieved\nfrom http://www.nieman.harvard.edu/reports/article/100725/The-Inadequacy-of-Objectivity-as-a-Touchstone.aspx\nReynolds, D. S. (1995). Walt Whitman’s America: A cultural biography. New York, NY: Alfred\nA. Knopf.\nRich, C. (2013). Writing and reporting news: A coaching method. Boston, MA: Wadsworth.\nSambrook, R. (2015). Objectivity and impartiality for digital news (Digital News Report 2014).\nReuters Institute for the Study of Journalism. Retrieved from http://www.digitalnewsreport.org/essays/2014/objectivity-and-impartiality-for-digital-news-consumers/\nSchudson, M. (1978). Discovering the news: A social history of American newspapers. New York,\nNY: Basic Books.\nSchudson, M. (2003). The sociology of news. New York, NY: W.W. Norton.\nSchudson, M. (2007, December 13). Lippmann and the news. The Nation. Retrieved from\nhttp://www.thenation.com/article/lippmann-and-news#\nSchudson, M., &amp; Tifft, S. E. (2005). American journalism in historical perspective. In G. Overholser &amp; K. H. Jamieson (eds.), The press (pp. 27-28). New York, NY: Oxford University\nPress.\n\nThe author\nJoseph Weber is the Jerry and Karla Huse Professor of News-Editorial and an associate\nprofessor of journalism in the College of Journalism &amp; Mass Communications at UNL. He\nworked in magazines and newspapers for 35 years, spending most of that time reporting\nand writing for BusinessWeek and left the magazine in 2009 as its Chief of Correspondents\nand Chicago Bureau Chief.\n\n</td>
    </tr>
  </tbody>
</table>