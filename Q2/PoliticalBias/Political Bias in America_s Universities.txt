 Florida State University Libraries Electronic Theses, Treatises and Dissertations  The Graduate School Political Bias in America's Universities Jessica L. Parsons Follow this and additional works at the DigiNole: FSU's Digital Repository. For more information, please contact lib-ir@fsu.edu FLORIDA STATE UNIVERSITY  COLLEGE OF SOCIAL SCIENCES AND PUBLIC POLICY     POLITICAL BIAS IN AMERICA’S UNIVERSITIES         By  JESSICA PARSONS       A Dissertation submitted to the Department of Political Science in partial fulfillment of the requirements for the degree of  Doctor of Philosophy        2020     ii Jessica Parsons defended this dissertation on April 3, 2020. The members of the supervisory committee were:              Brad Gomez       Professor Directing Dissertation        E. Ashby Plant       University Representative        Douglas Ahler       Committee Member        Hans Hassell       Committee Member       The Graduate School has verified and approved the above-named committee members, and certifies that the dissertation has been approved in accordance with university requirements.       iii TABLE OF CONTENTS List of Tables ................................................................................................................................. iv List of Figures ............................................................................................................................... xii Abstract ........................................................................................................................................ xiv  CHAPTER 1: INTRODUCTION ....................................................................................................1  CHAPTER 2: POLITICAL BIAS AMONG UNIVERSITY ADMINISTRATORS ....................12  CHAPTER 3: POLITICAL BIAS AMONG SOCIAL SCIENCE PROFESSORS.......................36  CHAPTER 4: POLITICAL BIAS AMONG HUMANITIES PROFESSORS ..............................63  CHAPTER 5: CONCLUSION ......................................................................................................94  APPENDICES .............................................................................................................................102  A: AUDIT AND CORRESPONDENCE STUDIES ...................................................................102 B: STUDENT GROUP EXPERIMENT BALANCE TESTS .....................................................109 C: STUDENT GROUP EXPERIMENT SUPPLEMENTARY MODEL RESULTS .................113 D: NEWSPAPER EDITORIAL EXPERIMENT BALANCE TESTS .......................................116 E: NEWSPAPER EDITORIAL EXPERIMENT SUPPLEMENTARY MODEL RESULTS ....121 F: INVITED SPEAKER SPACE EXPERIMENT BALANCE TESTS ......................................125 G: INVITED SPEAKER SPACE EXPERIMENT SUPPLEMENTARY MODEL RESULTS .129 H: SOCIAL SCIENCE PROFESSORS EXPERIMENT BALANCE TESTS ............................133 I: SOCIAL SCIENCE PROFESSORS EXPERIMENT SUPPLEMENTARY MODEL RESULTS ....................................................................................................................................140 J: HUMANITIES PROFESSORS EXPERIMENT BALANCE TESTS ....................................173 K: HUMANITIES PROFESSORS EXPERIMENT SUPPLEMENTARY MODEL  RESULTS ....................................................................................................................................180 L: IRB APPROVALS ..................................................................................................................218  References ....................................................................................................................................224  Biographical Sketch .....................................................................................................................240     iv LIST OF TABLES  Table 2.1: Administrator Experiments Difference of Means Test Results ....................................25  Table 3.1: Social Science Professors Experiment Overall Difference of Means Test Results ......50  Table 3.2: Social Science Professors Experiment Political Science Difference of Means Test Results ............................................................................................................................................55  Table 3.3: Social Science Professors Experiment Sociology Difference of Means Test Results..56  Table 3.4: Social Science Professors Experiment Economics Difference of Means Test Results 57  Table 3.5: Social Science Professors Experiment Difference of Means Test Results, Combined ACT Groups: Reply at All .............................................................................................................58  Table 3.6: Social Science Professors Experiment Difference of Means Test Results, Combined ACT Groups: Substantive Reply ...................................................................................................59  Table 3.7: Social Science Professors Experiment Difference of Means Test Results, Combined ACT Groups: Days to Reply ..........................................................................................................60  Table 4.1: Humanities Professors Experiment Overall Difference of Means Test Results ...........73  Table 4.2: Humanities Professors Experiment English Difference of Means Test Results ...........83  Table 4.3: Humanities Professors Experiment Philosophy Difference of Means Test Results .....85  Table 4.4: Humanities Professors Experiment Gender Studies Difference of Means Test Results ............................................................................................................................................86  Table 4.5: Humanities Professors Experiment Difference of Means Test Results, Reply at All Variable, High and Low ACT Conditions Combined ...................................................................87  Table 4.6: Humanities Professors Experiment Difference of Means Test Results, Substantive Reply Variable, High and Low ACT Conditions Combined .........................................................89  Table 4.7: Humanities Professors Experiment Difference of Means Test Results, Days to Reply Variable, High and Low ACT Conditions Combined ...................................................................90  Table A.1: Audit and Correspondence Studies ............................................................................102  Table B.1: Balance Tests, Liberal Condition, Student Group Experiment, All Emails Sent ......109  Table B.2: Balance Tests, Conservative Condition, Student Group Experiment, All Emails .....110    v Table B.3: Balance Tests, Control Condition, Student Group Experiment, All Emails  Sent ..............................................................................................................................................110  Table B.4: Balance Tests, Liberal Condition, Student Group Experiment, All Valid  Emails  .........................................................................................................................................111  Table B.5: Balance Tests, Conservative Condition, Student Group Experiment, All Valid Emails ..........................................................................................................................................111  Table B.6: Balance Tests, Control Condition, Student Group Experiment, All Valid  Emails ..........................................................................................................................................112  Table C.1: Logistic Regression Model of Response in Student Group Experiment ...................113  Table C.2: Cox Proportional-Hazards Model for Right-Censoring of Days to Reply in Student Group Experiment ........................................................................................................................114  Table C.3: Two-Stage Heckman Selection Model of Days to Reply in Student Group Experiment ...................................................................................................................................115  Table D.1: Balance Tests, Liberal Condition, Newspaper Editorial Experiment, All Emails Sent ..............................................................................................................................................117  Table D.2: Balance Tests, Conservative Condition, Newspaper Editorial Experiment, All Emails Sent ..............................................................................................................................................117  Table D.3: Balance Tests, Control Condition, Newspaper Editorial Experiment, All Emails Sent ..............................................................................................................................................118  Table D.4: Balance Tests, Liberal Condition, Newspaper Editorial Experiment, All Valid Emails ..........................................................................................................................................119  Table D.5: Balance Tests, Conservative Condition, Newspaper Editorial Experiment, All Valid Emails ..........................................................................................................................................119  Table D.6: Balance Tests, Control Condition, Newspaper Editorial Experiment, All Valid Emails ..........................................................................................................................................120  Table E.1: Logistic Regression Model of Response, Newspaper Experiment ............................122  Table E.2: Cox Proportional-Hazards Model for Right-Censoring of Days to Reply in Newspaper Experiment ...................................................................................................................................123  Table E.3: Two-Stage Heckman Selection Model of Days to Reply in Newspaper Experiment 124  Table F.1: Balance Tests, Liberal Condition, Invited Speaker Experiment, All Emails Sent .....126   vi Table F.2: Balance Tests, Conservative Condition, Invited Speaker Experiment, All Emails Sent ..............................................................................................................................................126  Table F.3: Balance Tests, Control Condition, Invited Speaker Experiment, All Emails Sent ....127  Table F.4: Balance Tests, Liberal Condition, Invited Speaker Experiment, All Valid Emails ...127  Table F.5: Balance Tests, Conservative Condition, Invited Speaker Experiment, All Valid Emails ..........................................................................................................................................128  Table F.6: Balance Tests, Control Condition, Invited Speaker Experiment, All Valid Emails...128  Table G.1: Logistic Regression Model of Response, Invited Speaker Space Experiment ..........130  Table G.2: Cox Proportional-Hazards Model for Right-Censoring of Days to Reply in Invited Speaker Space Experiment ..........................................................................................................131  Table G.3: Two-Stage Heckman Selection Model of Days to Reply in Invited Speaker Space Experiment ...................................................................................................................................132  Table H.1: Balance Tests, Liberal-Low ACT Condition, Social Science Professors Experiment, All Emails Sent ............................................................................................................................134  Table H.2: Balance Tests, Conservative-Low ACT Condition, Social Science Professors Experiment, All Emails Sent........................................................................................................135  Table H.3: Balance Tests, Control-Low ACT Condition, Social Science Professors Experiment, All Emails Sent ............................................................................................................................136  Table H.4: Balance Tests, Liberal-High ACT Condition, Social Science Professors Experiment, All Emails Sent ............................................................................................................................137  Table H.5: Balance Tests, Conservative-High ACT Condition, Social Science Professors Experiment, All Emails Sent........................................................................................................138  Table H.6: Balance Tests, Control-High ACT Condition, Social Science Professors Experiment, All Emails Sent ............................................................................................................................139  Table I.1: OLS-Social Sciences Overall Reply or Not, Low ACT Condition .............................141  Table I.2: OLS-Social Sciences Overall Substantive Reply, Low ACT Condition.....................141  Table I.3: Logit-Social Sciences Overall Reply or Not, Low ACT Condition ............................142  Table I.4: Logit-Social Sciences Overall Substantive Reply, Low ACT Condition ...................142    vii Table I.5: Hazard Model-Social Sciences Overall, Low ACT Condition ...................................143  Table I.6: Two-Stage Selection Model-Social Sciences Overall, Low ACT Condition .............144  Table I.7: OLS-Social Sciences Overall Reply or Not, High ACT Condition ............................145  Table I.8: OLS-Social Sciences Overall Substantive Reply, High ACT Condition ....................146  Table I.9: Logit-Social Sciences Overall Reply or Not, High ACT Condition ...........................146  Table I.10: Logit-Social Sciences Overall Substantive Reply, High ACT Condition .................147  Table I.11: Hazard Model-Social Sciences Overall High ACT Condition ..................................147  Table I.12: Two-Stage Selection Model-Social Sciences Overall High ACT Condition ............148  Table I.13: OLS-Political Science Reply or Not, Low ACT Condition ......................................149  Table I.14: OLS-Political Science Substantive Reply, Low ACT Condition ..............................150  Table I.15: Logit-Political Science Reply or Not, Low ACT Condition .....................................150  Table I.16: Logit-Political Science Substantive Reply, Low ACT Condition .............................151  Table I.17: Hazard Model-Political Science Low ACT Condition ..............................................151  Table I.18: Two-Stage Selection Model-Political Science Low ACT Condition ........................152  Table I.19: OLS-Political Science Reply or Not, High ACT Condition .....................................153  Table I.20: OLS-Political Science Substantive Reply, High ACT Condition .............................154  Table I.21: Logit-Political Science Reply or Hot, High ACT Condition ....................................154  Table I.22: Logit-Political Science Substantive Reply, High ACT Condition ............................155  Table I.23: Hazard Model-Political Science High ACT Condition .............................................155  Table I.24: Two-Stage Selection Model-Political Science High ACT Condition .......................156  Table I.25: OLS-Sociology Reply or Not, Low ACT Condition .................................................157  Table I.26: OLS-Sociology Substantive Reply, Low ACT Condition ........................................158  Table I.27: Logit-Sociology Reply or Not, Low ACT Condition................................................158    viii Table I.28: Logit-Sociology Substantive Reply, Low ACT Condition .......................................159  Table I.29: Hazard Model-Sociology Low ACT Condition ........................................................159  Table I.30: Two-Stage Selection Model-Sociology Low ACT Condition ..................................160  Table I.31: OLS-Sociology Reply or Not, High ACT Condition ................................................161  Table I.32: OLS-Sociology Substantive Reply, High ACT Condition ........................................162  Table I.33: Logit-Sociology Reply or Not, High ACT Condition ...............................................162  Table I.34: Logit-Sociology Substantive Reply, High ACT Condition.......................................163  Table I.35: Hazard Model-Sociology High ACT Condition .......................................................163  Table I.36: Two-Stage Selection Model-Sociology High ACT Condition ..................................164  Table I.37: OLS-Economics Reply or Not, Low ACT Condition ...............................................165  Table I.38: OLS-Economics Substantive Reply, Low ACT Condition .......................................166  Table I.39: Logit-Economics Reply or Not, Low ACT Condition ..............................................166  Table I.40: Logit-Economics Substantive Reply, Low ACT Condition ......................................167  Table I.41: Hazard Model-Economics, Low ACT Condition......................................................167  Table I.42: Two-Stage Selection Model-Economics, Low ACT Condition ................................168  Table I.43: OLS-Economics Reply or Not, High ACT Condition ..............................................169  Table I.44: OLS-Economics Substantive Reply, High ACT Condition ......................................170  Table I.45: Logit-Economics Reply or Not, High ACT Condition .............................................170  Table I.46: Logit-Economics Substantive Reply, High ACT Condition .....................................171  Table I.47: Hazard Model-Economics, High ACT Condition .....................................................171  Table I.48: Two-Stage Selection Model-Economics, High ACT Condition ...............................172  Table J.1: Balance Tests, Liberal-Low ACT Condition, Humanities Professors Experiment, All Emails Sent ..................................................................................................................................174   ix Table J.2: Balance Tests, Conservative -Low ACT Condition, Humanities Professors  Experiment,  All Emails Sent  ................................ ................................ ................................ ............................ 175  Table J.3: Balance Tests, Control -Low ACT Condition, Humanities Professors  Experiment, All  Emails Sent  ................................ ................................ ................................ ................................ ..176  Table J.4: Balance Tests, Liberal -High ACT Condition, Humanities Professors  Experiment, All  Emails Sent  ................................ ................................ ................................ ................................ ..177  Table J.5: Balance Tests, Conservative -High ACT Condition, Humanities Professors   Experiment, All Emai ls Sent ................................ ................................ ................................ ........ 178  Table J.6: Balance Tests, Control -High ACT Condition, Humanities Professors Experiment, All  Emails Sent  ................................ ................................ ................................ ................................ ..179  Table  K.1: OLS -Humanities Overall Reply or Not, Low ACT Condition  ................................ ..181  Table  K.2: OLS -Humanities Overall Substantive Reply, Low ACT Condition  .......................... 182  Table  K.3: Logit -Humanities Overall Reply or Not, Low ACT Condition  ................................ .182  Table  K.4: Logit -Humanities Overall Substantive Reply, Low ACT Condition  ........................ 183  Table  K.5: Hazard Model -Humanities Overall, Low ACT Condition  ................................ ........ 183  Table  K.6: Two -Stage Selection Model -Humanities Overall, Low ACT Condition  .................. 184  Table  K.7: OLS -Humanities Overall Re ply or Not, High ACT Condition  ................................ .185  Table  K.8: OLS -Humanities Overall Substanti ve Reply, High ACT Condition  ......................... 186  Table  K.9: Logit -Humanities Overall Repl y or Not, High ACT Condition  ................................ 186  Table  K.10: Logit -Humanities Overall Substantive Reply, High ACT Cond ition ...................... 187  Table  K.11: Hazard Model -Humanities  Overall, High ACT Condition  ................................ ......187  Table  K.12: Two -Stage Selection Model -Humanities  Overall, High ACT Condition  ................ 188  Table  K.13: OLS -English Rep ly or Not, Low ACT Condition  ................................ ................... 189  Table  K.14: OLS -English Substant ive Reply, Low ACT Condition  ................................ ........... 190  Table  K.15: Logit -English Rep ly or Not, Low A CT Condition  ................................ .................. 191  Table K.16: Logit-English Substantive Reply, Low ACT Condition ..........................................19 1   x Table K.17: Hazard Model-English, Low ACT Condition..........................................................192  Table K.18: Two-Stage Selection Model-English, Low ACT Condition ....................................193  Table K.19: OLS-English Reply or Not, High ACT Condition ..................................................194  Table K.20: OLS-English Substantive Reply, High ACT Condition ..........................................195  Table K.21: Logit-English Reply or Not, High ACT Condition .................................................196  Table K.22: Logit-English Substantive Reply, High ACT Condition .........................................196  Table K.23: Hazard Model-English, High ACT Condition .........................................................197  Table K.24: Two-Stage Selection Model-English, High ACT Condition ...................................198  Table K.25: OLS-Philosophy Reply or Not, Low ACT Condition .............................................199  Table K.26: OLS-Philosophy Substantive Reply, Low ACT Condition .....................................200  Table K.27: Logit-Philosophy Reply or Not, Low ACT Condition ............................................201  Table K.28: Logit-Philosophy Substantive Reply, Low ACT Condition ....................................201  Table K.29: Hazard Model-Philosophy, Low ACT Condition ....................................................202  Table K.30: Two-Stage Selection Model-Philosophy, Low ACT Condition ..............................203  Table K.31: OLS-Philosophy Reply or Not, High ACT Condition .............................................204  Table K.32: OLS-Philosophy Substantive Reply, High ACT Condition ....................................205  Table K.33: Logit-Philosophy Reply or Not, High ACT Condition............................................206  Table K.34: Logit-Philosophy Substantive Reply, High ACT Condition ...................................206  Table K.35: Hazard Model-Philosophy, High ACT Condition ...................................................207  Table K.36: Two-Stage Selection Model-Philosophy, High ACT Condition .............................208  Table K.37: OLS-Gender Studies Reply or Not, Low ACT Condition .......................................209  Table K.38: OLS-Gender Studies Substantive Reply, Low ACT Condition ..............................210  Table K.39: Logit-Gender Studies Reply or Not, Low ACT Condition ......................................210    xi Table K.40: Logit-Gender Studies Substantive Reply, Low ACT Condition .............................211  Table K.41: Hazard Model-Gender Studies, Low ACT Condition .............................................211  Table K.42: Two-Stage Selection Model-Gender Studies, Low ACT Condition .......................212  Table K.43: OLS-Gender Studies Reply or Not, High ACT Condition ......................................213  Table K.44: OLS-Gender Studies Substantive Reply, High ACT Condition ..............................214  Table K.45: Logit-Gender Studies Reply or Not, High ACT Condition .....................................214  Table K.46: Logit-Gender Studies Substantive Reply, High ACT Condition .............................215  Table K.47: Hazard Model-Gender Studies, High ACT Condition .............................................216  Table K.48: Two-Stage Selection Model-Gender Studies, High ACT Condition .......................217     xii LIST OF FIGURES  Figure 2.1: Student Group Experiment Reply at All Rates with 95% Confidence Intervals.........24  Figure 2.2: Student Group Experiment Substantive Reply Rates with 95% Confidence Intervals..........................................................................................................................................26  Figure 2.3: Student Group Experiment Mean Days to Reply with 95% Confidence Intervals .....27  Figure 2.4: Newspaper Editorial Experiment Reply at All Rates with 95% Confidence Intervals..........................................................................................................................................28  Figure 2.5: Newspaper Editorial Experiment Substantive Reply Rates with 95% Confidence Intervals..........................................................................................................................................29  Figure 2.6: Newspaper Editorial Experiment Mean Days to Reply with 95% Confidence Intervals..........................................................................................................................................30  Figure 2.7: Invited Speaker Space Experiment Reply at All Rates with 95% Confidence Intervals..........................................................................................................................................31  Figure 2.8: Invited Speaker Space Experiment Substantive Reply Rates with 95% Confidence Intervals..........................................................................................................................................32  Figure 2.9: Invited Speaker Space Experiment Mean Days to Reply with 95% Confidence Intervals..........................................................................................................................................33  Figure 3.1: Social Science Professors Experiment Overall Reply at All Rates with 95% Confidence Intervals ......................................................................................................................45  Figure 3.2: Social Science Professors Experiment Overall Substantive Reply Rates with 95% Confidence Intervals ......................................................................................................................46  Figure 3.3: Social Science Professors Experiment Overall Mean Days to Reply with 95% Confidence Intervals ......................................................................................................................48  Figure 3.4: Social Science Professors Experiment Reply at All Rates by Discipline with 95% Confidence Intervals ......................................................................................................................51  Figure 3.5: Social Science Professors Experiment Substantive Reply Rates by Discipline with 95% Confidence Intervals ..............................................................................................................52  Figure 3.6: Social Science Professors Experiment Mean Days to Reply by Discipline with 95% Confidence Intervals ......................................................................................................................54    xiii Figure 4.1: Humanities Professors Experiment Overall Reply at All Rates with 95% Confidence Intervals..........................................................................................................................................74  Figure 4.2: Humanities Professors Experiment Overall Substantive Reply Rates with 95% Confidence Intervals ......................................................................................................................75  Figure 4.3: Humanities Professors Experiment Overall Mean Days to Reply with 95% Confidence Intervals ......................................................................................................................77  Figure 4.4: Humanities Professors Experiment Reply at All Rates by Discipline with 95% Confidence Intervals ......................................................................................................................78  Figure 4.5: Humanities Professors Experiment Substantive Reply Rates by Discipline with 95% Confidence Intervals ......................................................................................................................79  Figure 4.6: Humanities Professors Experiment Mean Days to Reply by Discipline with 95% Confidence Intervals ......................................................................................................................81      xiv ABSTRACT  Allegations of bias against political conservatives and in favor of liberals in America’s universities are frequently heard from a variety of critics. This dissertation puts these claims to rigorous empirical test with five large-N correspondence experiments. In three experiments, university administrators are randomly assigned to receive a request to 1) form a new political student group on campus, 2) publish a political editorial in the campus newspaper, or 3) reserve space for an invited speaker to lecture on political issues from a politically liberal, conservative, or neutral student. In comparing rates of response across conditions, there is no significant bias against conservative students by university administrators. In two additional experiments, I test the political bias of university professors in the social sciences and the humanities. Subjects in these experiments were randomly assigned to receive a request for information about working on proposed research projects where the hypothesis aligned with liberal or conservative ideologies, or was unstated. The results of these experiments reveal significant bias against conservatives on three different measures among both social science and humanities professors, driven largely by political scientists in the social science sample, and by philosophers and professors of gender studies in the humanities sample. These results have significant implications for the history of universities as havens of political activism, norms of academic freedom, opportunities for the development of social capital among campus conservatives, and the polarized decline in trust in institutions.        1 CHAPTER 1  INTRODUCTION  Conservative media, politicians, interest groups, citizens, and even academics often lament the leftist orientation and perceived intellectual decline of the American college campus (e.g. d’Souza 1991; Schmidt 2011; Wood 2017; Blackwell 2018). Indeed, based on nationally representative surveys, a majority of Republican partisans and leaners say that colleges and universities have more of a negative than a positive impact on society (Fingerhut 2017). What stokes this conservative animus toward college? Numerous conservative critics claim that universities discriminate against conservatives (Schmidt 2011; d’Souza 1991; Kimball 1990). Some particular claims of discrimination from the conservative group Campus Reform include: 1) “Refusal or long delays in granting conservative student groups recognition as official campus groups, despite the presence of many officially recognized leftist student groups,” 2) “Leftist domination of almost all official campus newspapers,” and 3) “Refusal of administration to allow student groups to present conservative speakers on campus” (Blackwell 2018). I test all three of these claims in this dissertation project. I also examine whether social science and humanities professors are more receptive to students who wish to study topics that comport with liberal ideology over those who wish to study topics compatible with conservative ideology. I utilize large N correspondence experiments to put these claims to empirical test.  In this chapter, I first review the allegations of anti-conservative discrimination on campus, their significance, and preliminary evidence on the veracity of these allegations. Next, I explain why theoretically we might expect to find either bias against conservatives, in favor of conservatives, or no bias at all.  I then review the methodology of correspondence studies in general before finally delving into the specifics of the experiments I employ in this project.   2 Allegations of Political Bias in America’s Universities   As Paul Fain notes in Inside Higher Ed, “Virtually every day Fox News, Breitbart, and other conservative outlets run critical articles about free speech disputes on college campuses, typically with coverage focused on the perceived liberal orthodoxy and political correctness in higher education” (Fain 2017). The narrative of conservative victimization on campus is nothing new. The father of modern American conservatism, William F. Buckley Jr., lamented academia’s liberal orientation in 1951 (Buckley 1951). More recently, others have made similar critiques of perceived political correctness run amok and lowered academic standards (Maranto, Redding, and Hess 2009; Sykes 1988; d’Souza 1991; Rauch 1993; McArdle 2017; Vatz 2017).   Claims of anti-conservative discrimination on college campuses are thus quite common; they are also politically influential. Republicans have moved to enact steep budget cuts to higher education at both state and national levels (Fenn 2017; Hermes 2017; Blackford 2018; Office of the Governor of West Virginia 2017). President Trump clarified the link between perceived anti-conservative discrimination on campus and Republican attacks on higher education funding in a tweet in which he proposed defunding universities that do not invite conservative speakers on campus, an idea espoused with alacrity in conservative media (Wood 2017). 1  It is true that professors are overwhelmingly liberal (Gordon and Nilsson 2011; Klein and Stern 2009). Rothman, Lichter, and Nevitte (2005) find that liberals outnumber conservatives by wide margins in every academic field they study. Even among business professors, where the liberal advantage is smallest, 49% are liberal and 39% are conservative (Rothman, Lichter, and Nevitte 2005). Gross (2013) concludes this ideological lopsidedness among the professoriate is a  1 After UC-Berkeley canceled a conservative speaker’s visit, President Donald J. Trump tweeted, “If U.C. Berkeley does not allow free speech and practices violence on innocent people with a different point of view – NO FEDERAL FUNDS?” (Wood 2017).   3 result of self-selection: similar to how plumbing is viewed as a man’s work and nursing is viewed as a woman’s work, academia is viewed as the province of liberals. Abrams (2018) finds that among higher-level university administrators, this ideological lopsidedness is even more pronounced: liberal administrators outnumber conservatives by a ratio of 12 to 1.   While there is evidence that academics discriminate against conservative job candidates or fellow academics (Yancey 2011; Inbar and Lammers 2012), the few studies that have been conducted on the subject largely conclude that conservative students face no special disadvantage on campus: they have nearly identical levels of satisfaction with their college experience as liberal students (Woessner and Kelly-Woessner 2009a), and they are not discernibly influenced by their political science professors’ ideological leanings (Woessner and Kelly-Woessner 2009b). A recent field experiment found no statistically significant difference in the responsiveness of professors in various academic fields to liberal and conservative prospective students (Fosse, Gross, and Ma 2014). A conservative commentator complained that this field experiment failed to get at the real issue of discrimination on campus: topics compatible with a liberal orientation are safe for study (e.g. the marginalization of women or minorities), while topics compatible with a conservative orientation are not (e.g. the lowering of wages through immigration) (Horowitz in Schmidt 2011). My study directly tests these influential allegations. I see whether it is easier for liberals to publish editorials in student newspapers, establish new student political organizations, and invite speakers to campus, as well as whether liberal-friendly topics are “safer” to study than conservative-friendly topics in both the humanities and social sciences.   One exception to the trend of findings of no significant bias against conservatives is Woessner, Maranto, and Thompson (2019). These authors find that liberal students report higher   4 grades and closer relationships with faculty than conservative students even after controlling for SES, SAT scores, and demographics. The authors find that conservative students report higher grades in high school, and theorize that conservatives thrive in a more structured environment, while liberals thrive in a more open, inclusive atmosphere.   Bias Against Conservatives, in Favor of Conservatives, or No Bias at All?  When it comes to predicting whether university faculty and administrators discriminate against conservative students, theory proves an inconsistent guide. There are theoretical reasons to expect anti-conservative/pro-liberal bias, pro-conservative/anti-liberal bias, and no bias at all. I outline these reasons and formulate hypotheses in this subsection.  The sheer frequency of allegations of anti-conservative bias on campus from such a variety of claimants suggests that university faculty and administrators discriminate against politically conservative students and/or in favor of liberals. Allegations of anti-conservative/pro-liberal bias in America’s universities appear frequently in the media (Fain 2017). Similar charges also emanate from politicians (e.g. Trump in Wood 2017), interest groups (e.g. Blackwell 2018), citizens (Newport and Busteed 2017), and even academics (e.g. Kimball 1990). So many claims from so many types of actors suggest a particular pattern of political bias on campus.  Another reason to expect anti-conservative/pro-liberal bias on campus is the psychology of in-group and out-group biases. Standard tenets of social identity theory suggest that individuals demonstrate favoritism toward the in-group and derogation toward the out-group (Tajfel 1979). Social identity theory has been shown to apply to political orientation in the American context with Democratic and Republican party identifiers frequently viewing each other at odds (Green 2004). The affective polarization literature has shown that partisans much prefer their partisan in-group and derogate the out-group (Iyengar et al 2019). The fact of an   5 overwhelming liberal majority in academia, coupled with the tendencies for in-group favoritism and out-group derogation suggest the following hypothesis: H1: University faculty and administrators discriminate against conservatives and/or in favor of liberals.  While claims of political bias on campus seem almost entirely to allege bias against political conservatives and in favor of liberals, we might expect a kind of overcorrection effect, in which faculty and administrators are in fact biased in favor of conservatives and against liberals. Analyses of 2018 American National Election Study data reveal the existence of a pro-outgroup bias among white liberals in regard to racial groups (Goldberg 2019). Goldberg (2019) compared the difference in feeling thermometer ratings for racial groups by various racial and political groupings, finding that white liberals actually rated whites lower than nonwhites by 13 points on average. White non-liberals, along with all other subgroups examined, rated their own race more favorably than others.   The existence of this kind of overcorrection effect, as seen in the case of white liberals who favor other racial groups more than their own, suggests that university administrators and faculty may, in an attempt to overcorrect for perceived bias against conservatives in academia at another office or point in time, offer preferential treatment to conservatives over liberals. The fact that this kind of overcorrection effect appears to exist only among white liberals does nothing to diminish its probable occurrence in the academy: academia is very liberal (Abrams 2018; Rothman, Lichter, and Nevitte 2005) and very white (National Center for Education Statistics 2017; Espinosa et al. 2019 263). In other words, it is the very type of people prone to pro-outgroup bias who populate the academy, and bias against an out-group which is under scrutiny. This reasoning leads to the following hypothesis:   6 H2: University faculty and administrators discriminate against liberal students and/or in favor of conservatives.  In addition to the two types of bias hypothesized above, there are also theoretical reasons to expect no bias at all. In this scenario, faculty and administrators treat liberal and conservative students relatively equally. We might expect no bias if the two effects described above cancel or balance each other out: perhaps there is anti-conservative/pro-liberal bias at some institutions or offices, but enough pro-conservative/anti-liberal overcorrection to even out the score. It is also possible that there is no bias in the first place. Universities adhere to strict norms of academic freedom, which apply to both employees and students (AAUP 2018). The American Association of University Professors notes that political bias of the kind investigated here directly contradicts those norms (AAUP 2018). University faculty and administrators may exhibit no political bias in an effort to comply with these professional norms.   As bureaucrats in a public bureaucracy, university administrators in particular may feel bound by professionalism and professional norms in making decisions about the distribution of university resources, such as campus space for an invited speaker, the ability to publish editorials in the campus newspaper, or official university recognition for new politically-oriented student organizations. The professors targeted by the experiments described in this dissertation are also administrators, albeit in a secondary capacity. These professors are either the “Director of Undergraduate Study” or the Department Chair. Their first role is faculty, and administration is a secondary function. The literature on professional norms in public bureaucracies described in Chapter 2 may thus be more applicable to full-time administrators and only slightly, if at all, applicable to professors acting in their capacity as administrators.     7 Correspondence and Audit Experiments  The method I use to investigate allegations of discrimination against conservatives is the correspondence experiment. This method is commonly used to investigate discrimination of various kinds, e.g. gender discrimination in hiring (Booth and Leigh 2010). In correspondence experiments, researchers use some type of correspondence and vary the characteristics of the sender to test whether recipients are biased. For example, Booth and Leigh (2010) send fictitious CVs to apply for entry-level jobs. These researchers vary the gender of the sender and compare responses across experimental conditions to gauge the presence and extent of gender bias in entry-level hiring. A closely related method is audit experiments, in which an actor (also known as a “tester”) tries to expose discrimination in some service, such as filing a police report. To see if there is racial discrimination in the ability to file a police report, the experimenters would arrange for a black tester and a white tester to try to file police reports at various police departments. They would then compare the experiences of the two testers, to see if the white tester was better able to file reports than the black tester. Such a finding would show that there is discrimination against black people in terms of attempting to file a police report.   The key to both types of studies is that subjects do not know they are in a study before participating in it. Imagine if the black and white testers informed the police that they were conducting a study on police racial discrimination before they tried to file a report. Social desirability bias would lead the police to bend over backwards to accommodate the black tester, and the study would find no evidence of racial discrimination. Of course, such a finding would be useless. It would not tell us whether there actually is discrimination, only what the police want their public image to be. In fact, such a study would be worse than useless, as it may conclude discrimination is not an issue, when in fact it might be.    8  Are such methods ethical? In a thoughtful review of the ethics of such studies, Riach and Rich (2004) cite the reasoning of the Seventh Circuit Court of Appeals in endorsing audit studies to root out racial discrimination in housing:  It is frequently difficult to develop proof in discrimination cases and the evidence provided by testers is frequently valuable, if not indispensable. It is surely regrettable that testers must mislead commercial landlords and home-owners as to their real intentions to rent or buy housing. Nonetheless, we have long recognized that this requirement of deception was a relatively small price to pay to defeat racial discrimination. The evidence provided by testers both benefits unbiased landlords by quickly dispelling false claims of discrimination and is a major resource in society’s continuing struggle to eliminate the subtle but deadly poison of racial discrimination  My research is directly analogous. The benefits of exposing discrimination outweigh the costs of not obtaining informed consent. My study meets all of the requirements set forth by IRB rule §46.116(d) to waive obtaining informed consent from subjects: 1) ”the research involves no more than minimal risk to the subjects;” 2) “the waiver or alteration will not adversely affect the rights and welfare of the subjects;” 3) “the research could not practicably be carried out without the waiver or alteration;” and 4) “whenever appropriate, the subjects will be provided with additional pertinent information after participation.”   Another important point on the ethics of audit and correspondence studies is the precedent set by numerous previous studies. Researchers have conducted countless audit and correspondence studies. These types of experiments have found their way into top academic journals, including the Proceedings of the National Academy of the Sciences, the American Economic Review, the American Journal of Political Science, and Psychological Science). Table A.1 in Appendix A for a large listing of audit and correspondence studies.   Clearly there is ample precedent for audit and correspondence studies in both academia and the law. The IRBs of the above researchers’ institutions, as well as the journals that publish   9 such work, have thus concluded that subjects are not unduly harmed by this method. The benefits of exposing discrimination outweigh the potential harms.  Correspondence experiments like those described in this dissertation are powerful tools to understand causal, generalizable relationships. Correspondence experiments are a type of field experiment, maximizing both internal and external validity. Randomized experiments constitute the gold standard of internal validity. We can be sure that the treatment is the cause of differences in the dependent variable by eliminating all other possible causes. Field experiments take place in subjects’ natural settings, rather than an artificial laboratory setting, enhancing external validity. Thus, we can be sure the relationships we observe are causal in nature and applicable to the real world. Dissertation Overview  This dissertation has three parts. In the first part, I explore the possibility that university administrators discriminate against students based on political ideology in terms of establishing new political student groups, submitting political editorials to the campus newspaper, and reserving campus space for invited speakers to lecture on political issues. I email the administrators responsible for overseeing these functions at every college and university listed by US News and World Report as a National University, National Liberal Arts College, Regional University, or Regional College. I randomly assign these administrators to hear whether the group, editorial, or speaker about which I inquire is liberal, conservative, or of an unspecified political ideology. I analyze the data to examine differences in responsiveness across experimental conditions and by various subgroupings. This simple, clear design provides a powerful, objective test of the widespread claims that university administrators discriminate against conservative students.   10  In the second part of this dissertation project, I explore the possibility that social science professors discriminate against prospective students based on political ideology. I email professors at every sociology, economics, and political science department at every college or university listed as a National University, National Liberal Arts College, Regional University, or Regional College by US News and World Report. In a set-up similar to the experiments just described, I randomly assign one-third of the professors to receive a request for more information about research interests from a liberal student, one-third to receive that same request from a conservative, and another third to hear that request from a student with an unspecified political ideology. I also randomize indicators of the student’s IQ or academic capability in the form of ACT scores at either the 75th (28) or 25th (24) percentile. I then compare the response rates across the six experimental conditions.   The third part of this dissertation is also a correspondence experiment, similar to the first two. In this experiment, I explore whether humanities professors discriminate against students based on political ideology. I email professors at every philosophy, history, and English department at every college or university listed as a National University, National Liberal Arts College, Regional University, or Regional College by US News and World Report. I randomly assign one-third of the professors to receive a request for more information about research interests from a liberal student, one-third to receive a request from a conservative, and another third to receive a request from a student with an unspecified political ideology. I also use the 3x2 factorial design described above in the social science professors experiment by randomizing whether the professor receives a request from a student with a high (28) or low (24) ACT score. I conduct separate experiments for humanities and social science professors due to the different   11 research interests of these professors, as well as a greater potential for discrimination in the humanities, where professors are more liberal (Rothman, Lichter, and Nevitte 2005).  Like all studies, the experiments described herein are not without their limitations. The primary methodological limitation of these studies is that they are cross-sectional. They offer a snapshot of a single point in time, but do not allow us to examine changes over time. It could be the case that bias against political conservatives once dominated college campuses, but no longer remains an issue or is on the decline. The cross-sectional nature of these studies would miss such a finding. Nevertheless, the experiments reported in this dissertation represent the best, most direct test of the claim that universities are biased against conservative students.      12 CHAPTER 2  POLITICAL BIAS AMONG UNIVERSITY ADMINISTRATORS  The experiments described in this chapter involve university administrators. University administrators manage and administer services, programs, and people on college campuses. A college administrator might, for example, work in admissions, budgets, personnel, payroll, records, curricula development, alumni services, financial aid, or maintenance (Princeton Review 2019). The American Association of University Administrators, the chief professional organization for university administrators, publishes a guideline of ethical principles for administrators, including commitment to the highest level of integrity, upholding fairness and equality, and accuracy and transparency (AAUA 2017).   Critics of university administrators point to massive growth in the size and expense of their positions, and problems that arise from that growth. Citing U.S. Department of Education statistics, Belkin and Thurm (2012) state that the number of college administrators increased 50% faster than the number of college instructors between 2001 and 2011. Greene (2010) finds that the number of administrators increased by 39% but the number of positions devoted to instruction, research, and service increased by only 18% between 1993 and 2007 at America's leading universities. This increase in the number of administrative positions brought with it an increase in the cost of administration relative to instruction: Greene (2010) estimates that administrative spending increased by 61% while instructional spending increased by 39% in this period. This increase in the size and expense of university administration is blamed for the massive increase in the cost of higher education (Zwycki and Koopman 2017; Greene 2010), reduced faculty influence on campus (Ginsburg 2011), and policies which lead to the hiring of lower quality faculty out of "political correctness" (Vedder 2018). Vedder (2018) estimates that   13 fees at a typical American college could fall approximately 10%-20% by drastically cutting administration with hardly any impact to and academic functionality.  In this chapter, I describe and present the results of three experiments in which subjects are randomly assigned to receive a request to 1) form a new registered student organization, 2) publish an editorial in the campus newspaper, or 3) reserve a room on campus for an invited speaker from a student with politically conservative, liberal, or unspecified views. I compare response rates across experimental conditions to determine whether subjects discriminate against students based on political ideology. As explained in Chapter 1, there are reasons to expect anti-conservative/pro-liberal bias, anti-liberal/pro-conservative bias, or no political bias at all. Perhaps university professors favor their political in-group and are thus biased against conservatives and in favor of liberals, given their overwhelmingly liberal political orientation (Abrams 2018). Alternatively, we might expect administrators to "overcorrect" for the anti-conservative bias on campus so widely derogated in the media, and thus to exhibit an anti-liberal/pro-conservative bias. A third possibility is no political bias, which might occur if the two biases just described "cancel out" or if administrators' professional norms of "fairness and equality" extend to political ideology.  To expand on this latter reason why administrators in particular are not likely to exhibit discriminatory behavior, such behavior conflicts with their professionalization and professional norms. Academic bureaucracies are public bureaucracies, bound by professional standards and norms, including academic freedom. The professionalization of public bureaucracies and the individual bureaucrats who staff them is a core assumption or stylized fact in formal models of public agencies (e.g, Ting 2016; Dewatripont, Jewitt, and Tirole 1999). Lawson (2009) reports in his studies of social service workers of diverse backgrounds (including case workers, social   14 workers, and supervisors), professional norms and professionalism were emphasized as key factors driving bureaucratic decision-making. As professional bureaucrats bound by the professional norms of their agencies that explicitly prohibit political discrimination (AAUA 2017), we might expect university administrators to treat students in an equal fashion in their requests for university resources, regardless of political ideology. Methodology  The methodology utilized in the experiments described here is the correspondence experiment, described more generally in Chapter 1. I email university administrators at colleges across America seeking help with some request. Subjects are randomly assigned to receive a request from a politically conservative, liberal, or neutral sender. A comparison of the response rates across experimental conditions will reveal the presence and extent of political bias among university administrators. I analyze the data by first displaying the response rates in each condition and in various sub-samples (e.g. among administrators at liberal arts colleges) along with the standard errors of these response rates. I also present the results of difference of means tests, OLS regressions, and predicted probabilities of response.   The first experiment I conducted involved the formation of new registered student groups on campus. I emailed 1,470 administrators. I deleted 71 observations due to undeliverable addresses or automatic replies, leaving 1,399 cases, including 469 in the Conservative Condition, 465 in the Control Condition, and 465 in the Liberal Condition. Statistical power tests show that a two-sample difference of means test is able to detect even small-sized effects (difference = .2) at conventional levels of statistical significance (alpha = .05) between the Liberal and Control Conditions with a level of statistical power at .86. Difference of means test are able to detect   15 small-sized effects at conventional levels of statistical significance between the Conservative and Control Conditions with statistical power of .86.   Research assistants from the Department of Political Science’s Research-Intensive Bachelor’s Certificate (RIBC) program at Florida State University first identify the administrators at each of these institutions who are responsible for helping students establish new campus organizations. The assistants identify administrators via appropriate search terms, e.g. “[university name] start a new student group.” The assistants make use of Google and the universities’ own search engines, as well as any information on university websites indicating the appropriate person to contact, which is typically found on the university web-page on student life or engagement. The assistants also record the size (enrollment), endowment, setting (urban, suburban, or rural), geographical region, school type (e.g. regional or liberal arts college), and whether the school is religious (whether or not the school mentions religious terms such as “God” or “faith” in its mission statement).  Then, I randomly assign administrators to one of three conditions: the Liberal Treatment Condition, the Conservative Treatment Condition, or the Control Condition, using pure random assignment. I conduct balance tests to ensure the expectation of equivalence across conditions is met (see Appendix B). These administrators receive a request from a student to start a new political student group on campus from either a politically liberal student (the Liberal Treatment Condition), a conservative student (the Conservative Treatment Condition), or a student with no specified ideology (the Control Condition).  Administrators are given one month from the initial request to respond. The first dependent variable in this study is a binary indicator of whether the administrator did (1) or did   16 not reply (0) within 30 days, following previous, similar research, which notes the value of this measure is that it is objective (Butler and Broockman 2011 467). The second dependent variable is whether the administrator gave a substantive reply within 30 days (1) or not (0). Replies were counted as substantive if they encouraged the student to form the proposed group or laid out the next steps necessary to do so.   The third dependent variable is the number of days it took subjects to reply. Non-replies and replies beyond the 30-day response window were coded as 31. These data on the number of days to reply are analyzed with the difference of means tests presented in the results section, as well as Cox Proportional-Hazards models for right-censoring, and two-step Heckman selection models. Cox Proportional-Hazards models are commonly used in analyses of medical data, in which researchers seek to know the probability (or “hazard”) that a given case in the dataset experiences a particular event at a particular time of observation. For example, events may include death or relapse. Here, the event of concern is whether a subject replied. In the two-step models, the first model is a probit selection model, taking the reply at all variable as the dependent variable and treatment indicators along with enrollment as predictors. The second model takes number of days as the dependent variable and treatment indicators as predictors. The results from the first model are used to construct a variable, the inverse Mills ratio, that captures the selection effect in the second model. I include enrollment as a predictor in the first model (the selection model) to act as an instrument (Sundstrom 2016). The Cox Proportional-Hazards models may be interpreted by the value and significance of the hazard ratio for predictor variables. A hazard ratio below 1 suggests a smaller risk, while a hazard ratio above 1 suggests a higher risk. For example, a hazard ratio of .45 for the Conservative Treatment indicator would suggest a lower “risk” of receiving a reply, relative to the Control Condition. The two-stage   17 selection models may be interpreted with reference to the sign and significance of coefficients for predictor variables: a positive significant coefficient for the Conservative Treatment indicator, for example, would suggest it took subjects longer to reply to a conservative student relative to a student who did not state his political ideology.  The emails were sent February 18-20, 2019 and the response window closed 30 days later, in March 20-22, 2019. Given the 500 email per day limit policy of Gmail, the emails had to be sent across three days. The day on which the emails were sent was randomized with pure random assignment to obviate any potential effects of chronological order.  Conservative critics of academia would hypothesize that administrators will respond more to liberal students than to the students of unspecified ideology, and less to conservative students than to the students of unspecified ideology. I analyze the data with difference of means tests to see if there are any significant differences in responsiveness across experimental conditions. I also analyze the data with OLS regression and predicted probabilities of response.   I measure the independent variables, other than the treatment indicators, by relying on data from US News and World Report’s 2019 rankings of American colleges and universities. In all models, I include an indicator of Liberal Treatment Condition and an indicator of Conservative Treatment Condition, comparing the effects of these treatments to the Control Condition, left as the reference category. I use the US News and World Report categorization to record college setting, either urban, suburban, or rural. I use suburban as the reference category in all models with setting. I use US News and World Report data to measure school endowment, ranking, and enrollment.2 I use US News and World Report to classify school region as either  2  I broke these figures into deciles in order to preserve subject anonymity and comply with the IRB’s requests.    18 North, South, Midwest, or West. In some comparisons to follow, I group all schools in the North, Midwest, and West as schools outside the South, and only schools classified as in the South by US News and World Report as Southern schools.3 For all schools, I measure school religiosity by reading the school mission statement. A school’s religiosity is coded 1 if its mission statement has any mention of God, Jesus, faith, or similar faith-based vocabulary and 0 otherwise. I rely on the US News and World Report categorizations to determine a school’s type (either National University, National Liberal Arts College, Regional University, or Regional College).  If difference of means tests indicate that administrators are significantly more responsive to a liberal student than to student of unspecified ideology, this result will constitute evidence of pro-liberal bias. If administrators are significantly less responsive to a conservative student than a student of unspecified ideology, this result will constitute evidence of anti-conservative bias. Alternatively, there could be a pro-conservative bias (if administrators are significantly more responsive to the conservative student than to the student with no specified ideology) or an anti-liberal bias (if administrators are significantly less responsive to the liberal student than to the student with no specified ideology). Pro-conservative or anti-liberal biases would suggest that university administrators, cognizant of the allegations that they are discriminatory, have overcorrected to the point that they privilege conservative students or disadvantage liberals. If there is no significant difference between the treatment and control groups, but there is a significant difference between the two treatment groups, such a finding would still indicate some bias, although it would be unclear where that bias originates.  3 Following the regional classifications of US News and World Report, a school was classified as being in the South if located in any of the following states: Florida, Georgia, Alabama, Mississippi, Louisiana, Arkansas, Tennessee, South Carolina, North Carolina, Virginia, West Virginia, or Kentucky.    19  The text of the email request appears below. I composed this email with help from my undergraduate students. I had students in my undergraduate Public Opinion and Electoral Behavior course at Florida State University write emails seeking to establish a new student group for bonus points. Six students completed this task. I analyzed the grade level of writing for these emails with Microsoft Word and found the average student wrote at the 12th grade level. I then composed emails that were also at the 12th grade level of writing, so that my requests would have greater external validity. I also capitalized a couple words throughout the emails that do not actually need to be capitalized, a common mistake several of my students made, to enhance external validity even further. I chose the pseudonym Brett Clark, since it has been empirically demonstrated to elicit a high response rate (Butler and Homola 2017).  Hello,  Some other students and I were hoping to get a [liberal political /conservative  political/ political] student group established on campus, and recognized as an  official campus organization. We feel there is a Dire need for such an organization  on campus, in order for like-minded students to commiserate and build crucial  networking, organizational, and oratory skills in a welcoming and inclusive environment.    We expect at least a Dozen regular attendees of our group’s meetings, and hope  to persuade several more students to join eventually.    Are you the correct person to contact about this? If so, please relay additional  information about subsequent steps in getting the organization officially established.  If not, could you point me in the right direction?    Thank you.    Brett Clark,  Senior   The email requests come from a Gmail account (brettclark2019@gmail.com), not an official student email. Many colleges and universities have a policy that students should use their official student email account to contact faculty and administrators. A policy of this kind would explain some unresponsiveness, but it would not explain a differential responsiveness   20 across the experimental conditions. It is that difference that is of interest to us here. In other words, random assignment ensures that universities with such a policy have the same chance of being in the Liberal Treatment Condition, the Conservative Treatment Condition, or the Control Condition. Any difference in the way liberals and conservatives are treated cannot be blamed on a student email policy.  The second and third experiments described in this chapter are similar to the first. The independent and dependent variables are measured in the same way; data collection proceeded similarly, by identifying colleges from US News and World Report’s listings of American colleges classified as a National University, National Liberal Arts College, Regional University, or Regional College and identifying the appropriate person to contact; the three conditions are the same (the Liberal Condition, the Conservative Condition, and the Control Condition); the randomization is the same (pure random assignment with balance tests shown in Appendices C and E); the data analyses are the same (response rates, difference of means, OLS regression, and predicted probabilities of response with logistic regression results shown in Appendices D and F).   The second experiment identified college student newspapers and the email address to contact them. I then randomized these subjects to receive a request to publish a political editorial of either conservative, liberal, or neutral ideology. I emailed 1,058 administrators. I deleted 35 observations due to undeliverable addresses or automatic replies, leaving 1,023 cases, including 342 in the Conservative Condition, 341 in the Control Condition, and 340 in the Liberal Condition. Statistical power tests show that a two-sample difference of means test is able to detect even small-sized effects (difference = .2) at conventional levels of statistical significance (alpha = .05) between the Liberal and Control Conditions with a level of statistical power at .74.   21 Difference of means test are able to detect small-sized effects at conventional levels of statistical significance between the Conservative and Control Conditions with statistical power of .74. The text of the email message is shown below:  Hello,   I was hoping to publish a [politically liberal/ politically conservative/  political] editorial in the campus newspaper. I feel there is a Dire need for  such an editorial to appear in our campus newspaper in order to spread  awareness of several important political issues and generate dialogue.   My goal in publishing this editorial is to cause my fellow students to  reflect on the political issues I address, and discuss them frankly with  peers. I hope to provide an alternative viewpoint for students with an  opposing ideology through my editorial. It is my Firm belief that such  exposure to alternative viewpoints is an integral component in the  development of students as well-rounded intellectuals.   Are you the correct person to contact about this? If so, please relay  additional information about subsequent steps in publishing my editorial.  If not, could you point me in the correct direction?   Thank you for your assistance.    Maxwell Haas  Senior    The third experiment identified university administrators who students should contact to reserve a room for an invited speaker to deliver a lecture. I then randomized these subjects to receive a request to reserve a room for a speaker to lecture on political issues from either a conservative, liberal, or neutral ideological perspective. I emailed 1,439 administrators. I deleted 73 observations due to undeliverable addresses or automatic replies, leaving 1,366 cases, including 461 in the Conservative Condition, 455 in the Control Condition, and 450 in the Liberal Condition. Statistical power tests show that a two-sample difference of means test is able to detect even small-sized effects (difference = .2) at conventional levels of statistical significance (alpha = .05) between the Liberal and Control Conditions with a level of statistical   22 power at .85. Difference of means test are able to detect small-sized effects at conventional levels of statistical significance between the Conservative and Control Conditions with statistical power of .86. The text of the email message is shown below:  Hello,    Some other students and I were hoping to invite a [liberal political /conservative  political/ political] speaker to deliver a guest lecture on campus. We are  looking forward to this Excellent opportunity to exchange ideas and  information amongst fellow politically-minded denizens of the campus.  We need to secure a room on campus for the lecture to take place.    We expect at least two Dozen students to be in attendance at the lecture,  though this number may increase somewhat if the lecture topics gain  prominence in the news media, a distinct possibility. The maximum  expected number of audience members should be approximately 50.    Please assist us by explaining the appropriate next steps in securing a  campus facility in which to conduct our meeting.    Thank you for your assistance.    Bradley Schwartz  Senior  Results  The most important result is that, for all three experiments, across three dependent variables, there is only one significant difference in the responsiveness of subjects across experimental conditions. Subjects were not significantly more or less likely to reply at all to a student based on the student’s political ideology in terms of either establishing new political student organizations, publishing political editorials in campus newspapers, or securing campus space to host a political guest speaker in the overall samples. Subjects also did not take longer on average to reply to students based on student ideology for any of the three experiments. The sole statistically significant result across all experiments is that administrators were significantly more likely to reply substantively to the student seeking to establish a liberal student group,   23 compared to the student seeking to establish a political group of unspecified political ideology. In this experiment, administrators gave substantive replies to 60% of liberal students, 59% of conservative students, and 53% of students with no specified ideology. The bias thus appears to be more toward specificity (stating some ideology for the proposed group) than a preference for liberal ideology in particular. The mere 1% difference in responsiveness between the Liberal and Conservative Treatment Conditions is neither statistically nor substantively significant.  In the first experiment, in which administrators were randomly assigned to receive a request for assistance in forming a new political student organization, administrators replied to 65% of requests to form a liberal student group, 67% of requests to form a conservative group, and 62% of requests to form a political group of unspecified ideology. The sample size, response rate, and standard errors are shown in Figure 2.1. The mere 2% difference between the Liberal and Conservative Treatment Conditions is neither statistically nor substantively significant. The 5% difference between the Conservative Treatment and Control Condition is substantively large but not statistically significant. It is also in the opposite direction of the bias found in the rest of this dissertation. Both of these facts suggest the difference is due simply to chance and is neither meaningful or indicative of pro-conservative bias. Difference of means tests show that the differences between both treatment and control conditions are not statistically significant in the overall sample. The p-value for the test of statistical significance between the Liberal Condition and the Control Condition is .29 in the overall sample. The p-value for the test of statistical significance between the Conservative Condition and the Control Condition is .13 in the overall sample. The p-value for the test of statistical significance between the Liberal Condition and the Conservative Condition is .64 in the overall sample. These results and those for the other dependent variables and experiments with college administrators are shown in Table 2.1. All   24 differences are substantively quite small, with only three exceptions where differences are substantively moderate in size (5-6%), all in the student group experiment. These differences occur in comparisons of both Treatment Conditions to the Control Condition, where the Treatment Conditions elicited more responsiveness than the Control Condition, and so appear to reflect a preference for specificity rather than one ideology.   Figure 2.1: Student Group Experiment Reply at All Rates with 95% Confidence Intervals Points display the response rate in each condition. Bars display 95% normal-theory confidence intervals.     Figure 2.2 displays the substantive response rate for administrators in the student group experiment across the three experimental conditions. Substantive responses are those that encourage the student or provide the student with next steps to move forward in establishing a new student group. In this experiment, administrators gave substantive replies to 60% of liberal   25 students, 59% of conservative students, and 53% of students with no specified ideology. The 1% difference in responsiveness between Treatment Groups suggests no administrative preference for one ideology over the other. Table 2.1: Administrator Experiments Difference of Means Test Results  Values in the first column display the sample size for the specified test. Values in the second column display the difference of means for the specified test.  95% confidence intervals surrounding these estimates are shown in the third column. The fourth column displays T-Statistics, and the fifth, p-values. P-values of .00 are due to rounding.    26  Figure 2.2: Student Group Experiment Substantive Reply Rates with 95% Confidence Intervals Points display the substantive response rate in each condition. Bars display 95% normal-theory confidence intervals.     Figure 2.3 displays the mean number of days it took subjects in the student group experiment to reply across experimental conditions. The mean number of days to reply was 11.5 in the Liberal Condition, 12.5 in the Control Condition, and 11 in the Conservative Condition. These differences are neither statistically nor substantively significant. The largest difference is between the Liberal Treatment and Control Condition, but this difference of one day is not very large.   27   Figure 2.3: Student Group Experiment Mean Days to Reply with 95% Confidence Intervals Points display the mean days to reply in each condition. Bars display 95% normal-theory confidence intervals.     In the second experiment, involving publishing a political editorial in the campus newspaper, subjects replied to 37% of requests to publish a liberal editorial, 34% of requests to publish a conservative editorial, and 34% of requests to publish an editorial of unspecified ideology. The sample size, response rate, and standard errors are shown in Figure 2.4. These differences are neither statistically nor substantively significant. Conservative students who wish to publish editorials aligning with their political views in campus newspapers have an equal chance as students who do not state political views. The 3% difference between the Liberal   28 Treatment and Conservative Treatment Conditions is not substantively large. The lack of statistical significance and the substantively small difference suggests there is no real bias here.  Figure 2.4: Newspaper Editorial Experiment Reply at All Rates with 95% Confidence Intervals Points display the response rate in each condition. Bars display 95% normal-theory confidence intervals.     Difference of means tests show that the differences between both treatment and control conditions are not statistically significant in the overall sample. The p-value for the test of statistical significance between the Liberal Condition and the Control Condition is .42. The p-value for the test of statistical significance between the Conservative Condition and the Control Condition is .91. The p-value for the test of statistical significance between the Liberal Condition and the Conservative Condition is .48. These p-values are not close to .05 and so do not constitute a near-miss that might become statistically significant if only a larger sample size were   29 available. The lack of statistical significance and the substantively small difference suggests there is no political bias in terms of publishing a politically-oriented editorial in college newspapers at campuses across the country. A conservative student faces no disadvantage in this regard, as is claimed (Blackwell 2018).   Figure 2.5: Newspaper Editorial Experiment Substantive Reply Rates with 95% Confidence Intervals Points display the substantive response rate in each condition. Bars display 95% normal-theory confidence intervals.    Figure 2.5 displays the substantive reply rates for subjects in the newspaper editorial experiment across experimental conditions. This is the percent of subjects who gave encouraging replies or provided helpful information on next steps in getting an editorial published in the campus newspaper. The substantive response rate was 30% in the Liberal Condition, 29% in the   30 Control Condition, and 27% in the Conservative Condition. These differences are neither statistically nor substantively significant. The 1-3% differences across experimental conditions are substantively very small. These substantively small differences and lack of statistical significance suggest there is no meaningful political bias in eliciting substantive replies here.   Figure 2.6: Newspaper Editorial Experiment Mean Days to Reply with 95% Confidence Intervals Points display the mean days to reply in each condition. Bars display 95% normal-theory confidence intervals.    Figure 2.6 displays the mean number of days it took subjects in the newspaper editorial experiment to reply across experimental conditions. On average, subjects took 20 days to reply in the Liberal Condition, 21 days to reply in the Control Condition, and 21 days to reply in the Conservative Condition. These differences are neither statistically nor substantively significant.   31  In the third experiment, involving reserving a space on campus to host a political invited speaker, subjects replied to 54% of requests to host a liberal speaker, 55% of requests to host a conservative speaker, and 57% of requests to host a speaker of unspecified ideology. The sample size, response rate, and standard errors are shown in Figure 2.7.   Figure 2.7: Invited Speaker Space Experiment Reply at All Rates with 95% Confidence Intervals Points display the response rate in each condition. Bars display 95% normal-theory confidence intervals.    Difference of means tests show that the differences between both treatment and control conditions are not statistically significant in the overall sample. The p-value for the test of statistical significance between the Liberal Condition and the Control Condition is .45 in the overall sample. The p-value for the test of statistical significance between the Conservative Condition and the Control Condition is .58 in the overall sample. The p-value for the test of   32 statistical significance between the Liberal Condition and the Conservative Condition is .84 in the overall sample. These differences are neither statistically nor substantively significant. All of the differences are substantively quite small. None of the p-values are close to .05 and so none are considered near-misses that might become statistically significant if only a larger sample size were available. It thus appears there is no meaningful political bias here.   Figure 2.8: Invited Speaker Space Experiment Substantive Reply Rates with 95% Confidence Intervals Points display the substantive response rate in each condition. Bars display 95% normal-theory confidence intervals.     Figure 2.8 displays the substantive reply rates for subjects in the invited speaker space experiment across experimental conditions. This is the percent of subjects who gave encouraging replies or provided helpful information on next steps in getting a politically-oriented speaker a   33 space on campus in which to deliver a guest lecture. The substantive response rate was 42% in the Liberal Condition, 40% in the Control Condition, and 37% in the Conservative Condition. None of these differences were statistically significant. The 5% difference between the Liberal  and Conservative Treatment Conditions may be considered substantively significant and moderately sized. Still, the lack of statistical significance means that we cannot rule out the possibility that this difference is simply due to chance.   Figure 2.9: Invited Speaker Space Experiment Mean Days to Reply with 95% Confidence Intervals Points display the mean days to reply in each condition. Bars display 95% normal-theory confidence intervals.    Figure 2.9 displays the mean number of days it took subjects in the invited speaker space experiment to reply across experimental conditions. On average, subjects took 15 days to reply in   34 the Liberal Condition, 14 days to reply in the Control Condition, and 15 days to reply in the Conservative Condition. These differences are neither statistically nor substantively significant.  Discussion  The primary takeaway from this chapter is that there is little significant political bias in requests to form new political student organizations, publish political editorials in campus newspapers, or secure rooms for political guest speakers. Across three experiments with three dependent variables each, only one test reveals a statistically significant difference across experimental conditions: administrators in the student group experiment were significantly more likely to give substantive replies to the student requesting to form a liberal student group than the student requesting to form a group of unspecified ideology. The apparent bias shown here, however, appears to have more to do with administrative preferences for specificity in plans for a new student group than it does with preferences for liberal ideology in particular. Administrators in this experiment gave substantive replies to 60% of liberal students, 59% of conservative students, and 53% of control students. Administrators tasked with assisting students form new student groups thus appear to prefer students with more well-developed, specific plans for a new group, rather than groups of any particular political ideology. Across the rest of the experiments, as well as the other two dependent variables for the student group experiment, the null results reveal no significant political bias on the part of university administrators.  This is good news. Political bias of the kind investigated here would conflict with the history of universities as havens of political activism and norms of academic freedom. Such bias would also limit opportunities for the development of social capital among conservative college students and may exacerbate the polarized decline in trust in institutions of higher education.    35  The tendency to favor the in-group and derogate the out-group spelled out in social identity theory (Tajfel 1979) does not seem to prevail here. If anything, there is an instance of overcorrection, such as that observed by Goldberg (2019) among white liberals. Whether both mechanisms are active and essential cancel each other out in the overall samples, or whether professional norms prevail and there is simply no bias in the first place is unclear. What is clear, however, is that there is no significant political bias in the overall samples in all three of these experiments. At least with respect to the tasks studied here, university administrators are not significantly biased against conservative students, as is widely claimed.   The results described in this chapter suggest that, among this sample of university administrators, professionalism and professional norms win the day. This is good news for any person or group concerned about political bias in America’s universities. Like their counterparts in other public bureaucracies, university administrators act as if driven by professionalism and professional norms. The professional norms that specifically prohibit discrimination based on political ideology apparently outweigh any inclination or impulse to favor the in-group or to derogate the out-group in terms of shared political views. It is remarkable that even though liberals outnumber conservatives by an astounding ratio of 12 to 1 among higher-level university administrators (Abrams 2018), there is no significant preferential treatment for liberal students over conservative students in administrative distribution of university resources. University administrators are an extremely liberal group, but this is inconsequential for how conservative students are treated relative to liberal students when they request university resources, such as campus space for an invited speaker to lecture on political issues, official university recognition for a new political student organization, or the ability to publish political editorials in the campus newspaper.   36 CHAPTER 3  POLITICAL BIAS AMONG SOCIAL SCIENCE PROFESSORS  In this chapter, I describe and present the results of an experiment designed to gauge the presence and extent of political bias among university professors in social science fields. The social sciences are concerned with questions of human behavior. Social scientists use the scientific method to research areas including politics, economics, culture, society, and psychology. Social science disciplines include political science, economics, anthropology, sociology, and psychology.  In this study, I email department chairs and directors of undergraduate study at departments of political science, economics, sociology, and social science, as well as mixed departments containing at least one of those disciplines in the department name. These subjects are randomly assigned to receive a request for more information from a prospective student about a proposed research topic in which the hypothesis aligns with liberal or conservative ideology, or is unstated. By comparing response rates across experimental conditions, we will be able to gauge the presence and extent of political bias among social science professors.  Theory is an inconsistent guide in predicting whether these subjects will exhibit political bias. Social identity theory predicts subjects will favor their own in-group and derogate the out-group, which, for the vast majority of college professors in America, means favoring liberals and derogating conservatives. Rothman, Lichter, and Nevitte (2005) survey American college faculty on their political views and find that 81% of political scientists, 55% of economists, and 77% of sociologists say they are liberal. Only 2% of political scientists, 39% of economists, and 9% of sociologists say they are conservative (Rothman, Lichter, and Nevitte 2005). These same authors   37 find that, overall, 75% of professors whom the authors classify as social scientists identify as liberal, compared to only 9% who say they are conservative.  As college faculty, however, subjects may be well aware of this ideological lopsidedness, as well as the widespread criticism of their alleged anti-conservative bias. It is quite possible, then, that these subjects may in fact overcorrect for this supposed bias, by exhibiting a bias in favor of conservative students or against liberal students. Goldberg (2019) finds that white liberals actually display significant pro-outgroup bias. Since academia is very white (National Center for Education Statistics 2017) and very liberal (Rothman, Lichter, and Nevitte 2005), professionals in this field may be especially prone to pro-outgroup bias in terms of political ideology. In this case, pro-outgroup bias in terms of political ideology would mean significant pro-conservative bias, given academia’s extremely liberal tilt.  Alternatively, this experiment may find no evidence of significant bias. This might occur if the two effects described above cancel each other out. Such a situation may also obtain if there is no bias in the first place. Professional norms discourage political bias of the kind investigated here as unethical (AAUP 2018).  Methodology  The methodology utilized in the experiments described here is the correspondence experiment, described more generally in Chapter 1. I email professors in social science fields at colleges across America seeking more information about proposed research projects. Subjects are randomly assigned to receive a request from a politically conservative, liberal, or neutral sender. Subjects are also randomly assigned to receive a request from a sender with a high or low ACT score. The high ACT score is 28, at the 75th percentile. The low score is 24, at the 25th percentile. There are thus 6 conditions in this 3x2 design: Liberal-High ACT Score,   38 Conservative-High ACT Score, Control-High ACT Score, Liberal-Low ACT Score, Conservative-Low ACT Score, Control-Low ACT Score. The ACT scores were added as a proxy for academic aptitude to try to hold that factor constant. Without an objective measure of academic aptitude, subjects might make their own inferences about the sender’s aptitude based on their message and reply differentially across conditions based on those perceptions rather than any political bias.  A comparison of the response rates across experimental conditions will reveal the presence and extent of political bias among social science professors. I analyze the data by first displaying the response rates in each condition and in various sub-samples (e.g. among professors at liberal arts colleges) along with the standard errors of these response rates. I also present the results of difference of means tests, OLS regressions, and predicted probabilities of response.   In this experiment, I emailed 2,006 social science professors. I deleted 10 observations due to undeliverable addresses or automatic replies, leaving 1996 cases, including 334 in the Conservative-Low ACT Condition, 329 in the Control-Low ACT Condition, 334 in the Liberal-Low ACT Condition, 333 in the Conservative-High ACT Condition, 334 in the Control-High ACT Condition, and 332 in the Liberal-High ACT Condition. Statistical power tests show that a two-sample difference of means test is able to detect even small-sized effects (difference = .2) at conventional levels of statistical significance (alpha = .05) with a level of statistical power at .73, using the lowest sample size comparisons (332 and 329).   I first identify the department chair or director of undergraduate study at every department of political science/politics/government/political studies, sociology, economics, social science or department with one of those disciplines in the name at every college listed by   39 US News and World Report as a National College, National Liberal Arts College, Regional College, or Regional University. I record the director of undergraduate study if this position exists and the department chair if it does not. I identify these subjects via appropriate search terms, e.g. “[university name] department of political science director of undergraduate study.” I make use of Google and the universities’ own search engines, as well as any information on university or department website or in publicly available academic CVs. I also record the size (enrollment), endowment, setting (urban, suburban, or rural), geographical region, school type (e.g. regional or liberal arts college), and whether the school is religious (whether or not the school mentions religious terms such as “God” or “faith” in its mission statement).  Then, I randomly assign subjects to one of the six conditions. I conduct balance tests to ensure the expectation of equivalence across conditions is met (see Appendix H). These subjects receive a request from a prospective student for more information about a potential research project on a classic concept in economic, political science, and sociological research: the welfare trap. The welfare trap is the idea that receiving welfare discourages employment.  The topic of the email (the welfare trap) is an appropriate one because it is a legitimate academic topic of study, with many peer-reviewed papers in economics (e.g., Plant 1984; Sawhill 1988; Hansen and Lofstrom 2009), political science (e.g., Morgan and Kickham 1999; Daigneault 2015), and sociology (e.g., Schram, Turbett, and Wilken 1988; Cooke 2009).4 This  4 There are numerous other studies in all three social science disciplines concerning research on the topic of the welfare trap, ensuring that results across disciplines are not driven by the choice of research project. The welfare trap is a suitable topic for research in economics (Freeman 1995; Wunder and Riphahn 2014; Zimmerman and Levine 1993; Barrett and McCarthy 2008; Blundell 2001), political science (Beamer 1998; Pixley 1995; Gardner 1994; Goodin 2001; Aslund 1998), and sociology (Pearson 2007; Contini and Negri 2007; Harris 1993; Woodward 2008; Kelly 2010). In addition, as seen in Figures 3.4 and 3.5, the reply at all and substantive reply response rates are not very different across disciplines, as we would expect if the topic was better suited to certain disciplines and less suitable to others.    40 topic is also a good one for this study because liberals and conservatives take clear, opposite positions on it (Vinik 2014). A Pew Research Center poll found that, among “steadfast conservatives,” 86% say “poor people have it easy because they can get government benefits without doing anything,” while 86% of “solid liberals” took the opposite stance, saying “poor people have hard lives because government benefits don’t go far enough to help them live decently” (Vink 2014). While social issues have risen in importance, economic issues such as this one are still more strongly correlated than social issues with partisanship and presidential vote choice (Bartels 2006).  Subjects are randomly assigned to receive a message in which the hypothesis takes a liberal or conservative view on the welfare trap or is unstated. The liberal view is that welfare benefits do not pay enough to live decently. The conservative view is that people would rather not work than work to receive income.  Subjects are given one month from the initial request to respond. The first dependent variable in this study is a binary indicator of whether the subject did or did not reply within 30 days, following previous, similar research, which notes the value of this measure is that it is objective (Butler and Broockman 2011 467). The second dependent variable is whether the administrator gave a substantive reply within 30 days (1) or not (0). Replies were counted as substantive if they encouraged the student to form the proposed group or laid out the next steps necessary to do so.   The third dependent variable is the number of days it took subjects to reply. Non-replies and replies beyond the 30-day response window were coded as 31. These data on the number of days to reply are analyzed with the difference of means tests presented in the results section, as well as Cox Proportional-Hazards models for right-censoring, and two-step Heckman selection models. Cox Proportional-Hazards models are commonly used in analyses of medical data, in   41 which researchers seek to know the probability (or “hazard”) that a given case in the dataset experiences a particular event at a particular time of observation. For example, events may include death or relapse. Here, the event of concern is whether a subject replied. In the two-step models, the first model is a probit selection model, taking the reply at all variable as the dependent variable and treatment indicators along with enrollment as predictors. The second model takes number of days as the dependent variable and treatment indicators as predictors. The results from the first model are used to construct a variable, the inverse Mills ratio, that captures the selection effect in the second model. I include enrollment as a predictor in the first model (the selection model) to act as an instrument (Sundstrom 2016). The Cox Proportional-Hazards models may be interpreted by the value and significance of the hazard ratio for predictor variables. A hazard ratio below 1 suggests a smaller risk, while a hazard ratio above 1 suggests a higher risk. For example, a hazard ratio of .45 for the Conservative Treatment indicator would suggest a lower “risk” of receiving a reply, relative to the Control Condition. The two-stage selection models may be interpreted with reference to the sign and significance of coefficients for predictor variables: a positive significant coefficient for the Conservative Treatment indicator, for example, would suggest it took subjects longer to reply to a conservative student relative to a student who did not state his political ideology.  The emails were sent January 6-10, 2020 and the response window closed 30 days later, on February 5-9, 2020. Given the 500 email per day limit policy of Gmail, the emails had to be sent across several days. The order in which the emails were sent was randomized with pure random assignment to obviate any potential effects of chronological order.  I measure the independent variables, other than the treatment indicators, by relying on data from US News and World Report’s 2019 rankings of American colleges and universities. In   42 all models, I include indicators of experimental condition. I use the US News and World Report categorization to record college setting, either urban, suburban, or rural. I use suburban as the reference category in all models with setting. I use US News and World Report data to measure school endowment, ranking, and enrollment.5 I use US News and World Report to classify school region as either North, South, Midwest, or West. In some comparisons to follow, I group all schools in the North, Midwest, and West as schools outside the South, and only schools classified as in the South by US News and World Report as Southern schools.6 For all schools, I measure school religiosity by reading the school mission statement. A school’s religiosity is coded 1 if its mission statement has any mention of God, Jesus, faith, or similar faith-based vocabulary and 0 otherwise. I rely on the US News and World Report categorizations to determine a school’s type (either National University, National Liberal Arts College, Regional University, or Regional College).  If difference of means tests indicate that professors are significantly more responsive to a liberal student than to student of unspecified ideology, this result will constitute evidence of pro-liberal bias. If professors are significantly less responsive to a conservative student than a student of unspecified ideology, this result will constitute evidence of anti-conservative bias. Alternatively, there could be a pro-conservative bias (if professors are significantly more responsive to the conservative student than to the student with no specified ideology) or an anti-liberal bias (if professors are significantly less responsive to the liberal student than to the student with no specified ideology). Pro-conservative or anti-liberal biases would suggest that university professors, cognizant of the allegations that they are discriminatory, have  5 See Footnote 3. 6 See Footnote 4.   43 overcorrected to the point that they privilege conservative students or disadvantage liberals. If there is no significant difference between the treatment and control groups, but there is a significant difference between the two treatment groups, such a finding would still indicate some bias, although it would be unclear where that bias originates.  The text of the email request appears below. Relying on the bonus exercise my students completed described in Chapter 2, the email is written at the 12th grade writing level. I chose the pseudonym Logan Haas, since it has been empirically demonstrated to elicit a high response rate (Butler and Homola 2017).  Dear Dr. [professor name],   I am a second-year community college student considering a transfer to your university  next fall. I would eventually like to pursue a PhD in a social science field.   I have taken the ACT and scored [24/28], but I plan to retake the test and study to  improve my score.    I am very interested in scholarship about the "welfare trap". I plan to write my senior  thesis on this topic, and interview a sample of mothers receiving some form of  government assistance, such as WIC, food stamps, and/or TANF.    [Liberal Conditions: I expect to find that welfare receipt does not generally discourage  efforts to find gainful employment among these women. Welfare in America is generally  not sufficient to cover basic needs, so I do not expect welfare receipt to discourage  employment.]    [Conservative Conditions: I expect to find that welfare receipt generally discourages  efforts to find gainful employment among these women. Most people would prefer not to  work than to work to receive income, so I expect welfare receipt discourages working.]   I am considering pursuing this and other research topics in your department. Are you or  anyone else in your department interested in research on the welfare trap? If so, please  send me more information about the type of research projects that may be possible.   Thank you for your time,    Logan Haas   The email requests come from a Gmail account (loganhass2022@gmail.com), not   44 an official student email. Many colleges and universities have a policy that students should use their official student email account to contact faculty and administrators. A policy of this kind would explain some unresponsiveness, but it would not explain a differential responsiveness across the experimental conditions. It is that difference that is of interest to us here. In other words, random assignment ensures that universities with such a policy have the same chance of being in any of the six experimental conditions. Any difference in the way liberals and conservatives are treated cannot be blamed on a student email policy.  I include three measures of responsiveness: the first measure is whether subjects reply at all within 30 days (1) or not (0). The second measure is whether subjects give a substantive reply within 30 days (1) or not (0). A subject giving a substantive reply either expresses interest in working with the student, contacts, CCs or suggests others in the department who may be interested, refers to relevant classes or events in the department in which the student may be interested related to the student’s research interests, encourages the student to visit campus or apply to the university, or suggests meeting in person or talking on the telephone. The third measure of responsiveness is the number of days it takes subjects to reply, ranging from 0-31, with non-replies and late (beyond 30 days) replies coded as 31. Results  The results of this experiment reveal significant bias against conservative students on all three measures of responsiveness in both the overall sample as well as the sub-sample of political scientists. Figure 3.1 displays the response rate of subjects defined as replying at all within 30 days. While the response rates are similar across the Low ACT Conditions, the response rate for the conservative student with a high ACT score is lower than that for a liberal student with the same score. For students with a 28 ACT score, social science professors replied to the liberal   45 student 53% of the time, but only 46% of the time to a conservative student. This relationship narrowly misses statistical significance at p = .07. This near-miss in terms of statistical significance, however, is substantively significant, with a difference of 7% across experimental conditions. For comparison, Butler and Broockman (2011) report that legislators significantly racially discriminate against constituents based on a 5% difference. The difference of means test between the Conservative, High ACT and Control, High ACT Conditions, however, shows statistically significant anti-conservative bias (p-value =.02). Professors are significantly more likely to reply to a student who does not express any political view than a student who expresses a conservative view with the same (high) ACT score. Substantively, social science professors replied to 54% of requests from a high-performing student who does not specify ideology, but only 46% of requests from equally high-performing conservatives.   Figure 3.1: Social Science Professors Experiment Overall Reply at All Rates with 95% Confidence Intervals Points display the response rate in each condition. Bars display 95% normal-theory confidence intervals.     46  Figure 3.2 displays the substantive response rate of subjects in this experiment. Again we see response rates are similar across the Low ACT Conditions, but that, among students with high ACT scores, the conservative student faces a disadvantage. Subjects in this experiment provided a substantive reply to 45% of inquiries from a liberal student with a 28 ACT score, and 42% of inquiries from a student with no stated ideology, but only 34% of inquiries from a conservative with the same score. The p-value for the difference of means test of comparison between the Control, High ACT and Conservative, High ACT Conditions is .04. The p-value for the difference of means test of comparison between the Liberal, High ACT and Conservative, High ACT Conditions is .00. These differences are substantively significant: social scientists are 11% more likely to reply substantively to a high-performing liberal student than an equally high-performing conservative student. Social scientists are 8% more likely to reply to a student with no specified ideology than a conservative with the same high ACT score.    Figure 3.2: Social Science Professors Experiment Overall Substantive Reply Rates with 95% Confidence Intervals Points display the substantive response rate in each condition, i.e. the percent of all emails sent to valid addresses that received substantive, helpful, or encouraging replies. Bars display 95% normal-theory confidence intervals.     47  In Figure 3.3 we see the third measure of responsiveness across conditions, the mean number of days subjects took to reply student requests. Similar to the previous two figures, the conservative student with the high ACT score faces a significant disadvantage. Subjects in this experiment took an average of 17.44 days to reply to the conservative student with a high ACT, but only 15.48 and 14.8 days to reply to the liberal and control student with the same ACT score, respectively.  Subjects took nearly three days longer to reply to the conservative student compared to the liberal student. Three days represents a significant delay in responsiveness. The professors in this experiment may thus be trying to “wait out” students in hopes they will go away or forget about the email. This dilatory tactic may be another manifestation of anti-conservative bias more subtle than simply ignoring the email altogether.   These differences are substantively as well as statistically significant. The difference of means between the Conservative, High ACT and Control, High ACT Conditions is statistically significant (p-value= .02). Table I.12 in Appendix I presents the results of a two-stage selection model of the number of days it took professors in the overall social science sample to reply. Neither treatment indicator’s coefficient is statistically significant in this model.   Table I.11 in Appendix I presents the results of a Cox Proportional-Hazards model of response for the overall social science sample, which confirms that conservatives are significantly less likely to receive a reply at all. The results of these two models taken together suggest that the apparent significantly longer timeline for conservatives to receive a reply may be due to the fact that conservatives are less likely to receive a reply in the first place. Since non-replies were coded as the highest value on the number of days variable (31), the addition of all of these high values may drive up the mean number of days it took subjects to reply to the conservative student.    48   Figure 3.3: Social Science Professors Experiment Overall Mean Days to Reply with 95% Confidence Intervals Points display the mean number of days subjects took to reply in each condition. Bars display 95% normal-theory confidence intervals.    Table 3.1 displays the results of difference of means tests for various comparisons within this experiment. As noted above, there are several statistically significant results, all of which pertain to conservative students with high ACT scores. Such students are significantly less likely to receive any reply at all within 30 days relative to a student who does not express any kind of political view, significantly less likely to receive a substantive/encouraging reply than students who do not state a political view as well as students who state liberal views, and wait significantly longer to receive a reply relative to students who do not state political views. These differences are substantively significant as well.   The p-value for the difference of means comparison on the reply at all variable between the Conservative, High ACT Condition and the Control, High ACT Condition is .02. Substantively, social scientists were 9% more likely to reply in the Control Condition than in the Conservative Treatment Condition, a large bias. The p-value for the difference of means   49 comparison on the substantive reply variable between the Conservative, High ACT Condition and the Control, High ACT Condition in terms of substantive replies is .04. The difference of 8% is also substantively significant. The p-value for the difference of means comparison on the substantive reply variable between the Conservative, High ACT Condition and the Liberal, High ACT Condition is .00. Substantively, social scientists were 11% more likely to reply to a liberal than a conservative among high-performing students. The p-value for the difference of means comparison on the number of days to reply variable between the Conservative, High ACT Condition and the Control, High ACT Condition is .02. Social scientists took 2.63 days longer to reply to conservatives than students with no stated ideology, a substantively significant bias.   Two results in Table 3.1 are near-misses in terms of statistical significance. Subjects were 7% more likely to reply at all to a liberal student than a conservative student, put the comparison narrowly misses statistical significance (p-value = .07). Another near-miss occurs in the test of comparison between the Liberal, High ACT and Conservative, High ACT Conditions on the days to reply variable (p-value = .09). Subjects took nearly 2 days longer to reply to a high-performing conservative student than a high-performing liberal student.  The consistent direction of all statistically and substantively significant results in the overall sample of social scientists suggests that the bias revealed here is not a “fluke”, and is not dependent on any particular method of measuring responsiveness. The bias is consistently in an anti-conservative direction. Conservative students face significant disadvantages in attempting to work with social scientists, regardless of which measure of responsiveness is utilized. The fact that there is no significant anti-liberal or pro-conservative bias suggests that social scientists have clearly taken a side: against conservative prospective students. The consistency of the direction of bias suggests a genuine, meaningful pattern rather than a statistical fluke.   50 Table 3.1: Social Science Professors Experiment Overall Difference of Means Test Results   Values in the first column display the sample size for the specified test. Values in the second column display the difference of means for the specified test. Tests are shown for all departments in the experiment. P-values of .00 are due to rounding.    Figure 3.4 displays the rate of response for any response at all within 30 days. The top left pane displays the results for the discipline of political science, top right shows sociology, bottom left shows economics, and bottom right shows departments with “social science” in the name. The only discipline with significant anti-conservative bias on this measure is political science. Political scientists replied to 61% of requests from a liberal student with a high ACT score, but only 46% of requests from a conservative student with the same ACT score. This is a very large, substantively significant difference of 15%. As shown in Table 3.2, this difference is statistically significant (p-value= .02), as is the difference between the Conservative, High ACT and the Control, High ACT Conditions (p-value= .02). In fact, political scientists were more responsive to liberal students with a low ACT score than they were to conservative students with a high ACT score. Political scientists replied to 52% of requests from a low-ACT liberal, but only 46% of requests from a high-ACT conservative. This difference of 6% is substantively   51 significant. Political scientists would thus rather work with a low-performing liberal student than a high-performing conservative student. Sociologists and economists do not display a similar bias. The differences across experimental conditions in those subgroups are neither statistically nor substantively significant. The social science results thus appear driven by political science.   Figure 3.4: Social Science Professors Experiment Reply at All Rates by Discipline with 95% Confidence Intervals Points display the response rate for any reply at all within 30 days. Bars display 95% normal-theory confidence intervals. Each pane displays the results for the specified discipline. The lower right pane displays results for departments with “social science” in the name, not the overall results for all subjects.    Figure 3.5 below displays the rate of substantive response. This is the percent of all emails receiving a helpful or encouraging response. See the methods section of this chapter for what counts as a substantive response. The top left pane displays the results for the discipline of political science, top right shows sociology, bottom left shows economics, and bottom right shows departments with “social science” in the name. Again, the only discipline with significant   52 anti-conservative bias on this measure is political science. Conservative students with a high ACT score are significantly less likely to receive a substantive reply than liberal students with the same score from political scientists (p-value= .02). Political scientists in this experiment gave substantive replies to 52% of liberal students with high ACT scores, but only 36% of conservatives with the same score. This is a substantively large difference of 16%. Political scientists are in fact more likely to give substantive replies to low-scoring liberals (44%) than to high-scoring conservatives (36%), confirming the previous result that political scientists would rather work with an low-performing liberal than a high-performing conservative.    Figure 3.5: Social Science Professors Experiment Substantive Reply Rates by Discipline with 95% Confidence Intervals Points display the response rate for substantive replies. Bars display 95% normal-theory confidence intervals. Each pane displays the results for the specified discipline. The lower right pane displays results for departments with “social science” in the name, not the overall results for all subjects.     53  Figure 3.6 displays the mean number of days it took subjects to reply across experimental conditions, with each pane representing a discipline. Political science is shown in the top left, sociology in the top right, economics in the bottom left, and departments with “social science” in the title are in the bottom right. Continuing the pattern, only political scientists display statistically significant anti-conservative bias. Political scientists take significantly longer to reply to conservative students with high ACT scores than liberal students or students with unspecified political views with the same score. Indeed, political scientists are quicker to respond to liberals with low ACT scores (mean 15.36 days) than high-scoring conservatives (mean 17.45 days). This is a substantively significant difference of over two days. There is no similar bias from sociologists or economists on this measure. Again, the significant results of the overall social science sample thus appear to be driven by political scientists. Economists and sociologists do not take significantly longer to reply to students based on their ideology. The differences across experimental conditions in those subsamples are neither statistically nor substantively significant.  This last result continues the pattern of preference among political scientists for low-performing liberal students over high-performing conservative students. However, as shown in Appendix I in Table I.24, which displays the results of a two-stage selection model of the number of days to reply for political scientists, neither treatment indicator’s coefficient is statistically significant. Table I.23, which displays the results of a Cox Proportional-Hazards model for right-censoring among the political scientist sample, confirms that being conservative makes the probability of receiving any reply at all from a political scientist less likely. These two model results taken together suggest that the higher mean number of days to receive a reply from   54 a political scientist for a conservative student may simply be due to the fact that conservatives were less likely to receive any reply at all.   Figure 3.6: Social Science Professors Experiment Mean Days to Reply by Discipline with 95% Confidence Intervals Points display the mean number of days to reply in each condition. Bars display 95% normal-theory confidence intervals. Each pane displays the results for the specified discipline. The lower right pane displays results for departments with “social science” in the name, not the overall results for all subjects.    Table 3.2 displays the results of difference of means tests across conditions and all three measures of responsiveness for the sub-sample of political scientists. Political scientists display statistically and substantively significant anti-conservative bias on all three measures of responsiveness. They are significantly more likely to reply to a liberal and a student with unspecified political views than a conservative student with the same (28) ACT score (p-value=.02 for both comparisons). These differences are substantively large as well (16% and   55 15% respectively). These differences are three times as large as the racial bias of legislators against constituents found by Butler and Broockman (2011). Political scientists are significantly more likely to give a substantive reply to a high-scoring liberal than a conservative with the same score (p-value= .02), another substantively large difference of 15%. Finally, political scientists take significantly longer (nearly five days) to reply to a high-scoring conservative than a liberal or to student with unspecified political views with the same score (p-value=.02 for both comparisons).  Table 3.2: Social Science Professors Experiment Political Science Difference of Means Test Results    Values in the first column display the sample size for the specified test. Values in the second column display the difference of means for the specified test. Tests are shown for Political Science departments. P-values of .00 are due to rounding.    Table 3.3 displays the results of difference of means tests for comparisons between conditions within the sub-sample of sociologists. None of these differences are statistically or substantively significant. In addition, there are no near-misses in terms of statistical significance. All p-values are well above .05. This means there is no significant political bias here among   56 sociologists on any of the three measures of responsiveness. The p-values for all difference of means tests in this subsample were above .05. Table 3.3: Social Science Professors Experiment Sociology Difference of Means Test Results   Values in the first column display the sample size for the specified test. Values in the second column display the difference of means for the specified test. Tests are shown for Sociology departments. P-values of .00 are due to rounding.    Table 3.4 displays the results of difference of means tests for comparisons between conditions within the sub-sample of economists. None of these differences are statistically or substantively significant. There are two near-misses in terms of statistical significance (p-values above .05 but below .10), but substantively, these differences are very small. This means there is no significant political bias here among economists on any measure of responsiveness. Economists do not exhibit statistically significant political biases in terms of either replying at all, giving substantive replies, or the timeliness of their replies. There are several substantively significant results, however. Economists are 12% less likely to reply to a liberal and 13% less likely to reply to a conservative compared to a student who states no political views. The   57 preference thus appears to be for an unbiased or apolitical student rather than a student who espouses any particular political ideology. Table 3.4: Social Science Professors Experiment Economics Difference of Means Test Results   Values in the first column display the sample size for the specified test. Values in the second column display the difference of means for the specified test. Tests are shown for Economics departments. P-values of .00 are due to rounding.    The results thus far have been presented for high and low-scoring students separately. The theoretical reason for this is that the treatments may evoke perceptions other than those that were intended (Dafoe, Zhang, and Caughey 2018). In this case, subjects might view the email from the conservative student as indicative of less academic ability. In order to ensure that any difference in subject responsiveness was due primarily to political ideology rather than academic ability, I included ACT scores to control for academic ability. The inclusion of high or low ACT scores also allows for comparisons between such groups, for example, the remarkable finding in the sample of political scientists that these subjects would rather work with an underperforming liberal than a high-performing conservative. Nevertheless, it is worthwhile to present the results for all ACT groups together. Table 3.5 presents these results for the reply at all variable. Table 3.6 presents these results for the substantive reply variable. Table 3.7 presents these results for   58 the number of days to reply variable. The purpose of these analyses is to get an overall view of political bias, leaving the indicator of intelligence aside.  Table 3.5: Social Science Professors Experiment Difference of Means Test Results, Combined ACT Groups: Reply at All   Values in the first column display the sample size for the specified test. Values in the second column display the difference of means for the specified test. Values in the third column display the 95% normal theory confidence intervals. Values in the fourth column display the T-statistics and the fifth column displays p-values. P-values of .00 are due to rounding.    Table 3.5 shows two statistically significant results at the .05 level and a further substantively significant result that does not meet the criteria for statistical significance. In the overall sample, social science professors were significantly more likely to reply in the Control Condition relative to the Conservative Treatment Condition. This difference of 5% is statistically significant with a p-value of .05. In the sample of political scientists, subjects were also significantly more likely in the Control Condition relative to the Conservative Treatment Condition. This difference of 11% is substantively large and statistically significant. A further result which is substantively but not statistically significant is the comparison between the Liberal and Conservative Treatment Conditions in the political science sample. Political scientists were 8% more likely to reply in the Liberal Treatment Condition than in the   59 Conservative Treatment Condition, which is substantively large but statistically insignificant. If the sample size were larger, this difference would be statistically significant. For comparison, Butler and Broockman (2011) reported that legislators were significantly racially biased against constituents based on a 5% difference in responsiveness across experimental conditions.  I repeated the combined ACT analyses for the substantive reply and days to reply variables. Table 3.6 displays the results of these difference of means tests for the substantive reply variable. While there are no statistically significant results, there is one near-miss, with a p-value of .06 due to rounding. This value comes from the difference of means comparison in the overall sample comparing the Liberal and Conservative Treatment Conditions. Subjects in this experiment were 5% more likely to reply substantively to a liberal than a conservative student. Table 3.6: Social Science Professors Experiment Difference of Means Test Results, Combined ACT Groups: Substantive Reply   Values in the first column display the sample size for the specified test. Values in the second column display the difference of means for the specified test. Values in the third column display the 95% normal theory confidence intervals. Values in the fourth column display the T-statistics and the fifth column displays p-values. P-values of .00 are due to rounding.    Table 3.7 displays the results of difference of means tests for the combined ACT samples on the days to reply variable. There is one statistically significant result and two near-misses.   60 The only statistically significant result is the comparison between the Conservative Treatment and Control Conditions in the sample of political scientists. Political scientists were significantly more likely to reply in the Control Condition than in the Conservative Treatment Condition (p-value = .01). The difference is substantively large: political scientists took nearly three and a half days longer to reply in the Conservative Treatment Condition than in the Control Condition. One near-miss in terms of statistical significance occurs in the comparison of the Conservative Treatment and Control Conditions in the overall sample (p-value = .06). Overall, social scientists took 1.56 days longer to reply in the Conservative Treatment Condition than in the Control Condition. Another near-miss occurs in the political science sample in the comparison of the Liberal and Conservative Treatment Conditions (p-value - .08). Substantively, political scientists took 2.38 days longer to reply to a conservative student than a liberal student. This delay of 2.38 days represents a considerable delay. It could be that political scientists are trying to “wait out” conservative students in hopes they simply forget their request and go away. Table 3.7: Social Science Professors Experiment Difference of Means Test Results, Combined ACT Groups: Days to Reply   Values in the first column display the sample size for the specified test. Values in the second column display the difference of means for the specified test. Values in the third column display the 95% normal theory confidence intervals. Values in the fourth column display the T-statistics and the fifth column displays p-values. P-values of .00 are due to rounding.     61 Discussion  The results of this experiment reveal significant bias against conservative students on all three measures of responsiveness in both the overall sample as well as the sub-sample of political scientists. Sociologists and economists do not display similar bias, suggesting the overall results of bias are driven mainly by political scientists. The statistically significant and substantively large nature of political scientists’ anti-conservative bias is striking. With the inclusion of ACT scores, we are able to see that political scientists prefer to work with low-performing liberals over high-performing conservatives, measured by the rate of replying at all, substantive replies, and the timeliness of replies.   The significant bias of political scientists against conservative students suggests that the professionalism and professional norms of university administrators does not apply to this sub-sample of professors. Sociologists and economists, however, abide by professional norms of not discriminating against students on the basis of political views. In this regard, then, sociologists and economists are more similar to university administrators than they are to political scientists.   Another reason we may find anti-conservative bias among political scientists but not among sociologists or economists is the subject matter these professors teach and research. The political content of political scientists’ work appears to lend itself to politicized preferences in which prospective students to mentor. Economists and sociologists may touch on political ideas and themes in their work, but they are not the main focus. Perhaps this strictly political focus of political scientists’ work facilitates political bias.  A further reason political scientists, but not sociologists or economists, discriminate against conservative prospective students may be the more left-leaning views of these professors. 81% of political science professors self-identify as liberal, compared to 77% of sociologists, and   62 only 55% of economists (Rothman, Lichter, and Nevitte 2006). The overwhelming leftist orientation of political scientists likely leads them to discriminate against the conservative out-group, in accordance with social identity theory. The question remains, however, how sociologists and even economists are not prone to similar bias as these disciplines are also populated largely by liberals, albeit not as much as political science.  Professionalism and professional norms of non-discrimination may play a role in helping social science professors—in particular, political scientists—in ameliorating the current situation of significant anti-conservative discrimination. Bureaucratic preferences may be shaped through sufficient training, developing and maintaining professional norms, and the recruitment of people who are competent in their administrative roles (Brehm and Gates 1997, 202; Vinzant and Crothers 1998, 155–56). Such professional training, development, and recruitment might bring awareness that political bias against prospective students is an issue, and initiate some first steps in resolving it. Perhaps professors who take on the roles of department chair or undergraduate coordinator/director currently receive insufficient training in these administrative roles and that lack of training explains the current state of political bias. Full-time administrators, such as those who help students establish new university-recognized organizations, likely have received more administrative training than professors who take on administrative roles secondary to their main functions as researchers and teachers.     63 CHAPTER 4  POLITICAL BIAS AMONG HUMANITIES PROFESSORS  In this chapter, I describe and present the results of an experiment designed to gauge the presence and extent of political bias among university professors in humanities fields. The humanities are concerned with the human experience. Humanities disciplines include English and other languages as well as composition, literature, philosophy, art, gender and race studies, religion, and art. These disciplines are important to include in a study of political bias in academia because of the highly liberal tilt of professors in these fields (Rothman, Lichter, and Nevitte 2005), as well as complaints of political bias in these fields in particular (Kimball 1990).  In this study, I email department chairs and directors of undergraduate study at departments of English, gender studies, philosophy, and humanities, as well as mixed departments containing at least one of those disciplines in the department name. These subjects are randomly assigned to receive a request for more information from a prospective student about a proposed research topic in which the hypothesis aligns with liberal or conservative ideology, or is unstated. By comparing response rates across experimental conditions, we will be able to gauge the presence and extent of political bias among humanities professors.  When it comes to predicting whether humanities professors will exhibit political bias, theory is an inconsistent guide. Social identity theory predicts subjects will favor their own in-group and derogate the out-group, which, for the vast majority of college professors in America, means favoring liberals and derogating conservatives. Rothman, Lichter, and Nevitte (2005) survey American college faculty on their political views and find that 81% of professors of English literature and 80% of philosophers say they are liberal. Only 3% of professors of English literature and 5% of philosophers say they are conservative (Rothman, Lichter, and Nevitte   64 2005). These same authors find that, overall, 81% of humanities professors say they are liberal, compared to only 9% who say they are conservative.  Subjects in this experiment may be well aware of this ideological lopsidedness, as well as the widespread criticism of their alleged anti-conservative bias. Perhaps this awareness drives these subjects to actually overcorrect for this supposed bias, by exhibiting a bias in favor of conservative students or against liberal students. A further reason humanities professors might exhibit a pro-conservative bias has to do with who they are. College faculty are largely white (National Center for Education Statistics 2017) and liberal (Rothman, Lichter, and Nevitte 2005). Goldberg (2019) finds that white liberals actually exhibit a pro-outgroup bias, which contradicts the common tendency for people to favor their in-group over an out-group. In other words, the subjects in this experiment may exhibit a pro-outgroup (i.e., pro-conservative) bias because they are largely white and liberal, the type of group now known to exhibit pro-outgroup bias.   Alternatively, this experiment may find no evidence of significant bias. This might occur if the two effects described above cancel each other out. Such a situation may also obtain if there is no bias in the first place. Professional norms discourage political bias of the kind investigated here as unethical (AAUP 2018). Methodology  The methodology utilized in the experiments described here is the correspondence experiment, described more generally in Chapter 1. I email professors in humanities fields at colleges across America seeking more information about proposed research projects. Subjects are randomly assigned to receive a request from a politically conservative, liberal, or neutral sender. Subjects are also randomly assigned to receive a request from a sender with a high or low ACT score. The high ACT score is 28, at the 75th percentile. The low score is 24, at the 25th   65 percentile. There are thus 6 conditions in this 3x2 design: Liberal-High ACT Score, Conservative-High ACT Score, Control-High ACT Score, Liberal-Low ACT Score, Conservative-Low ACT Score, Control-Low ACT Score. The ACT scores were added as a proxy for academic aptitude to try to hold that factor constant. Without an objective measure of academic aptitude, subjects might make their own inferences about the sender’s aptitude based on their message and reply differentially across conditions based on those perceptions rather than any political bias.  A comparison of the response rates across experimental conditions will reveal the presence and extent of political bias among humanities professors. I analyze the data by first displaying the response rates in each condition and in various sub-samples (e.g. among professors at liberal arts colleges) along with the standard errors of these response rates. I also present the results of difference of means tests, OLS regressions, and predicted probabilities of response.   In this experiment, I emailed 1,550 humanities professors. I deleted 8 observations due to undeliverable addresses or automatic replies, leaving 1542 cases, including 255 in the Conservative-Low ACT Condition, 258 in the Control-Low ACT Condition, 257 in the Liberal-Low ACT Condition, 257 in the Conservative-High ACT Condition, 258 in the Control-High ACT Condition, and 257 in the Liberal-High ACT Condition. Statistical power tests show that a two-sample difference of means test is able to detect even small-sized effects (difference = .2) at conventional levels of statistical significance (alpha = .05) with a level of statistical power at .62.  I first identify the department chair or director of undergraduate study at every department of English, women’s/gender/sexuality studies, philosophy, or humanities as well as departments with one of those disciplines in the name at every college listed by US News and   66 World Report as a National College, National Liberal Arts College, Regional College, or Regional University. I record the director of undergraduate study if this position exists and the department chair if it does not. I identify these subjects via appropriate search terms, e.g. “[university name] department of English director of undergraduate study.” I make use of Google and the universities’ own search engines, as well as any information on university or department website or in publicly available academic CVs. I also record the size (enrollment), endowment, setting (urban, suburban, or rural), geographical region, school type (e.g. regional or liberal arts college), and whether the school is religious (whether or not the school mentions religious terms such as “God” or “faith” in its mission statement).  Then, I randomly assign subjects to one of the six conditions. I conduct balance tests to ensure the expectation of equivalence across conditions is met (see Appendix J). These subjects receive a request from a prospective student for more information about a potential research project on a topic of interest in English (e.g. Kemp 2019), gender studies (e.g. Stryker, Currah, and Moore 2008), and philosophical research (e.g. Murphy 2015): transgenderism.7 Transgender people identify with a different gender than the biological sex they were assigned at birth.  The topic of the email (transgender issues) is an appropriate one because it is a legitimate academic topic of study in humanities fields. This topic is also a good one for this study because liberals and conservatives take clear, opposite positions on it (Brown 2017). A 2017 Pew Research Center survey shows an overwhelming majority (80%) of Republicans believe gender  7 There are numerous other studies in all three humanities disciplines concerning research on the topic of transgenderism, ensuring that results across disciplines are not driven by the choice of research project. Transgenderism is a suitable topic for research in English (Crawford 2015; Hatfield 2019; Bychowski 2017; Zimman 2009; Thein 2013), philosophy (Walker 2005; Bianchi 2017; Calhoun 2007; Namaste 2009; Robson 2007), and gender studies (Abbott 2009; Keegan 2013; Hines 2006; Broad 2002; Roen 2001). In addition, as seen in Figures 4.4 and 4.5, the reply at all and substantive reply response rates are not very different across disciplines, as we would expect if the topic was better suited to certain disciplines and less suitable to others.   67 is determined at birth (Brown 2017). A majority of Democrats (64%) take the opposite view, saying that gender can be different from one’s sex at birth (Brown 2017). Subjects are randomly assigned to receive a message in which the hypothesis takes a liberal or conservative view on laws about respecting transgender individuals’ pronouns or is unstated. The liberal view is that laws that require citizens to refer to transgender individuals by their preferred pronouns are good in that they advance the perennial concern of liberals, equality. The conservative view is that such laws are concerning because they pose a threat to free speech.  Subjects are given one month from the initial request to respond. The first dependent variable in this study is a binary indicator of whether the subject did or did not reply within 30 days, following previous, similar research, which notes the value of this measure is that it is objective (Butler and Broockman 2011 467). The second dependent variable is whether the administrator gave a substantive reply within 30 days (1) or not (0). Replies were counted as substantive if they encouraged the student to form the proposed group or laid out the next steps necessary to do so.   The third dependent variable is the number of days it took subjects to reply. Non-replies and replies beyond the 30-day response window were coded as 31. These data on the number of days to reply are analyzed with the difference of means tests presented in the results section, as well as Cox Proportional-Hazards models for right-censoring, and two-step Heckman selection models. Cox Proportional-Hazards models are commonly used in analyses of medical data, in which researchers seek to know the probability (or “hazard”) that a given case in the dataset experiences a particular event at a particular time of observation. For example, events may include death or relapse. Here, the event of concern is whether a subject replied. In the two-step models, the first model is a probit selection model, taking the reply at all variable as the   68 dependent variable and treatment indicators along with enrollment as predictors. The second model takes number of days as the dependent variable and treatment indicators as predictors. The results from the first model are used to construct a variable, the inverse Mills ratio, that captures the selection effect in the second model. I include enrollment as a predictor in the first model (the selection model) to act as an instrument (Sundstrom 2016). The Cox Proportional-Hazards models may be interpreted by the value and significance of the hazard ratio for predictor variables. A hazard ratio below 1 suggests a smaller risk, while a hazard ratio above 1 suggests a higher risk. For example, a hazard ratio of .45 for the Conservative Treatment indicator would suggest a lower “risk” of receiving a reply, relative to the Control Condition. The two-stage selection models may be interpreted with reference to the sign and significance of coefficients for predictor variables: a positive significant coefficient for the Conservative Treatment indicator, for example, would suggest it took subjects longer to reply to a conservative student relative to a student who did not state his political ideology.  The emails were sent January 6-10, 2020 and the response window closed 30 days later, in February 5-9, 2020. Given the 500 email per day limit policy of Gmail, the emails had to be sent across several days. The order in which the emails were sent was randomized with pure random assignment to obviate any potential effects of chronological order.  I measure the independent variables, other than the treatment indicators, by relying on data from US News and World Report’s 2019 rankings of American colleges and universities. In all models, I include indicators of experimental condition. I use the US News and World Report categorization to record college setting, either urban, suburban, or rural. I use suburban as the reference category in all models with setting. I use US News and World Report data to measure   69 school endowment, ranking, and enrollment.8 I use US News and World Report to classify school region as either North, South, Midwest, or West. In some comparisons to follow, I group all schools in the North, Midwest, and West as schools outside the South, and only schools classified as in the South by US News and World Report as Southern schools.9 For all schools, I measure school religiosity by reading the school mission statement. A school’s religiosity is coded 1 if its mission statement has any mention of God, Jesus, faith, or similar faith-based vocabulary and 0 otherwise. I rely on the US News and World Report categorizations to determine a school’s type (either National University, National Liberal Arts College, Regional University, or Regional College).  If difference of means tests indicate that professors are significantly more responsive to a liberal student than to student of unspecified ideology, this result will constitute evidence of pro-liberal bias. If professors are significantly less responsive to a conservative student than a student of unspecified ideology, this result will constitute evidence of anti-conservative bias. Alternatively, there could be a pro-conservative bias (if professors are significantly more responsive to the conservative student than to the student with no specified ideology) or an anti-liberal bias (if professors are significantly less responsive to the liberal student than to the student with no specified ideology). Pro-conservative or anti-liberal biases would suggest that university professors, cognizant of the allegations that they are discriminatory, have overcorrected to the point that they privilege conservative students or disadvantage liberals. If there is no significant difference between the treatment and control groups, but there is a  8 See Footnote 3. 9 See Footnote 4.   70 significant difference between the two treatment groups, such a finding would still indicate some bias, although it would be unclear where that bias originates.  The text of the email request appears below. Relying on the bonus exercise my students completed described in Chapter 2, the email is at the 12th grade level of writing. I chose the pseudonym Kristen Clark, since it has been empirically demonstrated to elicit a high response rate (Butler and Homola 2017).  Dear Dr. [professor name],   I am a second-year community college student considering a transfer to your university  next fall. I would eventually like to pursue a PhD in a humanities field.   I have taken the ACT and scored [24/28], but I plan to retake the test and study to  improve my score.    My research interests lie in issues and rules surrounding transgender people and their  preferred pronouns. For example, New York City recently passed a law requiring citizens  to use others' preferred gender pronouns.    [Conservative Conditions: I am concerned this law stifles free speech as it would penalize  people for saying what they believe.]   [Liberal Conditions: I view this law as a solid step toward equality for all people  regardless of gender.]   I am interested in studying the experiences of students on college campuses as colleges  wrestle with similar issues and rules. Are you or anyone in your department interested in  similar research?   Thank you for your time,   Kristen Clark   The email requests come from a Gmail account (kristenclark2022@gmail.com), not an official student email. Many colleges and universities have a policy that students should use their official student email account to contact faculty and administrators. A policy of this kind would explain some unresponsiveness, but it would not explain a differential responsiveness   71 across the experimental conditions. It is that difference that is of interest to us here. In other words, random assignment ensures that universities with such a policy have the same chance of being in any of the six experimental conditions. Any difference in the way liberals and conservatives are treated cannot be blamed on a student email policy.  I include three measures of responsiveness: the first measure is whether subjects reply at all within 30 days (1) or not (0). The second measure is whether subjects give a substantive reply within 30 days (1) or not (0). A subject giving a substantive reply either expresses interest in working with the student, contacts, CCs or suggests others in the department who may be interested, refers to relevant classes or events in the department in which the student may be interested related to the student’s research interests, encourages the student to visit campus or apply to the university, or suggests meeting in person or talking on the telephone. The third measure of responsiveness is the number of days it takes subjects to reply, ranging from 0-31, with non-replies and late (beyond 30 days) replies coded as 31. Results  Like social scientists, professors in humanities disciplines display significant anti-conservative bias on all three measures of responsiveness in the overall sample. Humanities professors are significantly less likely to reply to a conservative student with a 24 ACT score than to liberal students or students with no specified political views with the same score. The subjects in this experiment are also significantly less likely to give substantive replies to low-scoring conservatives, and take significantly longer to reply, relative to liberals and students with unspecified views with the same score. These significance tests are shown in Table 4.1.   Unlike the social science experiment, where all significant differences take place at the high ACT scores, all the significant differences in this experiment are with low ACT scores. The   72 p-value for the difference of means for the reply at all variable between the Liberal, Low ACT Treatment Condition and the Conservative, Low ACT Condition is .01. This difference is also substantively significant: humanities professors were 12% more likely to reply to the liberal than the conservative student with the same (low) ACT score. The p-value for the difference of means for the substantive reply variable between the Control, Low ACT Treatment Condition and the Conservative, Low ACT Condition is .03. This difference is substantively significant as well, with subjects replying to 51% of requests in the Control, Low ACT Condition, but only 42% of requests in the Conservative, Low ACT Treatment Condition. The p-value for the difference of means for the substantive reply variable between the Liberal, Low ACT Treatment Condition and the Conservative, Low ACT Condition is .00. Subjects gave substantive replies to 41% of low-scoring liberal students, but only 28% of low-scoring conservative students. The p-value for the difference of means for the days to reply variable between the Control, Low ACT Treatment Condition and the Conservative, Low ACT Condition is .03. This difference is substantively large, with subjects replying to 37% of requests from a student with no stated ideology, but only 28% of conservative students with equally low ACT scores. The p-value for the difference of means for the days to reply variable between the Liberal, Low ACT Treatment Condition and the Conservative, Low ACT Condition is .00. Substantively, humanities professors took four days longer on average to reply to conservatives than liberals with the same low ACT score. There are no near-misses in this table and no results that are substantively but not statistically significant.  An important note is that the philosophers in this experiment realized they were participating in an experiment. I received a reply from a subject which read: “Discussions on FB have made it clear that different iterations of this email went to Philosophy Chairs across the country. Is this some sort of study?” The results of this experiment should thus be taken with a   73 grain of salt, particularly results from the sub-sample of philosophers and results which suggest a social desirability bias. Subjects who realize they are in an experiment are prone to social desirability bias, in which they give socially appropriate instead of truthful answers. In this case, the socially appropriate response is no political bias, and especially no anti-conservative bias, since this form of bias is so widely and vehemently alleged of professors in the media. Results that demonstrate anti-conservative discrimination even in the face of this social desirability bias suggest philosophers are even more biased against conservative students than is measured here.10 Table 4.1: Humanities Professors Experiment Overall Difference of Means Test Results   Values in the first column display the sample size for the specified test. Values in the second column display the difference of means for the specified test. Tests are shown for all departments in the experiment. P-values of .00 are due to rounding.   10 In any case, I re-ran the analyses for the overall humanities sample on all three measures of responsiveness, excluding philosophy departments altogether. The results were remarkably consistent: liberal students were always more likely to receive any reply at all, a substantive reply, and more timely replies than students who did not state political views. Students who did not state political views were always more likely to receive any reply at all, a substantive reply, and more timely replies than conservatives. Thus the consistent pro-liberal/anti-conservative bias observed in the humanities sample does not depend on data from philosophy departments. There was only one statistically significant result (the difference of means comparison between the liberal and conservative condition on the substantive reply variable), which is to be expected given the reduced sample size. Nevertheless, humanities professors are consistently biased in favor of liberals and against conservatives even without philosophers included in the sample.   74  Figure 4.1 displays the response rate in terms of replying at all in the overall humanities sample. For students with low (24) ACT scores, subjects replied to 54% of requests from a liberal, 42% of requests from a conservative, and 51% of requests from a student with unspecified political views. These results reflect a substantively large bias against conservatives, more than twice the size of the racial bias of legislators against constituents found in Butler and Broockman (2011). Two of these differences are statistically significant, as shown in Table 4.1 and described above: the difference of means between the Conservative, Low ACT Condition and Control, Low ACT Condition (p-value = .03) as well as the difference of means between the Liberal, Low ACT, and Conservative, Low ACT Conditions (p-value = .01). There are no significant differences across High ACT Conditions in this experiment. These significant differences mean that liberal students as well as students who do not specify a political ideology are significantly more likely to receive a reply from humanities professors than conservative students, when all of these students have the same (low) ACT scores.   Figure 4.1: Humanities Professors Experiment Overall Reply at All Rates with 95% Confidence Intervals Points display the response rate in each condition. Bars display 95% normal-theory confidence intervals.    Figure 4.2 displays the substantive response rate in the overall humanities sample. For low-ACT students, subjects gave substantive replies to 41% of liberals, 28% of conservatives,   75 and 37% of students with no specified political views. Again, we see a substantively large anti-conservative bias. Two of these differences are statistically significant: the difference of means between the Liberal, Low ACT and Control, Low ACT Conditions (p-value = .03), as well as the difference between the Liberal, Low ACT and Conservative, Low ACT Conditions (p-value = .00). This means that, among students with low ACT scores, both liberal students as well as students who do not reveal their political ideology are significantly more likely to receive a substantive reply from humanities professors, compared to conservative students. These results constitute evidence of significant anti-conservative bias. Conservative students are significantly less likely to receive a substantive reply from professors in the humanities than liberals or students who hide their political views, even when they have the same (low) ACT score.   Figure 4.2: Humanities Professors Experiment Overall Substantive Reply Rates with 95% Confidence Intervals Points display the substantive response rate in each condition. Bars display 95% normal-theory confidence intervals.      Figure 4.3 displays the mean number of days subjects took to reply. Again, among low-scoring students, conservatives face a statistically and substantively significant disadvantage. Among these low-scoring students, subjects took 14.88 days on average to reply to the liberal student, 18.9 days to reply to the conservative, and 16.08 days to reply to the control student. The p-value for the difference of means comparison between the Liberal, Low ACT Condition and   76 the Conservative, Low ACT Condition is .00, which is due to rounding. The p-value for the difference of means comparison between the Control, Low ACT Condition and the Conservative, Low ACT Condition is .03. There are no significant differences in responsiveness across the high-scoring conditions.   Among low-scoring students, liberals and students with no specified ideology receive significantly more timely replies than conservative students from humanities professors. This represents a significant delay in responsiveness. The professors in this experiment may thus be trying to “wait out” students in hopes they will go away or forget about the email. This dilatory tactic may be another manifestation of anti-conservative bias more subtle than simply ignoring the email altogether. However, these results may be driven by the coding of all non-replies and late replies (beyond the 30-day response window) as 31. Appendix K, Table K.6 presents the results of a two-stage selection model of the number of days to reply for professors in the overall humanities sample. In the second stage model, neither treatment indicator variable’s coefficient is statistically significant, which suggests no significant difference in the number of days it took humanities professors to reply to liberal or conservative students, relative to the control group.   The significant differences shown in Figure 4.3 may thus reflect only the tendency of these professors to give any reply at all significantly more often to liberals and the control group, compared to conservatives. Table K.5 in Appendix K presents the results of a Cox Proportional-Hazards model of responses in the overall humanities sample and confirms that being in the Conservative Treatment Condition decreases the probability of receiving a reply at all. Conservative students thus face significant discrimination in receiving any reply at all from humanities professors, but likely not in terms of the timeliness of those replies.     77   Figure 4.3: Humanities Professors Experiment Overall Mean Days to Reply with 95% Confidence Intervals Points display the mean days subjects took to reply in each condition. Bars display 95% normal-theory confidence intervals.     Figure 4.4 displays the rate at which subjects replied at all within 30 days across the six experimental conditions with a pane for each discipline. There are no significant results here except for philosophy departments in the low-ACT conditions. Philosophers replied to 60% of requests from a low-scoring liberal student, 42% of requests from a low-scoring conservative student, and 61% of requests from a low-scoring student who did not specify any political views. These results constitute a substantively large anti-conservative bias. This bias (21-22%) is over four times as large as the racial bias of legislators against constituents (Butler and Broockman 2011). The p-value of the difference of means comparison between the Conservative, Low ACT and the Control, Low ACT Conditions in this subsample of philosophers is .01. The p-value of the difference of means comparison between the Conservative, Low ACT and the Liberal, Low ACT Conditions in this subsample of philosophers is also .01. This substantively and statistically significant bias of philosophers against conservative students is all the more striking given that some of the philosophers knew they were participating in a study and should have been biased toward the opposite, more socially appropriate, responses.   78    Figure 4.4: Humanities Professors Experiment Reply at All Rates by Discipline with 95% Confidence Intervals Points display the response rate for any reply at all within 30 days. Bars display 95% normal-theory confidence intervals. Each pane displays the results for the specified discipline. The lower right pane displays results for departments with “humanities” in the name, not the overall results for all subjects.      Figure 4.5 displays the substantive response rate across experimental conditions with each pane representing an academic discipline. The only statistically significant difference here is among professors of gender studies who were significantly less likely to reply to a low-scoring conservative student than to a student with the same score who does not specify any political views (p-value = .02). This statistical significance is all the more striking given the very low sample size (n=188). The difference is substantively very large: professors of gender studies gave substantive replies to 60% of requests from a low-scoring student who does not specify any political views, but only 17% of requests from a conservative student with the same ACT score. This incredibly large, 43% difference is over eight times as large as the 5% racial bias of legislators against constituents (Butler and Broockman 2011). Professors of gender studies thus   79 strongly prefer students who do not specify their ideology over conservatives with the same (low) ACT score. This result does not appear to be caused simply by a preference for more well-developed plans, as does the only significant result reported in Chapter 2. Gender studies professors gave substantive replies to 41% of requests from low-scoring liberals, substantially more than the 17% of requests from low-scoring conservatives. While this comparison between low-scoring liberals and conservatives among gender studies professors is not statistically significant, it is substantively large.    Figure 4.5: Humanities Professors Experiment Substantive Reply Rates by Discipline with 95% Confidence Intervals Points display the response rate for substantive replies. Bars display 95% normal-theory confidence intervals. Each pane displays the results for the specified discipline. The lower right pane displays results for departments with “humanities” in the name, not the overall results for all subjects.     Figure 4.6 displays the mean number of days subjects took to reply across experimental conditions with each pane representing an academic discipline. The only statistically significant results here are among philosophers in the Low ACT Conditions, specifically for the difference of means tests comparing the Conservative, Low ACT Condition and the Control, Low ACT   80 Condition (p-value = .01) as well as the comparison between the Liberal, Low ACT Condition and the Conservative, Low ACT Condition (p-value= .00). Philosophers took an average of 12.36 days to reply to the low-scoring liberal, 18.87 days to reply to the low-scoring conservative, and 13.42 days to reply to the low-scoring student who did not specify any political views. These are substantively significant differences in the direction of anti-conservative bias. Five or six days represents a significant delay in responsiveness. The professors in this experiment may thus be trying to “wait out” students in hopes they will go away or forget about the email. This dilatory tactic may be another manifestation of anti-conservative bias more subtle than simply ignoring the email altogether.  Table K.30 in Appendix K presents the results of a two-stage selection model of the number of days it took philosophers to reply to emails in the low-scoring conditions. As neither treatment indicator variable is statistically significant, the significant difference of means among philosophers in the number of days to reply may be due to philosophers’ tendency to reply at all significantly more often to liberal and apolitical students, relative to conservatives. In other words, the mean number of days to reply to conservatives may be significantly higher than that for liberals or apolitical students, because non-replies were coded as the highest value (31) and philosophers were significantly less likely to reply at all to conservatives, thus adding more values of 31 to the days to reply variable column. Table K.29 in Appendix K presents the results of a Cox Proportional-Hazards model of responses among philosophers, and confirms that being in the Conservative Treatment Condition decreases the probability of receiving a reply at all. The significantly more timely responses of philosophers for liberal and apolitical students relative to conservatives may thus simply reflect the fact that philosophers were more likely to reply to the liberal and apolitical students in the first place.   81  Figure 4.6: Humanities Professors Experiment Mean Days to Reply by Discipline with 95% Confidence Intervals Points display the mean number of days to reply in each condition. Bars display 95% normal-theory confidence intervals. Each pane displays the results for the specified discipline. The lower right pane displays results for departments with “humanities” in the name, not the overall results for all subjects.    Table 4.2 displays the results of difference of means tests for comparisons between conditions within the sub-sample of English professors. None of these differences are significant. This means there is no significant political bias here among professors of English on any of the three measures of responsiveness in these analyses, but when high and low ACT scores are combined, English professors display anti-conservative biases. The p-values for all of these comparisons are above .05. This means that English professors do not exhibit any statistically significant political bias in terms of either replying at all, giving a substantive reply, or the timeliness of their replies. There are several substantively significant results, however. English professors are 9% less likely to reply to a high-scoring conservative than a student who does not   82 specify any ideology with the same high ACT score. English professors are 11% more likely to reply to high-scoring liberals than high-scoring conservatives, 8% more likely to reply to a high-scoring liberal than a high-scoring student who does not specify political views, and 7% more likely to reply to a low-scoring liberal than a low-scoring conservative.   In terms of giving substantive replies, English professors are 6% more likely to reply to high scoring liberals than high-scoring students with no stated political views, 10% more likely to reply to high-scoring liberals than high-scoring conservatives, 8% more likely to reply to low-scoring liberals than low-scoring students with no stated political views and 11% more likely to reply to low-scoring liberals than low-scoring conservatives.   English professors also took 2.27 days longer to reply to a high-scoring conservative than a high-scoring student with no stated political views, 2.65 days longer to reply to a high-scoring conservative than a high-scoring liberal, 2.66 days longer to reply to a low-scoring liberal than a low-scoring student with no stated political views, and 2.3 days longer to reply to a low-scoring conservative than a low-scoring liberal.   These differences are substantively large, though not statistically significant. One of the results is a near-miss in terms of statistical significance. The p-value for the difference of means comparison for the Liberal and Conservative, Low ACT Treatment Conditions is .07. Thus while English professors do not exhibit statistically significant political bias in this analysis, they do display a consistent, substantively significant political bias in the pro-liberal/anti-conservative direction. Furthermore, in Tables 4.5, 4.6, and 4.7, which display the results of analyses in which High and Low ACT Conditions are combined, the only discipline with statistically significant bias is English.     83 Table 4.2: Humanities Professors Experiment English Difference of Means Test Results   Values in the first column display the sample size for the specified test. Values in the second column display the difference of means for the specified test. Tests are shown for English departments. P-values of .00 are due to rounding.    Table 4.3 displays the results of difference of means tests for comparisons between conditions within the sub-sample of philosophy professors. Philosophers display significant anti-conservative bias on two of the three measures of responsiveness. For low-scoring students, philosophers are significantly more likely to reply to liberals and students who do not specify their political views than conservatives, and take significantly less time to reply. The p-value for the difference of means comparison between liberal and conservative students on the reply at all variable for philosophers is .01. This difference is substantively very large, with philosophers 19% more likely to reply to a low-scoring liberal than a low-scoring conservative. The p-value for the difference of means comparison between conservative students and the control group on the reply at all variable for philosophers is also .01. This difference is also substantively very large: philosophers were 18% more likely to reply to a low-scoring student who does not state any political views than an equally low-scoring conservative. The p-value for the difference of means comparison between liberal and conservative students on the days to reply variable for   84 philosophers is .00. The p-value for the difference of means comparison between conservative students and the control group on the days to reply variable for philosophers is .01.   These differences are substantively very large: among low-scoring students, philosophers took 5.45 days longer to reply to the conservative than a student with no stated political views and 6.51 days longer to reply to a conservative than a liberal. Five to six days represents a significant delay in responsiveness. The professors in this experiment may thus be trying to “wait out” students in hopes they will go away or forget about the email. This dilatory tactic may be another manifestation of anti-conservative bias more subtle than simply ignoring the email altogether.   As noted above, Tables K.29 and K.30 in Appendix K suggest the significant political bias of philosophers in terms of the number of days to reply may be a result of philosophers’ significant political bias in terms of replying at all. Regardless, the fact that philosophers are significantly more likely to reply to emails from liberal students and students who do not specify their ideology than conservative students demonstrates a significant anti-conservative bias on the part of philosophers.   Additionally, there are numerous instances of near-misses (p-values above .05 but below .10) and substantively but not statistically significant results. For example, philosophers were 12% more likely to reply to a high-scoring liberal than an equally high-scoring conservative, but the p-value for the difference of means comparison misses the threshold for statistical significance at .08. These results suggest a strong anti-conservative/pro-liberal bias among philosophers. These findings are all the more striking given the knowledge of some philosophers that they were participating in an experiment. Subjects who have knowledge that they are participating in an experiment should be prone to give the socially desirable response of no bias.   85 Table 4.3: Humanities Professors Experiment Philosophy Difference of Means Test Results   Values in the first column display the sample size for the specified test. Values in the second column display the difference of means for the specified test. Tests are shown for Philosophy departments. P-values of .00 are due to rounding.    Table 4.4 displays the results of difference of means tests for comparisons between conditions within the sub-sample of gender studies professors. Professors of gender studies display significant anti-conservative bias on the substantive reply measure. These professors are significantly more likely to give a substantive reply to a low-scoring student who does not express any political views than a low-scoring conservative (p-value= .02). This difference is substantively very large, with a first difference of .43. Gender studies professors thus appear to exhibit a statistically significant and substantively large bias against conservative students. Liberal students and those who do not reveal their political views will have a significantly easier time eliciting substantive replies from professors of gender studies. Additionally, many other results in this subsample of gender studies professors are substantively very large but do not rise to the level of statistical significance given the small sample size. For example, gender studies professors are 33% more likely to reply to a low-scoring conservative student than an equally   86 low-scoring student with no stated political views. This is an incredibly large anti-conservative bias. The other substantively significant results are also in the direction of pro-liberal/anti-conservative bias, suggesting a genuine pattern, rather than a statistical fluke.  Table 4.4: Humanities Professors Experiment Gender Studies Difference of Means Test Results   Values in the first column display the sample size for the specified test. Values in the second column display the difference of means for the specified test. Tests are shown for Gender Studies departments. P-values of .00 are due to rounding.     The results thus far have been presented for high and low-scoring students separately. The theoretical reason for this is that the treatments may evoke perceptions other than those that were intended (Dafoe, Zhang, and Caughey 2018). In this case, subjects might view the email from the conservative student as indicative of less academic ability. In order to ensure that any difference in subject responsiveness was due primarily to political ideology rather than academic ability, I included ACT scores to control for academic ability. The inclusion of high or low ACT scores also allows for comparisons between such groups, for example, the remarkable finding in the sample of political scientists that these subjects would rather work with an underperforming   87 liberal than a high-performing conservative. Nevertheless, it is worthwhile to present the results for all ACT groups together. Table 4.5 presents these results for the reply at all variable.  Table 4.5: Humanities Professors Experiment Difference of Means Test Results, Reply at All Variable, High and Low ACT Conditions Combined   Values in the first column display the sample size for the specified test. Values in the second column display the difference of means for the specified test. Values in the third column display the 95% normal theory confidence intervals. Values in the fourth column display the T-statistics and the fifth column displays p-values. P-values of .00 are due to rounding.    Table 4.5 displays only one statistically significant result but two near-misses that are also substantively significant. English professors are significantly more likely to reply at all to a liberal student than to a conservative student (p-value = .05). This difference is also substantively significant, with English professors 9% more likely to reply to the liberal than the conservative. In the overall sample, humanities professors are 6% more likely to reply to a liberal student than a conservative, but the test narrowly misses statistical significance (p-value = .06). Another result that is substantively but not quite statistically significant occurs in the overall sample in the test of comparison between the Conservative Treatment and Control Conditions. Humanities professors were 5% more likely to reply in the Control Condition than in the Conservative Treatment Condition (p-value = .08).   88  Table 4.6 displays the results for difference of means tests for all ACT groups combined on the substantive reply variable. There are two statistically significant results and several results that are substantively but not statistically significant. In the overall sample, humanities professors were 6% more likely to reply to a liberal than a conservative student (p-value = .04). The other statistically significant result occurs in the sample of English professors, who were 10% more likely to reply to a liberal than a conservative student (p-value = .02).   Several other results were substantively significant and would likely be statistically significant as well if the sample sizes were increased. Humanities professors in the overall sample were 5% more likely to reply in the Control Condition than in the Conservative Treatment Condition. English professors were 7% more likely to reply in the Liberal Treatment Condition than in the Control Condition. Philosophers were 6% more likely to reply in the Control Condition than in the Conservative Treatment Condition.   All three tests of comparison among gender studies professors were substantively large. Gender studies professors were 11% more likely to reply to a liberal than a student who did not specify her ideology, 16% less likely to reply to a conservative student than a student who did not specify her ideology, and 5% more likely to reply to a liberal than to a conservative.   These tests, particularly among gender studies professors, were underpowered but the biases were quite large, so the tests would have reached the threshold for statistical significance had the sample sizes been larger. The remarkably large anti-conservative bias among gender studies professors (16%) is over three times the size of the racial bias of legislators against constituents found by Butler and Broockman (2011), for example. The consistent pro-liberal/anti-conservative direction of bias suggests the results represent a meaningful, genuine pattern of political bias rather than a statistical fluke.    89 Table 4.6: Humanities Professors Experiment Difference of Means Test Results, Substantive Reply Variable, High and Low ACT Conditions Combined   Values in the first column display the sample size for the specified test. Values in the second column display the difference of means for the specified test. Values in the third column display the 95% normal theory confidence intervals. Values in the fourth column display the T-statistics and the fifth column displays p-values. P-values of .00 are due to rounding.    Table 4.7 presents the results of the difference of means tests for the number of days it took subjects to reply, combining low and high ACT groups. There is one statistically significant result, along with two near-misses in terms of statistical significance that are substantively large. In the overall sample, humanities professors took 1.81 days longer to reply to a conservative than a liberal (p-value = .05). Humanities professors in the overall sample also took 1.59 days longer to reply to a conservative student than a student with no specified ideology, a substantively large result that does not quite meet the threshold for statistical significance (p-value = .09). Another near miss occurs among English professors, who took 2.46 days longer to reply to conservative students than to liberal students (p-value = .07). It is interesting that in the analyses with high and low ACT scores combined, English professors appear to exhibit the most statistically significant political biases, whereas in the analyses where low and high ACT scores are separated, this is the province of gender studies and philosophy professors. Nonetheless, the consistent direction of   90 bias against conservatives/in favor of liberals suggests that humanities professors are actually biased in this manner, and that the results are not due to statistical chance.   Table 4.7: Humanities Professors Experiment Difference of Means Test Results, Days to Reply Variable, High and Low ACT Conditions Combined   Values in the first column display the sample size for the specified test. Values in the second column display the difference of means for the specified test. Values in the third column display the 95% normal theory confidence intervals. Values in the fourth column display the T-statistics and the fifth column displays p-values. P-values of .00 are due to rounding.  Discussion   This chapter has shown evidence of anti-conservative bias in the humanities across three different measures of responsiveness in the overall sample, as well as bias in two of the three sub-samples. Professors of humanities are significantly less likely to reply, give substantive replies, and are less timely in their replies to conservative students relative to both liberals and students who do not specify their political views.   Similar to social scientists and dissimilar to university administrators, faculty in humanities fields are biased against conservative prospective students. These findings suggest the professional norms that guide university administrators away from political discrimination are not functional among faculty. Faculty do in fact discriminate against politically conservative students in direct violation of the norms established by the American Association of University Professors (AAUP 2018). The solution is unclear, but highlighting the problem seems a prudent first step.    91  The sub-sample results of bias in the disciplines of philosophy and gender studies suggest conservative students face a disadvantage in these disciplines. English professors did not display significant anti-conservative bias, suggesting that field is not as difficult for conservative students to navigate as philosophy or gender studies. Why this might be the case is unclear. Rothman, Lichter, and Nevitte (2006) survey academics on their political views, and, while they do not survey gender studies professors, find that 88% of English Literature professors and 80% of philosophy professors self-identify as liberal. Overall, these authors find 81% of professors in humanities disciplines identify as liberal. It is curious that English professors are more liberal, yet philosophy professors are more politically biased against conservatives. Future work might examine what it is about the content or standard practices in research or teaching across various disciplines make them more or less prone to political bias.   Like their counterparts in social science, professors in the humanities might exhibit bias due to a lack of professional training, clear norms, and recruitment of competent individuals to serve in administrative roles. Such measures can shape the preferences of bureaucrats (Brehm and Gates 1997 202; Vinzant and Crothers 1998 155-156). A plausible reason why we might find political bias in the professoriate, but not among university administrators, is that administrators receive adequate training and guidance in professional norms, while professors do not. In addition, professionalism and adherence to professional norms may play a larger role in the recruitment of university administrators than professors, who are recruited primarily for their skills in research and teaching. Perhaps with such measures, professors in the humanities--particularly gender studies and philosophy—might discriminate less against conservative students. If the guidance from the American Association of University Professors against political discrimination is to be taken seriously, professors in both social sciences as well as the   92 humanities must change their behavior (AAUP 2018). The fields of political science, philosophy, and gender studies are particularly problematic.   These experiments provoke several intriguing questions that get to the heart of what the purpose of a college education and academic research ought to be. The social sciences typically purport to view the purpose of research to be the advancement of knowledge through empirical research, not the advancement of normative views. If this is the purpose of research, the political bias uncovered in these experiments suggests a failure to adhere to this view. A dispassionate, unbiased researcher should not discriminate against prospective students on the basis of political disagreements. This discrimination suggests that the view of academic research as an unbiased quest for knowledge does not reflect the reality of biased responses to prospective students on the basis of their political views.  If liberals and students who hide their political views have an easier time advancing in the academy than conservatives, perhaps the whole academic enterprise suffers from a lack of genuine intellectual diversity. Concerns for free speech and welfare dependence are summarily dismissed in favor of concerns for equality and more money for poor people, as demonstrated in the two experiments with professors. The dismissal of concerns that align with a conservative perspective suggests that academic research in social science and the humanities might be more focused on advancing personal political opinions than the unbiased generation of knowledge. Is this the purpose of academic research and/or a college education? Does academic research as a whole neglect projects or findings that may upset the apple cart by aligning with a conservative perspective? These questions provoked by the findings of significant anti-conservative bias in political science, philosophy, and gender studies are key to defining what academia and college are supposed to be and do. Conservative skepticism of higher education will likely only grow   93 with the knowledge that professors in these fields discriminate against conservative prospective students. These findings and their consequences represent a prime opportunity for changing the way academia functions. We can dismiss those who disagree with our personal political views, or we can welcome intellectual diversity and tolerate differences in political views as a necessary feature of that diversity.      94 CHAPTER 5  CONCLUSION   Across three experiments with university administrators, I find no significant bias against conservative students. This finding suggests that perhaps anti-conservative bias and overcorrection effects have canceled each other out or that professional norms have prevailed such that administrators treat student requests without regard to political ideology. In the two experiments with professors, I find significant anti-conservative bias in some disciplines, namely political science, philosophy, and gender studies. In this conclusion chapter, I underscore the broader significance of these studies and claims of political bias on campus in terms of the real-world politics of America’s universities, as well as political science theory.  America’s universities adhere to norms of academic freedom for both students and professors (AAUP 2018). These norms have been recognized by the American Association of University Professors for over 100 years, as they were enshrined by AAUP in a 1915 statement: “[a]cademic freedom has traditionally had two applications—to the freedom of the teacher and to that of the student” (AAUP 2018). Ideological discrimination of the kind investigated here would be in direct contradiction of these norms. Conservative students should not have a harder time trying to establish new student groups, publish editorials in campus newspapers, invite speakers to lecture on campus, or advance their academic careers simply because they are conservative. I have found that America’s university administrators do not significantly violate these norms.  Political discrimination on campus would also conflict with the history of American universities as havens of political activism. Activist subcultures on campus established some universities as hotbeds of political activism in the 1960s (Van Dyke 1998). The tradition of political activism on campus continues today (Pedris 2018). In fact, there has been an increase in   95 campus activism in recent years (Eagan et al. 2015). A team of researchers from the Cooperative Institutional Research Program (CIRP) at the Higher Education Research Institute at the University of California-Los Angeles found in 2015 that the percent of college students who say they would participate in a protest was at the highest level recorded since 1967, when researchers began asking this question (Eagan et al. 2015). In other words, the study of campus political activism is more relevant than ever. I directly test an important issue in this domain, by seeing if universities systematically favor one type of political activism over another. The findings of no significant bias in the overall samples of the three experiments with administrators are thus good news for the real-world politics of universities. The Polarized Decline in Trust in Institutions of Higher Education  Among American citizens, sharp partisan divisions exist in evaluations of colleges and universities, with a large majority of Democrats saying these institutions have a positive impact on the country, and a large majority of Republicans saying the opposite (Fingerhut 2017). In 2017, a solid majority (58%) of Republicans and Republican-leaning independents said colleges and universities have a negative impact on the way things are going in the country (Fingerhut 2017). Just two years earlier, it was the reverse: a solid majority of these respondents (54%) said these institutions have a positive impact (Fingerhut 2017). This polarized decline in trust in institutions of higher education underscores the significance of these experiments designed to test political discrimination in the academy.   There is already clear evidence that perceived discrimination against conservatives on campus plays a role in this lack of trust: Republicans who professed low trust in colleges and universities largely cited political concerns in a 2017 Gallup poll (Newport and Busteed 2017). Republicans who distrust these institutions, in other words, distrust them because they perceive   96 them as inhospitable to the ideas of conservatives, and overly indulgent to the ideas of liberals. By contrast, the smaller number of Democrats who distrusted colleges and universities cite largely financial reasons (Newport and Busteed 2017).   The polarized decline in trust in institutions of higher education mirrors polarized patterns of trust in government (Hetherington and Rudolph 2015; Pew 2017): those whose party is out of power tend to distrust the government. In colleges and universities, liberals are always in power. Professors are overwhelmingly liberal (Rothman, Lichter, and Nevitte 2005; Klein and Stern 2009). Conservative parents wonder why they should send their children to liberal campuses to have all their hard ideological work undone (Kimball 1990) and, similarly, why their tax dollars should fund their political rivals (Lu 2017). Thus, while liberals may question the decline of trust in higher education among conservatives as a negative development, from a conservative perspective, too much trust is gullibility (Rotter 1980).  Similar to the way perceptions of the media as hostile lead to selective exposure and the creation of politically homogenous online spaces (Ladd 2012; Borah, Thorson, and Hwang 2015), conservatives have responded to the left’s control of the American college campus with a “hot new brand in higher education –the conspicuously conservative college” (Wheeler 2017). The creation and proliferation of avowedly conservative colleges allows conservative parents to send their children off to college, safe in the knowledge that their preconceived notions will not be too strenuously tested. Surely the better outcome for the intellectual diversity of college life is not to segregate along political fault lines. Yet, the perception of university discrimination against conservatives is so widespread, the rise of conservative-oriented colleges makes perfect sense. I provide some much-needed scientific evidence on the veracity of these perceptions.      97 Social Capital and Campus Conservatives   This research also carries important implications for the development of social capital among the campus conservative community. The term social capital refers to social connections and norms of reciprocity and trust that arise from them, which enable more effective solutions to collective action problems than if each individual had to go it alone (Putnam 2000). Putnam (2000 19) identifies the first use of the term social capital by a Progressive-Era state supervisor of rural West Virginia schools, J.L. Hanifan. Later researchers also studied social capital in the educational environment (Coleman 1988). I carry on this century-old line of research on social capital in the context of education by investigating one possible impediment to the development of social capital: university discrimination.   Social capital has been linked to an astounding array of positive outcomes: civic engagement (Brehm and Rahn 1997; Putnam 2000; La Due Lake and Huckfeldt 2002), better quality of governance (Knack 2002), support for democracy (Muller and Seligson 1994), better health outcomes (Kawachi 1999; Kennedy, Kawachi, and Brainerd 1998; Kawachi, Kennedy, and Glass 1999), better educational outcomes (Coleman 1988; Goldin and Katz 1998), decreased juvenile delinquency and behavioral issues (Sampson, Raudenbush, and Arls 1997; Parcel and Menaghan 1993; Furstenburg and Hughes 1995), economic development (Fukuyama 1995; Knack and Keefer 1997; Zak and Knack 2001), charitable giving (Brooks 2005), community resilience to natural disasters (Aldrich and Meyer 2015), less income inequality (Kawachi, Kennedy, Lochner, Prothrow-Stith 1997) and even lower levels of violent crime (Kennedy, Kawachi, Prothrow-Stith, Lochner, and Gupta 1998; Rosenfield, Balmer, and Messner 2001). Any impediment to the development of social capital may thus hinder positive outcomes like these.   98  Young people are a particular group of concern in the literature on the decline of social capital (Twenge, Campbell, and Carter 2014; Rahn and Transue 2002; Putnam 2000 247-277; Glaeser, Laibson, Scheinkman, and Soutter 2000). I investigate whether the development of social capital among conservative college students faces a threat in the form of discrimination at the hands of institutions of higher education. This research thus fits nicely within the literature on social capital among the youth.   I investigate activities with a direct bearing on the development of social capital. I study whether administrators discriminate against conservatives in terms of inviting speakers to campus, publishing political editorials in the campus newspaper, and establishing new campus organizations. Face-to-face meetings, such as those offered by campus groups and lectures by invited speakers, are critical to the development of social capital, yet are in decline, especially among young people (Putnam 2000). Campus newspapers may also facilitate developing social capital by establishing a sense of community among readers, and providing an outlet to express one’s views publicly. I also study whether conservatives have a harder time advancing their academic careers, which is relevant to social capital given the link between education and social capital (Coleman 1988; Goldin and Katz 1998).  There is ample evidence that discrimination lowers social capital. Black people are much less trusting in general than white people, and women are marginally less trusting than men (Alesina and La Ferrara 2002; Uslaner 2002; Smith 2010). Among people of color, those who have experienced more frequent discrimination are even less trusting (blacks: Nunally 2012; Latinos: Levitt 2015). This lower level of trust among groups who have faced discrimination and even less trust among those who report more discrimination suggests discrimination lowers the norms of trust integral to social capital. Scholars agree that discrimination lowers social capital   99 (DeMaris & Yang 1994; Brehm and Rahn 1997; Smith 1997; Patterson 1999; Claibourn and Martin 2000; Alesina and La Ferrara 2002), as does simply belonging to an “outsider” group, i.e. not the “in-group” (Boldizar and Messick 1988; Tajfel 1970; Tajfel et al. 1971; Jetten, Spears, and Manstead 1996; Gaertner and Insko 2000; Glaeser et al. 2000; Fershtman and Gneezy 2001; Simpson 2007; Barr 2004).   The parallels here between campus conservatives and traditionally marginalized groups are readily apparent. Conservatives are the minority “out-group” on the campus numerically dominated by liberals, and they perceive discrimination on the basis of that out-group status. While obvious differences in the basis of discrimination remain (ideology is a choice; gender, race, and ethnicity are not), complicating any attempt to equate, say, racial discrimination with ideological discrimination, there are clear, negative consequences of discrimination against conservative students on college campuses. Ideological discrimination would exacerbate the polarized decline in trust in institutions of higher education, inhibit the development of social capital among conservative students, weaken the intellectual diversity of university life, and spell massive funding cuts to higher education at the hands of Republican administrations. There is clearly a lot at stake here.  My hope is that this dissertation’s impact goes beyond the ivory tower and contributes valuable empirical evidence to a long-standing, spirited national conversation. I have found no significant bias against conservative students by university administrators across three fair tests. Conservative students face no significant bias in their efforts to establish political student groups, publish political editorials in the campus newspaper, or invite speakers to lecture on political issues on campus. These specific activities were chosen because they were sites of alleged “leftist abuses and bias” on campus by a prominent conservative interest group focused on higher   100 education, Campus Reform (Blackwell 2018). The experiments with professors were also designed with the concerns of popular conservative critics of the academy in mind (Horowitz in Schmidt 2011). I tested whether university professors are more responsive to students interested in research that matches liberal ideology. The findings that professors do discriminate against conservatives suggest these perceptions are correct.   In the long run, will it matter that I find conservative students face no significant bias in establishing new student groups, publishing editorials in the campus newspaper, or inviting speakers to campus? Popular dissemination of these findings might make a tiny, temporary dent in the polarized decline in trust in institutions of higher education. Conservatives who are open to empirical evidence that contradicts their strongly held beliefs might reconsider those beliefs upon hearing these results. This has certainly been the case for Matt Woessner, a conservative political scientist who, along with his wife April Kelly-Woessner, went looking for evidence of political indoctrination on campus, and didn’t really find any (Maranto and Woessner 2017).  Future research might examine reactions to learning the results of no political bias, particularly among conservatives.   The news that professors discriminate against conservative students while university administrators do not prompts the question of why this is the case. One reason this might be the case is that the student sender in the administrator experiments is presumably a current student, already on campus, and thus clearly a member of the population served by the subjects, while in the professor experiments, the sender is a prospective student. Another reason there may be bias among professors but not among administrators is that administrators would not have to work with the students long-term, while professors could potentially work with the student sender for years. Professors who take on a student interested in pursuing research projects may have to deal   101 with the student for a very long time, involving many conversations and interactions. Perhaps the potentially long-term nature of such a relationship makes professors more responsive to students who share their views. Administrators in these experiments, by contrast, would only have to approve or disapprove the student’s request, and not have much more interaction afterword. It may be more palatable to forgo political bias when one knows they will not have to deal with a student for very long.  I have also suggested that professionalization and adherence to professional norms may explain this discrepancy. These factors may play more of a role in the hiring and retention of university administrators than faculty, who are hired and retained primarily for their skills in research and teaching. These activities (development and maintenance of professional norms, proper training, recruitment of competent individuals) may shape bureaucratic preferences (Brehm and Gates 1997 202; Vinzant and Crothers 1998 155-156). These facts suggest not only a reason for the discrepancy in findings between the sample of administrators and the sample of professors, but a way forward for professors that currently exhibit political bias. With the development of stronger professional norms prohibiting political bias, proper training, and recruitment of competent individuals to fill administrative roles such as the chair of the department, professors in the social sciences and humanities may become less discriminatory toward conservative students. The experimental results presented in this dissertation project should serve as a tool for awareness, bringing the issue to the attention of professors in the social sciences and humanities. They may dismiss the experiments as critically flawed and thus uninformative or utilize this opportunity for change.   102 APPENDIX A  AUDIT AND CORRESPONDENCE STUDIES   This appendix shows a table listing a large number of audit and correspondence experiments similar to the experiments presented in this dissertation. The large number of similar studies demonstrates that these methodologies are both popular and widely accepted as standard practice in social science fields. The first column of Table A.1 displays the topic of each study; the second column displays the authors and year of publication; the third column displays the journal in which each study was published.  Table A.1: Audit and Correspondence Studies Topic  Author (Year)  Journal  Anti-Muslim discrimination  in the French labor market  Adida, Laitin, and Valfort  (2010)  Proceedings of the National  Academy of Sciences  Sex discrimination in  restaurant hiring  Neumark, Bank, and Van  Nort (1996)  The Quarterly Journal of  Economics  The effect of time on race and  gender discrimination in  academia  Milkman, Akinola, and  Chugh (2012)  Psychological Science  The effect of incarceration on  job opportunities  Pager (2003)  American Journal of  Sociology  The effect of race and alma  mater on job opportunities  Gaddis (2014)  Social Forces  Racial discrimination in  housing  Yinger (1986)  American Economic Review  Race and gender  discrimination in bargaining  for a new car  Ayres and Siegelman (1995)  American Economic Review  Racial discrimination in job  recruitment  Newman (1978)  Industrial and Labor  Relations Review  Racial and gender  discrimination in entry -level  job opportunities  McIntyre, Moberg, and  Posner (1980)  Academy  of Management  Journal  Racial discrimination in the  market for sports cards  List (2004)  The Quarterly Journal of  Economics  The effects of race and  physical attractiveness on job  opportunities  Galarza and Yamada (2014)  World Development    103 Topic  Author (Year)  Journal  Ethnic discrimination in  China’s Internet job board  labor market  Maurer -Fazio (2012)  IZA Journal of Migration  Ethnic discrimination in the  Australian labor market  Booth, Leigh, and Varganova  (2011)  Oxford Bulletin of Economics  and Statistics  The effect of periods of unemployment on job opportunities Eriksson and Rooth (2014) American Economic Review Anti-Arabic discrimination in  the Dutch labor market  Blommaert, Coenders, and  van Tubergen (2014)  Social Forces  Racial discrimination in the  labor market for recen t  college graduates  Nunley, Pugh, Romero, and  Seals (2014)  Auburn University  Department of Economics  Working Paper Series  The effect of periods of  unemployment on job  opportunities  Ghayad (2013)  Working Paper  The effect of racialized job  applicant names on effort of  employers to inspect resumes  Bartoš, Bauer, Chytilová, and  Matějka (2013)  American Economic Review  Religious affiliation and  hiring discrimination in New  England  Wright, Wallace, Bailey, and  Hyde (2013)  Research in Social  Stratification and Mobility  The effect of periods of  unemployment on job  opportunities  Kroft, Lange, and  Notowidigdo (2013)  The Quarterly Journal of  Economics  The effect of difficulty in  filling job vacancies on  employer discrimination in  hiring  Baert, Cockx, Gheyle, and  Vandamme (2015)  ILR Review  Anti-gay discrimination in  job hiring  Bailey, Wallace, and Wright  (2013)  Journal of Homosexuality  The effect of increasing  information about job  applicants on employer hiring  discrimination  Ahmed, Ande rsson, and  Hammarstedt (2010)  Land Economics  Age discrimination in the  Swedish labor market  Ahmed, Andersson, and  Hammarstedt (2012)  Applied Economics Letters  Anti-gay discrimination in  job hiring  Ahmed, Andersson, and  Hammarstedt (2013)  Southern Economi c Journal  Anti-gay discrimination in  housing  Ahmed and Hammarstedt  (2009)  Economica  Hiring discrimination based  on religion, sexuality using  personal information posted  to social media  Acquisti and Fong (2013)  Working Paper    104 Topic  Author (Year)  Journal  Discrimination based on sex  and sexuality in the European  labor market  Patacchini, Ragusa, and  Zenou (2015)  Population Economics  Ethnic discrimination in the  German labor market  Kaas and Manger (2012)  German Economic Review  Racial discrimination in the  Chicago labor market  Jacquemet and Yannelis  (2012)  Labor Economics  Anti-immigrant discrimination in the Toronto labor market Oreopoulos (2011) American Economic Journal: Economic Policy Gender discrimination in the  Swedish labor market  Carlsson (2011)  Feminist Economics  Gender discrimination in  female -dominated jobs  Booth and Leigh (2010)  Economics Letters  Age discrimination in the  English labor market  Riach and Rich (2010)  Annals of Economics and  Statistics  The effects of obesity and  attractiveness on job  opportunities  Rooth (2009)  Journal of Human Resources  Ethnic and national origin  discriminat ion in the Irish  labor market  McGinnity, Nelson, Lunn,  and Quinn (2009)  report by The Equality  Authority and the Economic  and Social Research Institute  Caste discrimination in the  Delhi, India labor market  Banerjee, Bertrand, Datta,  and Mullainathan (2009)  Journal of Comparative  Economics  Age and gender  discrimination in hiring  Lahey (2008)  Journal of Human Resources  The effects of age and family  constraints on job  opportunities in the French  financial sector  Petit (2006)  Labour Economics  Racial and religious  discrimination in marketing  job opportunities  Jolson (1974)  Journal of Marketing  Ethnic discrimination in the  Swedish labor market  Bursell (2007)  SULCIS Reports and Working  Papers  Racial discrimination in the  labor market  Bertrand and Mullainathan  (2004)  American Economic Review  Gender bias among science  faculty  Moss - Racusin , Dovidio , Brescoll ,  Graham , and  Handelsman  (2012)  Proceedings of the National  Academy of Sciences  Black legislators’ intrinsic  motivation to advance black  interests  Broockman (2013)  American Journal of Political  Science    105 Topic  Author (Year)  Journal  Racial discrimination among  politicians  Butler and Broockman (2011)  American Journal of Political  Science  Ethnic discrimination in  online auctions  Shohat and Musch (2003)  Swiss Journal of Psychology  Attractiveness discrimination  in the labor market  Boo, Rossi, and Urzu (2013)  Economics Letters  Gender and racial discrimination in hiring in Lima, Peru Moreno, Nopo, Saavedra, and Torero (2012) World Development Attractiveness discrimination  in hiring  Ruffle and Shtudiner (2014)  Management Science  The effect of sex and  personality on hiring  Weichselbaumer (2004)  Eastern Economic Journal  Gender and social class  discrimination in the Chilean  labor market  Bravo, Sanhueza, and Urzua  (2007)  Gender Discrimination and  Economic Outcomes in Chile,  Special Project  Caste -based discrimination  Siddique (2008)  Institute for the Study of  Labor in Bonn Discussion  Paper  Ethnic discrimination in the  Swedish  labor market  Carlsson and Rooth (2007)  Labor Economics  Ethnic discrimination in the  Canadian labor market  Oreopoulos and Dechief  (2011)  Working Paper, Center of  Excellence for Research on  Immigration and Diversity  Labor market discrimination  against migrant workers in  Italy Allasino, Reyneri, Venturini,  and Zincome (2004)  ILO Migration Papers  Ethnic and gender  discrimination in the Swedish  labor market  Arai, Bursell, and Nekby  (2008)  Institute for the Study of  Labor in Bonn Dis cussion  Paper  Discrimination in low -wage  labor markets  Pager, Bonikowski, and  Western (2009)  American Sociological  Review  The effect of ex -offender  status on employability  Pager and Quillian (2005)  American Sociological  Review  Racial discrimination in  employment, housing, credit,  and consumer markets  Pager and Shepherd (2008)  Annual Review of Sociology  Age discrimination in hiring  Bendick, Brown, and Wall  (1999)  Journal of Aging and Social  Policy  Age discrimination in hiring  Bendick, Jackson, and  Romero (1996)  Journal of Aging and Social  Policy  Age discrimination in the  French labor market  Riach and Rich (2006a)  Institute for the Study of  Labor in Bonn Discussion  Paper    106 Topic  Author (Year)  Journal  Age discrimination in the  Spanish labor market  Riach and Rich (2007)  Institute for the Study of  Labor in Bonn Discussion  Paper  Sex and age discrimination in  the Madrid labor market  Albert, Escot, and Fernandez - Cornejo (2011)  International Journal of  Human Resource  Management  Race and gender discrimination in bargaining over a new car Ayres (1991) Harvard Law Review Discrimination Against  Latino Job Applicants  Bendick, Jackson, Reinoso,  and Hodges (1991)  Human Resources  Management  Ethnic discrimination in  rental applications  Carpusor and Loges (2006)  Journal of Applied Social  Psychology  Sexual orientation  discrimination in the labor  market  Drydakis (2009)  Labour Economics  Women’s sexual orientation  discrimination in the Greek  Labor Market  Drydakis (2009)  Feminist Economics  Ethnic discrimination in the  French l abor market  Duguet, Leandri, L’horty, and  Petit (2010)  Annals of Economics and  Statistics/ Annales  d’Economie et de Statistique  Hiring discrimination in the  French financial sector  Duguet and Petit (2005)  Annals of Economics and  Statistics/ Annales  d’Economie et de Statistique  Gender discrimination in job  opportunities  Firth (1982)  Sex Roles  Discrimination against  female -headed households in  rental housing  Galster and Constantine  (1991)  Review of Social Economy  Anti-gay discrimination in the  labor  market  Hebl, Foster, Mannix, and  Dovidio (2002)  Personality and Social  Psychology Bulletin  Discrimination against young  Hispanic jobseekers  Kenney and Wissoker (1994)  American Economic Review  Sexual discrimination in the  labor market  Riach and Rich (198 7) Australian Economic Papers  Sexual orientation  discrimination in hiring  Weichselbaumer (2003)  Labour Economics  Racial discrimination in  recruitment in British cities  Wood, Hales, Purdon,  Sejerson, and Hayllar (2009)  DWP Research Reports  Discrimination in the rental  housing market  Ahmed and Hammarstedt  (2008)  Journal of Urban Economics  Discrimination against those  of foreign origin in the  Belgian labor market  Arrijn, Feld, and Nayer  (1998)  International Migration  Papers    107 Topic  Author (Year)  Journal  Discrimination ag ainst  immigrants in the Swedish  labor market  Attstron (2007)  International Migration  Papers  Racial discrimination in new  car negotiations  Ayres (1995)  Michigan Law Review  Ethnic discrimination in the  Italian rental housing market  Baldini and Federici (2011)  Journal of Housing  Economics  Racial and ethnic discrimination in the US labor market  Bedick (1996) International Migration Papers Racial and ethnic  discrimination in the Spanish  rental housing market  Bosch, Carnero, and Farré   (2010).  Regional Science and Urban  Economics  Discrimination against  migrant workers in the  Netherlands  Bovenkerk, Gras, and  Ramsoedh (1994)  International Migration  Papers  The effects of gender, race,  age, and employment status  in the Swedish rental market  Carlsson and Eriksson (2014)  Journal of Housing  Economics  Discrimination against those  of foreign origin in the  French labor market  Cediey and Foroni (2008)  International Migration  Papers  The role of race in online  market outcomes  Doleac and Stein (2013)  The Economic Journal  Ethnic discrimination in the  Greek labor market  Drydakis and Vlassis (2010)  The Manchester School  Discrimination against  doctors on the basis of race  Esmail and Everington (1993)  British Medical Journal  Discrimination against  Asian  doctors  Esmail and Everington (1997)  British Medical Journal  Racial discrimination in the  U.S. rental housing market  Ewens, Tomlin, and Wang  (2014)  Review of Economics and  Statistics  Discrimination against  foreign workers in the  German labor market  Goldberg, Mourinho, and  Kulke (1996)  International Migration  Papers  Discrimination in the U.S.  online rental housing market  Hanson and Hawley (2011)  Journal of Urban Economics  Racial and ethnic  discrimination in the rental  housing market  Hogan and Berry (2011)  City and Community  Anti-Muslim discrimination  in retail job applications  King and Ahmad (2010)  Personnel Psychology  Discrimination against same - sex couples and single Lauster and Easterbrook  (2011)  Social Problems    108 Topic  Author (Year)  Journal  parents in the rental housing  market  Sex discrimination in the  labor market  Levinson (1975)  Social Problems  Racial discrimination in  urban housing markets  Massey and Lundy (2001)  Urban Affairs Review  Racial discrimination in urban housing markets Page (1995) Journal of Urban Economics Labor market discrimination  against migrant workers in  Spain  de Prada, Actis, Pereda, and  Perez (1996)  International Migration  Papers  Sexual discrimination in the  English labor market  Riach and Rich (2006b)  Advances in Economic  Analysis and Policy  Racial discrimination in the  housing market  Roychoudhury and Goodman  (1996)  Real Estate Economics  Anti-gay discrimination in  employment in the U.S.  Tilcsik (2011)  American Journal of  Sociology  Ethnic discrimination in the Israeli online market for used cars Zussman (2013) The Economic Journal      109 APPENDIX B STUDENT GROUP EXPERIMENT BALANCE TESTS  Despite recent criticisms (Mutz 2019), balance tests are often used to demonstrate fulfillment of the expectation of equivalence across experimental conditions. Tables B.1, B.2, and B.3 display the results of balance tests for all emails sent in the overall sample (n = 1,470). Tables B.4, B.5, and B.6 display the results of balance tests for the set of all usable emails (excluding those that were answered by an automated system or came back marked undeliverable) in the overall sample (n=1366). No p-value for any coefficient is at .05 or below, suggesting the experimental conditions were balanced on all covariates and equal in expectation in the overall sample. The dependent variable in each model is coded 1 if the subject was assigned to the specified condition and 0 otherwise. For example, the dependent variable in the model whose results are reported in Table B.1 is scored 1 if the subject was assigned to the Liberal Condition and 0 otherwise. Table B.1: Balance Tests, Liberal Condition, Student Group Experiment, All Emails Sent     110  Table B.2: Balance Tests, Conservative Condition, Student Group Experiment, All Emails Sent     Table B.3: Balance Tests, Control Condition, Student Group Experiment, All Emails Sent        111   Table B.4: Balance Tests, Liberal Condition, Student Group Experiment, All Valid Emails     Table B.5: Balance Tests, Conservative Condition, Student Group Experiment, All Valid Emails        112 Table B.6: Balance Tests, Control Condition, Student Group Experiment, All Valid Emails        113 APPENDIX C  STUDENT GROUP EXPERIMENT SUPPLEMENTARY MODEL RESULTS   The tables shown in this Appendix display the results logistic regression, Cox Proportional-Hazards, and two-stage selection models of response as a robustness check on the results presented in the body of the dissertation. The dependent variable in the logistic regression models is a binary indicator of whether a subject replied (1) or not (0). The dependent variable in the two-stage models is the number of days to reply. The hazards models take both the reply at all and the number of days to reply variables to compute a hazard ratio for each predictor. HR for a predictor variable over 1 indicate that predictor is associated with an increase in the risk of an event (here, a reply), while HR under 1 indicate a decrease in such a risk.   Table C.1: Logistic Regression Model of Response in Student Group Experiment     114 Table C.2: Cox Proportional-Hazards Model for Right-Censoring of Days to Reply in Student Group Experiment       115 Table C.3: Two-Stage Heckman Selection Model of Days to Reply in Student Group Experiment       116 APPENDIX D  NEWSPAPER EDITORIAL EXPERIMENT BALANCE TESTS   Despite recent criticisms (Mutz 2019), balance tests are often used to demonstrate fulfillment of the expectation of equivalence across experimental conditions. Tables D.1, D.2, and D.3 display the results of balance tests for all emails sent in the overall sample (n = 1,058). No p-value for any coefficient is at .05 or below, suggesting the experimental conditions were balanced on all covariates and equal in expectation in the overall sample. Tables D.4, D.5, and D.6 display the results of balance tests for the set of all usable emails (excluding those that were answered by an automated system or came back marked undeliverable) in the overall sample (n=1,023). The only p-value for a coefficient below .05 in these tables is for the coefficient of the variable indicating school religiosity in the Liberal Condition balance test. This is not an issue, however, because the results of no significant political bias remain unchanged when controlling for religiosity.  The dependent variable in each model is coded 1 if the subject was assigned to the specified condition and 0 otherwise. For example, the dependent variable in the model whose results are reported in Table D.1 is scored 1 if the subject was assigned to the Liberal Condition and 0 otherwise.     117 Table D.1: Balance Tests, Liberal Condition, Newspaper Editorial Experiment, All Emails Sent    Table D.2: Balance Tests, Conservative Condition, Newspaper Editorial Experiment, All Emails Sent        118 Table D.3: Balance Tests, Control Condition, Newspaper Editorial Experiment, All Emails Sent        119 Table D.4: Balance Tests, Liberal Condition, Newspaper Editorial Experiment, All Valid Emails     Table D.5: Balance Tests, Conservative Condition, Newspaper Editorial Experiment, All Valid Emails         120 Table D.6: Balance Tests, Control Condition, Newspaper Editorial Experiment, All Valid Emails         121 APPENDIX E   NEWSPAPER EDITORIAL EXPERIMENT SUPPLEMENTARY MODEL RESULTS   The tables shown in this Appendix display the results logistic regression, Cox Proportional-Hazards, and two-stage selection models of response as a robustness check on the results presented in the body of the dissertation. The dependent variable in the logistic regression models is a binary indicator of whether a subject replied (1) or not (0). The dependent variable in the two-stage models is the number of days to reply. The hazards models take both the reply at all and the number of days to reply variables to compute a hazard ratio for each predictor. HR for a predictor variable over 1 indicate that predictor is associated with an increase in the risk of an event (here, a reply), while HR under 1 indicate a decrease in such a risk.       122 Table E.1: Logistic Regression Model of Response, Newspaper Experiment      123 Table E.2: Cox Proportional-Hazards Model for Right-Censoring of Days to Reply in Newspaper Experiment        124 Table E.3: Two-Stage Heckman Selection Model of Days to Reply in Newspaper Experiment         125 APPENDIX F   INVITED SPEAKER SPACE EXPERIMENT BALANCE TESTS   Despite recent criticisms (Mutz 2019), balance tests are often used to demonstrate fulfillment of the expectation of equivalence across experimental conditions. Tables F.1, F.2, and F.3 display the results of balance tests for all emails sent in the overall sample (n = 1,439). No p-value for any coefficient is at .05 or below, suggesting the experimental conditions were balanced on all covariates and equal in expectation in the overall sample. Tables F.4, F.5, and F.6 display the results of balance tests for the set of all usable emails (excluding those that were answered by an automated system or came back marked undeliverable) in the overall sample (n=1366). The only p-value for a coefficient below .05 in these tables is for the coefficient of the variable indicating school religiosity in the Conservative Condition balance test. This is not an issue, however, because the results of no significant political bias remain unchanged when controlling for religiosity.  The dependent variable in each model is coded 1 if the subject was assigned to the specified condition and 0 otherwise. For example, the dependent variable in the model whose results are reported in Table F.1 is scored 1 if the subject was assigned to the Liberal Condition and 0 otherwise.     126 Table F.1: Balance Tests, Liberal Condition, Invited Speaker Experiment, All Emails Sent    Table F.2: Balance Tests, Conservative Condition, Invited Speaker Experiment, All Emails Sent         127 Table F.3: Balance Tests, Control Condition, Invited Speaker Experiment, All Emails Sent     Table F.4: Balance Tests, Liberal Condition, Invited Speaker Experiment, All Valid Emails         128 Table F.5: Balance Tests, Conservative Condition, Invited Speaker Experiment, All Valid Emails      Table F.6: Balance Tests, Control Condition, Invited Speaker Experiment, All Valid Emails         129 APPENDIX G   INVITED SPEAKER SPACE EXPERIMENT SUPPLEMENTARY MODEL RESULTS   The tables shown in this Appendix display the results logistic regression, Cox Proportional-Hazards, and two-stage selection models of response as a robustness check on the results presented in the body of the dissertation. The dependent variable in the logistic regression models is a binary indicator of whether a subject replied (1) or not (0). The dependent variable in the two-stage models is the number of days to reply. The hazards models take both the reply at all and the number of days to reply variables to compute a hazard ratio for each predictor. HR for a predictor variable over 1 indicate that predictor is associated with an increase in the risk of an event (here, a reply), while HR under 1 indicate a decrease in such a risk.       130 Table G.1: Logistic Regression Model of Response, Invited Speaker Space Experiment         131 Table G.2: Cox Proportional-Hazards Model for Right-Censoring of Days to Reply in Invited Speaker Space Experiment       132 Table G.3: Two-Stage Heckman Selection Model of Days to Reply in Invited Speaker Space Experiment       133 APPENDIX H   SOCIAL SCIENCE PROFESSORS EXPERIMENT BALANCE TESTS   Despite recent criticisms (Mutz 2019), balance tests are often used to demonstrate fulfillment of the expectation of equivalence across experimental conditions. Tables H.1, H.2, H.3, H.4, H.5, and H.6 display the results of balance tests for all emails sent in the overall sample (n = 2,006). No p-value for any coefficient is at .05 or below, suggesting the experimental conditions were balanced on all covariates and equal in expectation in the overall sample. The dependent variable in each model is coded 1 if the subject was assigned to the specified condition and 0 otherwise. For example, the dependent variable in the model whose results are reported in Table H.1 is scored 1 if the subject was assigned to the Liberal, Low ACT Condition and 0 otherwise.     134 Table H.1: Balance Tests, Liberal-Low ACT Condition, Social Science Professors Experiment, All Emails Sent        135 Table H.2: Balance Tests, Conservative-Low ACT Condition, Social Science Professors Experiment, All Emails Sent        136 Table H.3: Balance Tests, Control-Low ACT Condition, Social Science Professors Experiment, All Emails Sent        137 Table H.4: Balance Tests, Liberal-High ACT Condition, Social Science Professors Experiment, All Emails Sent        138 Table H.5: Balance Tests, Conservative-High ACT Condition, Social Science Professors Experiment, All Emails Sent        139 Table H.6: Balance Tests, Control-High ACT Condition, Social Science Professors Experiment, All Emails Sent        140 APPENDIX I   SOCIAL SCIENCE PROFESSORS EXPERIMENT SUPPLEMENTARY MODEL RESULTS   The tables shown in this Appendix display the results logistic regression, Cox Proportional-Hazards, and two-stage selection models of response as a robustness check on the results presented in the body of the dissertation. The dependent variable in the logistic regression models is a binary indicator of whether a subject replied (1) or not (0). The dependent variable in the two-stage models is the number of days to reply. The hazards models take both the reply at all and the number of days to reply variables to compute a hazard ratio for each predictor. HR for a predictor variable over 1 indicate that predictor is associated with an increase in the risk of an event (here, a reply), while HR under 1 indicate a decrease in such a risk.        141 Table I.1: OLS-Social Sciences Overall Reply or Not, Low ACT Condition   Table I.2: OLS-Social Sciences Overall Substantive Reply, Low ACT Condition     142  Table I.3: Logit-Social Sciences Overall Reply or Not, Low ACT Condition    Table I.4: Logit-Social Sciences Overall Substantive Reply, Low ACT Condition        143 Table I.5: Hazard Model-Social Sciences Overall, Low ACT Condition        144 Table I.6: Two-Stage Selection Model-Social Sciences Overall, Low ACT Condition        145 Table I.7: OLS-Social Sciences Overall Reply or Not, High ACT Condition        146 Table I.8: OLS-Social Sciences Overall Substantive Reply, High ACT Condition    Table I.9: Logit-Social Sciences Overall Reply or Not, High ACT Condition      147 Table I.10: Logit-Social Sciences Overall Substantive Reply, High ACT Condition    Table I.11: Hazard Model-Social Sciences Overall High ACT Condition        148 Table I.12: Two-Stage Selection Model-Social Sciences Overall High ACT Condition        149 Table I.13: OLS-Political Science Reply or Not, Low ACT Condition        150 Table I.14: OLS-Political Science Substantive Reply, Low ACT Condition    Table I.15: Logit-Political Science Reply or Not, Low ACT Condition     151  Table I.16: Logit-Political Science Substantive Reply, Low ACT Condition    Table I.17: Hazard Model-Political Science Low ACT Condition      152  Table I.18: Two-Stage Selection Model-Political Science Low ACT Condition        153   Table I.19: OLS-Political Science Reply or Not, High ACT Condition        154 Table I.20: OLS-Political Science Substantive Reply, High ACT Condition    Table I.21: Logit-Political Science Reply or Hot, High ACT Condition      155 Table I.22: Logit-Political Science Substantive Reply, High ACT Condition     Table I.23: Hazard Model-Political Science High ACT Condition      156 Table I.24: Two-Stage Selection Model-Political Science High ACT Condition       157 Table I.25: OLS-Sociology Reply or Not, Low ACT Condition        158 Table I.26: OLS-Sociology Substantive Reply, Low ACT Condition    Table I.27: Logit-Sociology Reply or Not, Low ACT Condition      159 Table I.28: Logit-Sociology Substantive Reply, Low ACT Condition    Table I.29: Hazard Model-Sociology Low ACT Condition        160 Table I.30: Two-Stage Selection Model-Sociology Low ACT Condition        161 Table I.31: OLS-Sociology Reply or Not, High ACT Condition        162 Table I.32: OLS-Sociology Substantive Reply, High ACT Condition    Table I.33: Logit-Sociology Reply or Not, High ACT Condition      163 Table I.34: Logit-Sociology Substantive Reply, High ACT Condition    Table I.35: Hazard Model-Sociology High ACT Condition        164 Table I.36: Two-Stage Selection Model-Sociology High ACT Condition        165 Table I.37: OLS-Economics Reply or Not, Low ACT Condition        166 Table I.38: OLS-Economics Substantive Reply, Low ACT Condition    Table I.39: Logit-Economics Reply or Not, Low ACT Condition      167 Table I.40: Logit-Economics Substantive Reply, Low ACT Condition    Table I.41: Hazard Model-Economics, Low ACT Condition        168 Table I.42: Two-Stage Selection Model-Economics, Low ACT Condition        169 Table I.43: OLS-Economics Reply or Not, High ACT Condition        170 Table I.44: OLS-Economics Substantive Reply, High ACT Condition    Table I.45: Logit-Economics Reply or Not, High ACT Condition        171 Table I.46: Logit-Economics Substantive Reply, High ACT Condition    Table I.47: Hazard Model-Economics, High ACT Condition      172 Table I.48: Two-Stage Selection Model-Economics, High ACT Condition        173 APPENDIX J   HUMANITIES PROFESSORS EXPERIMENT BALANCE TESTS   Despite recent criticisms (Mutz 2019), balance tests are often used to demonstrate fulfillment of the expectation of equivalence across experimental conditions. Tables J.1, J.2, H.3, J.4, J.5, and J.6 display the results of balance tests for all emails sent in the overall sample (n = 2,006). No p-value for any coefficient is at .05 or below, suggesting the experimental conditions were balanced on all covariates and equal in expectation in the overall sample. The dependent variable in each model is coded 1 if the subject was assigned to the specified condition and 0 otherwise. For example, the dependent variable in the model whose results are reported in Table J.1 is scored 1 if the subject was assigned to the Liberal, Low ACT Condition and 0 otherwise.      174 Table J.1: Balance Tests, Liberal-Low ACT Condition, Humanities Professors Experiment, All Emails Sent        175 Table J.2: Balance Tests, Conservative-Low ACT Condition, Humanities Professors Experiment, All Emails Sent        176 Table J.3: Balance Tests, Control-Low ACT Condition, Humanities Professors Experiment, All Emails Sent        177 Table J.4: Balance Tests, Liberal-High ACT Condition, Humanities Professors Experiment, All Emails Sent        178 Table J.5: Balance Tests, Conservative-High ACT Condition, Humanities Professors Experiment, All Emails Sent        179 Table J.6: Balance Tests, Control-High ACT Condition, Humanities Professors Experiment, All Emails Sent         180 APPENDIX K   HUMANITIES PROFESSORS EXPERIMENT SUPPLEMENTARY MODEL RESULTS   The tables shown in this Appendix display the results logistic regression, Cox Proportional-Hazards, and two-stage selection models of response as a robustness check on the results presented in the body of the dissertation. The dependent variable in the logistic regression models is a binary indicator of whether a subject replied (1) or not (0). The dependent variable in the two-stage models is the number of days to reply. The hazards models take both the reply at all and the number of days to reply variables to compute a hazard ratio for each predictor. HR for a predictor variable over 1 indicate that predictor is associated with an increase in the risk of an event (here, a reply), while HR under 1 indicate a decrease in such a risk.        181 Table K.1: OLS-Humanities Overall Reply or Not, Low ACT Condition        182 Table K.2: OLS-Humanities Overall Substantive Reply, Low ACT Condition    Table K.3: Logit-Humanities Overall Reply or Not, Low ACT Condition      183 Table K.4: Logit-Humanities Overall Substantive Reply, Low ACT Condition     Table K.5: Hazard Model-Humanities Overall, Low ACT Condition      184 Table K.6: Two-Stage Selection Model-Humanities Overall, Low ACT Condition        185 Table K.7: OLS-Humanities Overall Reply or Not, High ACT Condition         186 Table K.8: OLS-Humanities Overall Substantive Reply, High ACT Condition    Table K.9: Logit-Humanities Overall Reply or Not, High ACT Condition      187 Table K.10: Logit-Humanities Overall Substantive Reply, High ACT Condition    Table K.11: Hazard Model-Humanities Overall, High ACT Condition        188 Table K.12: Two-Stage Selection Model-Humanities Overall, High ACT Condition        189 Table K.13: OLS-English Reply or Not, Low ACT Condition        190 Table K.14: OLS-English Substantive Reply, Low ACT Condition        191 Table K.15: Logit-English Reply or Not, Low ACT Condition    Table K.16: Logit-English Substantive Reply, Low ACT Condition      192 Table K.17: Hazard Model-English, Low ACT Condition         193 Table K.18: Two-Stage Selection Model-English, Low ACT Condition        194 Table K.19: OLS-English Reply or Not, High ACT Condition        195 Table K.20: OLS-English Substantive Reply, High ACT Condition        196 Table K.21: Logit-English Reply or Not, High ACT Condition    Table K.22: Logit-English Substantive Reply, High ACT Condition      197 Table K.23: Hazard Model-English, High ACT Condition         198 Table K.24: Two-Stage Selection Model-English, High ACT Condition        199 Table K.25: OLS-Philosophy Reply or Not, Low ACT Condition        200 Table K.26: OLS-Philosophy Substantive Reply, Low ACT Condition        201 Table K.27: Logit-Philosophy Reply or Not, Low ACT Condition    Table K.28: Logit-Philosophy Substantive Reply, Low ACT Condition        202 Table K.29: Hazard Model-Philosophy, Low ACT Condition        203 Table K.30: Two-Stage Selection Model-Philosophy, Low ACT Condition      204 Table K.31: OLS-Philosophy Reply or Not, High ACT Condition         205 Table K.32: OLS-Philosophy Substantive Reply, High ACT Condition        206 Table K.33: Logit-Philosophy Reply or Not, High ACT Condition    Table K.34: Logit-Philosophy Substantive Reply, High ACT Condition        207 Table K.35: Hazard Model-Philosophy, High ACT Condition        208 Table K.36: Two-Stage Selection Model-Philosophy, High ACT Condition        209 Table K.37: OLS-Gender Studies Reply or Not, Low ACT Condition        210 Table K.38: OLS-Gender Studies Substantive Reply, Low ACT Condition    Table K.39: Logit-Gender Studies Reply or Not, Low ACT Condition      211 Table K.40: Logit-Gender Studies Substantive Reply, Low ACT Condition    Table K.41: Hazard Model-Gender Studies, Low ACT Condition       212 Table K.42: Two-Stage Selection Model-Gender Studies, Low ACT Condition        213 Table K.43: OLS-Gender Studies Reply or Not, High ACT Condition        214 Table K.44: OLS-Gender Studies Substantive Reply, High ACT Condition    Table K.45: Logit-Gender Studies Reply or Not, High ACT Condition     215  Table K.46: Logit-Gender Studies Substantive Reply, High ACT Condition        216 Table K.47: Hazard Model-Gender Studies, High ACT Condition        217 Table K.48: Two-Stage Selection Model-Gender Studies, High ACT Condition       218 APPENDIX L   IRB APPROVALS    The application that you submitted to this office in regard to the use of human subjects in the research proposal referenced above has been reviewed by the Human Subjects Committee at its meeting on 06/13/2018 Your project was approved by the Committee.  The Human Subjects Committee has not evaluated your proposal for scientific merit, except to weigh the risk to the human participants and the aspects of the proposal related to potential risk and benefit. This approval does not replace any departmental or other approvals which may be required.  If you submitted a proposed consent form with your application, the approved stamped consent form is attached to this approval notice. Only the stamped version of the consent form may be used in recruiting research subjects.  If the project has not been completed by 06/12/2019 you must request a renewal of   219 approval for continuation of the project. As a courtesy, a renewal notice will be sent to you prior to your expiration date; however, it is your responsibility as the Principal Investigator to timely request renewal of your approval from the Committee.  You are advised that any change in protocol for this project must be reviewed and approved by the Committee prior to implementation of the proposed change in the protocol. A protocol change/amendment form is required to be submitted for approval by the Committee. In addition, federal regulations require that the Principal Investigator promptly report, in writing, any unanticipated problems or adverse events involving risks to research subjects or others.  By copy of this memorandum, the chairman of your department and/or your major professor is reminded that he/she is responsible for being informed concerning research projects involving human subjects in the department, and should review protocols as often as needed to insure that the project is being conducted in compliance with our institution and with DHHS regulations.  This institution has an Assurance on file with the Office for Human Research Protection. The Assurance Number is IRB00000446.  Cc: HSC No.  12/04/2018  Jessica Parsons < >  POLITICAL SCIENCE   Do Universities Discriminate Against Conservative Students?    Douglas Ahler < >, Advisor   2018.23806     220  The application that you submitted to this office in regard to the use of human subjects in the proposal referenced above have been reviewed by the Secretary, the Chair, and two members of the Human Subjects Committee. Your project is determined to be Expedited per 45 CFR § 46.110(7) and has been approved by an expedited review process.  The Human Subjects Committee has not evaluated your proposal for scientific merit, except to weigh the risk to the human participants and the aspects of the proposal related to potential risk and benefit. This approval does not replace any departmental or other approvals, which may be required.  If you submitted a proposed consent form with your application, the approved stamped consent form is attached to this approval notice. Only the stamped version of the consent form may be used in recruiting research subjects.  If the project has not been completed by 06/02/2020 you must request a renewal of approval for continuation of the project. As a courtesy, a renewal notice will be sent to you prior to your expiration date; however, it is your responsibility as the Principal Investigator to timely request renewal of your approval from the Committee.  You are advised that any change in protocol for this project must be reviewed and   221 approved by the Committee prior to implementation of the proposed change in the protocol. A protocol change/amendment form is required to be submitted for approval by the Committee. In addition, federal regulations require that the Principal Investigator promptly report, in writing any unanticipated problems or adverse events involving risks to research subjects or others.  By copy of this memorandum, the chairman of your department and/or your major professor is reminded that he/she is responsible for being informed concerning research projects involving human subjects in the department, and should review protocols as often as needed to insure that the project is being conducted in compliance with our institution and with DHHS regulations.  This institution has an Assurance on file with the Office for Human Research Protection. The Assurance Number is IRB00000446.  Cc: HSC No.  06/05/2019  Jessica Parsons   POLITICAL SCIENCE   Do Universities Discriminate Against Conservative Students? Part Two   Brad Gomez, Advisor   2019.26393    222  The application that you submitted to this office in regard to the use of human subjects in the proposal referenced above have been reviewed by the Secretary, the Chair, and two members of the Human Subjects Committee. Your project is determined to be Expedited per 45 CFR § 46.110(7) and has been approved by an expedited review process.  The Human Subjects Committee has not evaluated your proposal for scientific merit, except to weigh the risk to the human participants and the aspects of the proposal related to potential risk and benefit. This approval does not replace any departmental or other approvals, which may be required.  If you submitted a proposed consent form with your application, the approved stamped consent form is attached to this approval notice. Only the stamped version of the consent form may be used in recruiting research subjects.  If the project has not been completed by No Expirat you must request a renewal of approval for continuation of the project. As a courtesy, a renewal notice will be sent to you prior to your expiration date; however, it is your responsibility as the Principal Investigator to timely request renewal of your approval from the Committee.  You are advised that any change in protocol for this project must be reviewed and approved by the Committee prior to implementation of the proposed change in the   223 protocol. A protocol change/amendment form is required to be submitted for approval by the Committee. In addition, federal regulations require that the Principal Investigator promptly report, in writing any unanticipated problems or adverse events involving risks to research subjects or others.  By copy of this memorandum, the chairman of your department and/or your major professor is reminded that he/she is responsible for being informed concerning research projects involving human subjects in the department, and should review protocols as often as needed to insure that the project is being conducted in compliance with our institution and with DHHS regulations.  This institution has an Assurance on file with the Office for Human Research Protection. The Assurance Number is IRB00000446.  Cc: HSC No.  06/05/2019  Jessica Parsons  POLITICAL SCIENCE   Do Universities Discriminate Against Conservative Students? Part Two   Brad Gomez, Advisor   2019.26393            224 REFERENCES AAUA. 2017. “Ethical Principles for College and University Administrators.” American  Association of University Administrators.  AAUP. 2018. “Academic Freedom of Students and Professors, and Political Discrimination.”  American Association of University Professors. https://www.aaup.org/academic- freedom-students-and-professors-and-political-discrimination (September 28, 2018). Abbot, Traci B. 2009. “Teaching Transgender Literature at a Business College.” Race, Gender &  Class (January): 152-169. Acquisti, Alessandro and Christina M. Fong. 2013. “An Experiment in Hiring Discrimination via  Online Social Networks.” Available at SSRN: http://ssrn.com/abstract=2031979 or  http://dx.doi.org/10.2139/ssrn.2031979. (June 20, 2018).  Adida, Claire L, David D. Laitin, and Marie-Anne Valfort. 2010. "Identifying Barriers to Muslim  Integration in France." Proceedings of the National Academy of Sciences 107 (December): 22384-22390.  Ahmed, Ali M, Lina Andersson, and Mats Hammarstedt. 2010. “Can Discrimination in the  Housing Market Be Reduced by Increasing the Information about the Applicants?” Land  Economics 86 (February): 79–90.   Ahmed, Ali M, Lina Andersson, and Mats Hammarstedt. 2013. “Are Gay Men and Lesbians  Discriminated against in the Hiring Process?” Southern Economic Journal 79 (January):  565–585.   Ahmed, Ali M, Lina Andersson, and Mats Hammarstedt. 2012. “Does Age Matter for  Employability? A Field Experiment on Ageism in the Swedish Labour Market.” Applied  Economics Letters 19 (March): 403–406.   Aldrich, Daniel P, and Michelle A. Meyer. 2015. "Social Capital and Community Resilience."  American Behavioral Scientist 59 (February): 254-269.  Alesina, Alberto, and Eliana La Ferrara. 2002. "Who Trusts Others?" Journal of Public  Economics 85 (August): 207-234. Aslund, Anders. 1998. “Varieties of Transition: The East Europeans and East German  Experience.” Political Science Quarterly 113 (March): 155-157.  Ayres, Ian and Peter Siegelman. 1995. “Race and Gender Discrimination in Bargaining for a  New Car.” American Economic Review 85 (June): 304–321.   Baert, Stijn, Bart Cockx, Niels Gheyle, and Cora Vandamme. 2013. “Do Employers  Discriminate Less If Vacancies Are Difficult to Fill? Evidence from a Field Experiment.”  https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2214896 (June 20, 2018).   225 Bailey, John, Michael Wallace, and Bradley Wright. 2013. “Are Gay Men and Lesbians  Discriminated Against When Applying for Jobs? A Four-City, Internet-Based Field  Experiment.” Journal of Homosexuality 60 (June): 873–894.   Banerjee, Abhijit, Marianne Bertrand, Saugato Datta, and Sendhil Mullainathan. 2009. “Labor  Market Discrimination in Delhi: Evidence from a Field Experiment.” Journal of  Comparative Economics 37 (March): 14–27.   Barr, Abigail. 2004. "Kinship, Familiarity, and Trust: An Experimental Investigation." In  Foundations of Human Sociality: Economic Experiments and Ethnographic Evidence  from Fifteen Small-Scale Societies. Oxford, UK: Oxford University Press on Demand. Barrett, Alan and Yvonne McCarthy. 2008. “Immigrants and Welfare Programmes: Exploring  the Interactions between Immigrant Characteristics, Immigrant Welfare Dependence, and  Welfare Policy.” Oxford Review of Economic Policy 24 (October): 542-559.   Bartels, Larry. “What’s the Matter with What’s the Matter with Kansas?” Quarterly Journal of  Political Science 1 (March): 201-226.  Bartoš, Vojtěch, Michal Bauer, Julie Chytilová, and Filip Matějka. 2013. “Attention  Discrimination: Theory and Field Experiments.” American Economic Review 106 (June):  1437-75. Beamer, Glenn. 1998. “Service Learning: What’s a Political Scientist Doing in Yonkers?” PS:  Political Science & Politics 31 (September):557-561. Belkin, Douglas and Scott Thurm. 2012. “Dean’s List Hiring Spree Fattens College  Bureaucracy—And Tuition.” Wall Street Journal, December 28.   Bertrand, Marianne and Sendhil Mullainathan. 2004. “Are Emily and Greg More Employable  Than Lakisha and Jamal? A Field Experiment on Labor Market Discrimination.”  American Economic Review 94 (June): 991–1013.  Bianchi, Andria. 2017. “Transgender Women in Sport.” Journal of the Philosophy of Sport 44  (May): 229-242.  Blackford, Linda. 2018. "At First, University Cuts Didn't Seem So Bad. Then Leaders Took  Another Look." Kentucky.com, January 17. http://www.kentucky.com/news/local/education/article195203044.html (March 30,  2018).  Blackwell, Morton C. 2018. “The Evil Empire on Campus: Leftist Abuses and Bias.” Campus  Reform. https://www.campusreform.org/img/writings/Left_Bias_and_Abuse.pdf (June  12, 2018).  Blommaert, Lieselotte, Marcel Coenders, and Frank van Tubergen. 2014. “Discrimination of  Arabic-Named Applicants in the Netherlands: An Internet-Based Field Experiment   226  Examining Different Phases in Online Recruitment Procedures.” Social Forces 92  (December): 957–982.  Blundell, Richard. 2001. “Welfare Reform for Low Income Workers.” Oxford Economic Papers  53 (April): 542-559.   Boggs, Roderic V. O, Joseph M. Sellers and Marc Bendick Jr. (1993). “Use of Testing in Civil  Rights Enforcement.” In Clear and Convincing Evidence: Measurement of  Discrimination in America, ed. Michael Fix and Raymond J. Struyk. Washington DC:  The Urban Institute Press.   Boldizar, Janet P, and David M. Messick. 1988. "Intergroup Fairness Biases: Is Ours the Fairer  Sex?" Social Justice Research 2 (June): 95-111.  Booth, Alison and Andrew Leigh. 2010. “Do Employers Discriminate by Gender? A Field  Experiment in Female-Dominated Occupations.” Economics Letters 107 (May): 236- 238.   Booth, Alison L, Andrew Leigh, and Elena Varganova. 2011. “Does Ethnic Discrimination Vary  Across Minority Groups? Evidence from a Field Experiment.” Oxford Bulletin of  Economics and Statistics 74 (August): 547–573.   Borah, Porismita, Kjerstin Thorson, and Hyunseo Hwang. 2015. "Causes and Consequences of  Selective Exposure Among Political Blog Readers: The Role of Hostile Media Perception  in Motivated Media Use and Expressive Participation." Journal of Information  Technology & Politics 12 (April): 186-199. Brehm, John, and Scott Gates. 1997. Working, Shirking and Sabotage: Bureaucratic Response to  a Democratic Public. Ann Arbor: University of Michigan Press.  Brehm, John, and Wendy. M. Rahn. 1997. “Individual-Level Evidence for the Causes and  Consequences of Social Capital.” American Journal of Political Science 41 (July): 999- 1023. Broad, Kendal L. 2002. “GLB+T? Gender/Sexuality Movements and Transgender Collective  Identity (De) Constructions.” International Journal of Sexuality and Gender Studies 7  (October): 241-264.  Broockman, David E. 2013. "Black Politicians Are More Intrinsically Motivated to Advance  Blacks’ Interests: A Field Experiment Manipulating Political Incentives." American  Journal of Political Science 57 (July): 521-536.  Brooks, Arthur C. 2005. "Does Social Capital Make You Generous?" Social Science Quarterly  86 (March): 1-15.  Brown, Anna. 2017. “Republicans, Democrats Have Starkly Different Views on Transgender  Issues.” Pew Research Center. http://www.pewresearch.org/fact   227 tank/2017/11/08/transgender-issues-divide-republicans-and-democrats/ (September 10,  2018).  Bursell, Moa. 2007. “What’s in a Name? A Field Experiment Test for the Existence of Ethnic  Discrimination in the Hiring Process.” SULCIS Reports and Working Papers. Stockholm.  http://www.diva-portal.org/smash/record.jsf?pid=diva2%3A821429&dswid=-4355 (June  20, 2018).  Buckley, William F. 1951. God and Man at Yale: The Superstitions of Academic Freedom.  Washington, DC: Regnery Publishing.   Butler, Daniel M, Christopher F. Karpowitz, and Jeremy C. Pope. 2012. "A Field Experiment on  Legislators’ Home Styles: Service Versus Policy." The Journal of Politics 74 (March):  474-486.  Butler, Daniel M, and David W. Nickerson. 2011. "Can Learning Constituency Opinion Affect  How Legislators Vote? Results from a Field Experiment." Quarterly Journal of Political  Science 6 (August): 55-83.  Butler, Daniel M, and David Broockman. 2011. “Do Politicians Racially Discriminate Against  Constituents?” American Journal of Political Science 55 (July): 463-477. Bychowski, Gabrielle MW. 2017. “Trans Literature: Transgender Histories and Genres of  Embodiment, Medieval and Post-Medieval.” PhD Dissertation, The George Washington  University.    Calhoun, Cheshire. 2007. “Lesbian Philosophy.” Feminist Philosophy (January): 177. Carlsson, Magnus. 2011. “Does Hiring Discrimination Cause Gender Segregation in the  Swedish Labor Market? Feminist Economics 17 (July): 71-102.  Claibourn, Michele P, and Paul S. Martin. 2000. "Trusting and Joining? An Empirical Test of the  Reciprocal Nature of Social Capital." Political Behavior 22 (December): 267-291.  Coleman, James S. 1988. "Social Capital in the Creation of Human Capital." American Journal  of Sociology 94 (January): S95-S120.  Contini, Dalit and Nicola Negri. “Would Declining Exit Rates from Welfare Provide Evidence of  Welfare Dependence in Homogeneous Environments?” European Sociological Review  23 (February): 21-33.  Cooke, Martin. 2009. "A Welfare Trap? The Duration and Dynamics of Social Assistance Use  Among Lone Mothers in Canada." Canadian Review of Sociology/Revue Canadienne de  Sociologie 46 (August): 179-206.  Crawford, Lucas. 2015. “Woolf’s Einfühlung: An Alternative Theory of Transgender Affect.”  Mosaic: a Journal for the Interdisciplinary Study of Literature. (March): 165-181.    228 Cross, Harry, Genevieve Kenney, Jane Mell, and Wendy Zimmerman. 1990. “Employer Hiring  Practices: Differential Treatment of Hispanic and Anglo Job Seekers.” Urban Institute  Report. http://webarchive.urban.org/publications/203655.html (July 1, 2018).  Dafoe, Allan, Baobao Zhang, and Devin Caughey. “Information Equivalence in Survey  Experiments.” Political Analysis 4 (October): 399-416.  Daigneault, Pierre-Marc. 2015. "Ideas and Welfare Reform in Saskatchewan: Entitlement,  Workfare or Activation?" Canadian Journal of Political Science/Revue Canadienne de  Science Politique 48 (April): 147-171.  Demaris, Alfred, and Renxin Yang. 1994. "Race, Alienation, and Interpersonal Mistrust."  Sociological Spectrum 14 (October): 327-349.  D'Souza, Dinesh. 1991. Illiberal Education: The Politics of Race and Sex on Campus. New  York, NY: The Free Press.   Dewatripont, Mathias, Ian Jewitt, and Jean Tirole. 1999. “The Economics of Career Concerns,  Part II: Application to Missions and Accountability of Government Agencies.” Review of  Economic Studies 66 (October): 199-217.  Eagan, Kevin, Ellen Bara Stolzenberg, Abigail K. Bates, Melissa C. Aragon, Maria Ramirez  Suchard, and Cecilia Rios-Aguilar. 2015. “The American Freshman: National Norms Fall  2015.” Cooperative Institutional Research Program at the Higher Education Research  Institute at UCLA. https://www.heri.ucla.edu/monographs/TheAmericanFreshman2015.pdf (September 27,  2018).  Eriksson, Stefan and Dan-Olof Rooth. 2014. “Do Employers Use Unemployment as a Sorting  Criterion When Hiring? Evidence from a Field Experiment.” American Economic Review  104 (March): 1014-1039.   Espinosa, Lorelle I, Jonathan M. Turk, Morgan Taylor, and Hollie M. Chessman. 2019. “Race  and Ethnicity in Higher Education: A Status Report.” The American Council on  Education.  Fain, Paul. 2017. "Deep Partisan Divide on Higher Education." Inside Higher Ed, July 11.  https://www.insidehighered.com/news/2017/07/11/dramatic-shift-most-republicans-now- say-colleges-have-negative-impact (March 1, 2018).  Fenn, Peter. 2017. "The Reverse GI Bill." U.S. News, November 28.  https://www.usnews.com/opinion/thomas-jefferson-street/articles/2017-11-28/the-gop- tax-reform-bill-guts-american-higher-education (January 5, 2018). Fershtman, Chaim, and Uri Gneezy. 2001. "Discrimination in a Segmented Society: An  Experimental Approach." The Quarterly Journal of Economics 116 (February): 351-377.    229 Fingerhut, Hannah. 2017. "Republicans Skeptical of Colleges’ Impact on U.S., but Most See  Benefits for Workforce Preparation" Pew Research Center.  http://www.pewresearch.org/fact-tank/2017/07/20/republicans-skeptical-of-colleges- impact-on-u-s-but-most-see-benefits-for-workforce-preparation/ (March 1, 2018).  Fix, Michael and Raymond J. Struyk. 1993. Clear and Convincing Evidence: Measurement of  Discrimination in America. Washington DC: The Urban Institute Press.   Fosse, Ethan, Neil Gross, and Joseph Ma. 2014. “Political Bias in the Graduate Admissions  process: A Field Experiment.” In Professors and Their Politics, ed. Neil Gross and Solon  Simmons. Baltimore, MD: Johns Hopkins University Press.  Franklin, James and Steve W. DelCastillo. 1991. “Measuring Job Discrimination by Private  Employers against Young Black and Hispanic Seeking Entry Level Work in Denver  Metropolitan Area.” Unpublished report. Denver: University of Colorado.   Freeman, David. 1995. “The Large Welfare State as a System.” The American Economic Review  85 (May): 16-21.  Fukuyama, Francis. 1995. Trust: The Social Virtues and the Creation of Prosperity. New York,  NY: Free Press Paperbacks.  Furstenberg, Frank F, and Mary Elizabeth Hughes. 1995. "Social Capital and Successful  Development Among At-Risk Youth." Journal of Marriage and the Family 57 (August):  580-592.  Gaddis, S. Michael. 2014. "Discrimination in the Credential Society: An Audit Study of Race  and College Selectivity in the Labor Market." Social Forces 93 (November): 1451-1479.  Gaertner, Lowell, and Chester A. Insko. 2000. "Intergroup Discrimination in the Minimal Group  Paradigm: Categorization, Reciprocation, or Fear?" Journal of Personality and Social  Psychology 79 (July): 77-94.  Galarza, Francisco B, and Gustavo Yamada. 2014. “Labor Market Discrimination in Lima, Peru:  Evidence from a Field Experiment.” World Development 58 (June): 83-94.   Galster, George. 1990. “Racial Discrimination in Housing Markets during the 1980s: A Review  of the Audit Evidence.” Journal of Planning Education and Research 9 (July): 165-175.   Gardner, Paul. 1994. “Social Science Versus Ideology: a Reflection on the Work of Christopher  Jencks.” Policy Studies Journal 22 (September): 530-534.  Ghayad, Rand. 2013. “The Jobless Trap.” Unpublished Report.  http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.692.6736&rep=rep1&type=pdf  http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.692.6736 (July 1, 2018).   230 Ginsberg, Benjamin. 2011. The Fall of the Faculty. Oxford University Press.  Glaeser, Edward L, David I. Laibson, Jose A. Scheinkman, and Christine L. Soutter. 2000.  "Measuring Trust." The Quarterly Journal of Economics 115 (August): 811-846.  Goldberg, Zach. 2019. “America’s White Saviors” in Tablet, June 5.  https://www.tabletmag.com/jewish-news-and-politics/284875/americas-white-saviors  (January 13, 2020).  Goldin, Claudia, and Lawrence F. Katz. 1999. "Human Capital and Social Capital: The Rise of  Secondary Schooling in America, 1910–1940." Journal of Interdisciplinary History 29  (April): 683-723.  Goodin, Robert E. 2001. “Work and Welfare: Towards a Post-Productivist Welfare Regime.”  British Journal of Political Science 31 (January): 13-39.  Gray, Rosie. 2017. “How Milo Yiannopoulos’s Berkeley ‘Free Speech Week’ Fell Apart. The  Atlantic, September 22. https://www.theatlantic.com/politics/archive/2017/09/how-milo- yiannopoulos-berkeley-free-speech-week-fell-apart/540867/ (July 1, 2018).  Greene, Jay P. 2010. “Administrative Bloast at American Universities: The Real Reason for  High Costs in Higher Education.” Goldwater Institute Policy Report 239 (August): 1-19.  Gordon, David and Per Nilsson. 2011. “The Ideological Profile of Harvard University Press:  Categorizing 494 Books Published 2000-2010.” Investigating the Apparatus 8  (January): 76-95.  Greene, Steven. 2004. “Social Identity Theory and Party Identification.” Social Science  Quarterly 85 (February): 136-153.  Gross, Neil. 2013. Why Are Professors Liberal and Why Do Conservatives Care? Cambridge,  MA: Harvard University Press.   Hansen, Jorgen, and Magnus Lofstrom. 2009. "The Dynamics of Immigrant Welfare and Labor  Market Behavior." Journal of Population Economics 22 (October): 941-970.  Harris, Kathleen Mullan. 1993. “Work and Welfare Among Single Mothers in Poverty.”  American Journal of Sociology 99 (September): 317-352.  Hatfield, Joe Edward. 2019 “The Queer Kairotic: Digital Transgender Suicide memories and  Ecological Rhetorical Agency.” Rhetoric Society Quarterly 49 (January): 25-48.  Hermes, Grant. 2017. "State Agencies to See Cuts If New Budget Passes." News 9, November  14. http://www.news9.com/story/36839014/state-agencies-to-see-cuts-if-new-budget- passes (January 5, 2018).    231 Hetherington, Marc J, and Thomas J. Rudolph. 2015. Why Washington Won't Work:  Polarization, Political Trust, and the Governing Crisis. Chicago, IL: University of  Chicago Press.  Hines, Sally. 2006. “What’s the Difference? Bringing Particularity to Queer Studies of  Transgender.” Journal of Gender Studies 15 (March): 49-66.   Inbar, Yoel and Joris Lammers. 2012. “Political Diversity in Social and Personality Psychology.”  Perspectives on Psychological Science 7 (September): 496-503.  Iyengar, Shanto, Yphtach Lelkes, Matthew Levendusky, Neil Mahotra, and Sean J. Westwood.  2019. “The Origins and Consequences of Affective Polarization in the United States.”  Annual Review of Political Science 22: 129-146.  Jacquemet, Nicolas and Constantine Yannelis. 2012. “Indiscriminate Discrimination: A  Correspondence Test for Ethnic Homophily in the Chicago Labor Market.” Labour  Economics 19 (December): 824-832.   Jetten, Jolanda, Russell Spears, and Antony SR Manstead. 1996. "Intergroup Norms and  Intergroup Discrimination: Distinctive Self-Categorization and Social Identity Effects."  Journal of Personality and Social Psychology 71 (December): 1222-1233.  Jolson, Marvin A. 1974. “Employment Barriers in Marketing.” Journal of Marketing (April): 67- 69.  Kaas, Leo and Christian Manger. 2012. “Ethnic Discrimination in Germany’s Labour Market: A  Field Experiment.” German Economic Review 13 (February): 1-20.   Kawachi, Ichiro. 1999. "Social Capital and Community Effects on Population and Individual  Health." Annals of the New York Academy of Sciences 896 (December): 120-130.  Kawachi, Ichiro, Bruce P. Kennedy, and Roberta Glass. 1999. "Social Capital and Self-Rated  Health: A Contextual Analysis." American Journal of Public Health 89 (August): 1187- 1193.  Kawachi, Ichiro, Bruce P. Kennedy, Kimberly Lochner, and Deborah Prothrow-Stith. 1997.  "Social Capital, Income Inequality, and Mortality." American Journal of Public Health  87, no. 9: 1491-1498.  Keegan, Cael M. 2013. “Moving Bodies: Sympathetic Migrations in Transgender Narrativity.”  Genders 57 (March).   Kelly, Maura. 2010. “Regulating the Reproduction and Mothering of Poor Women: The  Controlling Image of the Welfare Mother in Television News Coverage of Welfare  Reform.” Journal of Poverty 14 (January): 76-96.    232 Kemp, Sawyer K. 2019. “In That Dimension Grossly Clad: Transgender Rhetoric,  Representation, and Shakespeare.” Shakespeare Studies 47: 120-123.  Kennedy, Bruce P, Ichiro Kawachi, and Elizabeth Brainerd. 1998. "The Role of Social Capital  in the Russian Mortality Crisis." World Development 26 (November): 2029-2043.  Kennedy, Bruce P, Ichiro Kawachi, Deborah Prothrow-Stith, Kimberly Lochner, and Vanita  Gupta. 1998b. "Social Capital, Income Inequality, and Firearm Violent Crime." Social  Science & Medicine 47 (July): 7-17.  Kimball, Roger. 1990.Tenured Radicals: How Politics Has Corrupted Our Higher Education.  New York, NY: Harper and Row.  Klein, Daniel, and Charlotta Stern. 2009. “Groupthink in Academia: Majoritarian Departmental  Politics and the Professional Pyramid.” In The Politically Correct University, eds. Robert  Maranto, Richard Redding, and Frederick Hess. Washington, DC: American Enterprise  Institute Press.  Knack, Stephen. 2002. "Social Capital and the Quality of Government: Evidence from the  States." American Journal of Political Science 46 (October): 772-785.  Knack, Stephen, and Philip Keefer. 1997. "Does Social Capital Have an Economic Payoff? A  Cross-Country Investigation." The Quarterly Journal of Economics 112 (November):  1251-1288.  Kroft, Kory, Fabian Lange, and Matthew J. Notowidigdo. 2013. “Duration Dependence and  Labor Market Conditions: Evidence from a Field Experiment.” The Quarterly Journal of  Economics 128 (June): 1123-1167.   Ladd, Jonathan M. 2011. Why Americans Hate the Media and How It Matters. Princeton, NJ:  Princeton University Press.  La Due Lake, Ronald, and Robert Huckfeldt. 1998. "Social Capital, Social Networks, and  Political Participation." Political Psychology 19 (September): 567-584.  Lahey, Joanna N. 2008. “Age, Women, and Hiring: An Experimental Study.” Journal of Human  Resources 43 (January): 30-56.   Lawson, Hal. 2009. “Coercive and Enabling Bureaucracies: A Summary of the Main  Differences.” Invited Brief, University of Albany.  Levinson, Rachel. 2007. “Academic Freedom and the First Amendment.” Presentation to the  AAUP Summer Institute. American Association of University Professors.  https://www.aaup.org/our-work/protecting-academic-freedom/academic-freedom-and- first-amendment-2007 (September 25, 2018).    233 Levitt, Barry S. 2015. "Discrimination and the Distrust of Democratic Institutions in Latin  America." Politics, Groups, and Identities 3 (July): 417-437.  List, John A. 2004. “The Nature and Extent of Discrimination in the Marketplace: Evidence from  the Field.” The Quarterly Journal of Economics 119 (February): 49-89.   Lu, Rachel. 2017. “The Case for Conservative Universities.” The Week, March 13.  http://theweek.com/articles/759624/case-conservative-universities (June 10, 2018).  Maranto, Robert, Richard E. Redding, and Frederick M. Hess. 2009. "The PC University:  Questions Not Asked." In The Politically Correct University: Problems, Scope, and  Reforms. ed. Maranto, Robert, and Richard E. Redding. Washington DC: The AEI Press.  Maranto, Robert and Matt Woessner. 2017. “Why Conservative Fears of Campus Indoctrination  Are Overblown.” The Chronicle of Higher Education, July 31.  https://www.chronicle.com/article/Why-Conservative-Fears-of/240804  Maurer-Fazio, Margaret. 2012. “Ethnic Discrimination in China’s Internet Job Board Labor  Market.” IZA Journal of Migration 1(December): 1-24.   McArdle, Megan. 2017. "Conservatives Are Souring on Colleges. Blame Colleges." Bloomberg  View, July 11. https://www.bloomberg.com/view/articles/2017-07-11/conservatives-are- souring-on-colleges-blame-colleges (January 5, 2018).   McGinnity, Frances, Jacqueline Nelson, Pete Lunn, and Emma Quinn. 2009. “Discrimination in  Recruitment.” The Equality Authority and the Economic and Social Research Institute.  https://www.lenus.ie/bitstream/handle/10147/76641/Discrimination_in_Recruitment(1).P DF;jsessionid=46D5FC2858A48B36FC407ED24775A3B4?sequence=1 (July 1, 2018).  McIntyre, Shelby, Dennis J. Moberg, and Barry Z. Posner. 1980. “Preferential Treatment in  Preselection Decisions according to Race and Sex.” Academy of Management Journal,  23 (December): 738-49.   Milkman, Katherine L, Modupe Akinola, and Dolly Chugh. 2012. "Temporal Distance and  Discrimination: An Audit Study in Academia." Psychological Science 23 (July): 710- 717.  Morgan, David R, and Kenneth Kickham. 1999. "Work and Welfare in the American States:  Analyzing the Effects of the JOBS Program." Political Research Quarterly 52  (December): 867-883.  Moss-Racusin, Corinne A., John F. Dovidio, Victoria L. Brescoll, Mark J. Graham, and Jo  Handelsman. 2012. "Science Faculty’s Subtle Gender Biases Favor Male  Students." Proceedings of the National Academy of Sciences 109 (October): 16474- 16479.    234 Muller, Edward N, and Mitchell Seligson. 1994. “Civic Culture and Democracy: The Question  of Causal Relationships.” American Political Science Review 88 (September): 635–652.  Murphy, Timothy F. 2015. “Assisted Gestation and Transgender Women.” Bioethics 29 (July):  389-397.  Namaste, Viviane. 2009. “Undoing Theory: The ‘Transgender Question’ and the Epistemic  Violence of Anglo-American Feminist Theory.” Hypatia 24 (August): 11-32.  National Center for Education Statistics. 2017. “Race/Ethnicity of College Faculty.”  https://nces.ed.gov/fastfacts/display.asp?id=61  Neumark, David, Roy J. Bank, and Kyle D. Van Nort. 1996. "Sex Discrimination in Restaurant  Hiring: An Audit Study." The Quarterly Journal of Economics 111 (August): 915-941.  Newman, Jerry M. 1978. “Discrimination in Recruitment: An Empirical Analysis.” Industrial  and Labor Relations Review 32 (October): 15-23.   Newport, Frank, and Brandon Busteed. 2017. "Why Are Republicans Down on Higher Ed?"  Gallup. http://news.gallup.com/poll/216278/why-republicans-down-higher.aspx (March  1, 2018).  Nunley, John M, Adam Pugh, Nicholas Romero, and R. Alan Seals. 2014. “An Examination of  Racial Discrimination in the Labor Market for Recent College Graduates: Estimates from  the Field.” Working Paper. http://cla.auburn.edu/econwp/archives/2014/2014-06.pdf  (June 20, 2018).  Nunnally, Shayla C. 2012. Trust in Black America: Race, Discrimination, and Politics. New  York, NY: NYU Press.  Office of the Governor of West Virginia. 2017. "GOP Budget Slashes Higher Education and  Threatens to Close Schools". Office of the Governor, Jim Justice.  https://governor.wv.gov/News/press-releases/2017/Pages/GOP-Budget-Slashes-Higher- Education-and-Threatens-to-Close-Schools.aspx (January 5, 2018).  Philip Oreopoulos. 2011. “Why Do Skilled Immigrants Struggle in the Labor Market? A Field  Experiment with Thirteen Thousand Resumes.” American Economic Journal: Economic  Policy 3 (November): 148-171.   Pager, Devah. 2003. "The Mark of a Criminal Record." American Journal of Sociology 108  (March): 937-975.  Parcel, Toby L, and Elizabeth G. Menaghan. 1993. "Family Social Capital and Children's  Behavior Problems." Social Psychology Quarterly 56 (June): 120-135.  Patacchini, Eleonora, Giuseppe Ragusa, and Yves Zenou. 2012. “Unexplored Dimensions of   235  Discrimination in Europe: Religion, Homosexuality and Physical Appearance.”  Unpublished manuscript:  http://www.frdb.org/upload/file/FRDB_Rapporto_PATACCHINI.pdf. (June 20, 2018).  Patterson, Orlando. 1999. "Liberty Against the Democratic State: On the Historical and  Contemporary Sources of American Distrust." Democracy and Trust 28 (October): 151- 207.  Pearson, A. Fiona. 2007. “The New Welfare Trap: Case Managers, College, Education, and  TANF Policy.” Gender & Society 21 (October): 723-748.  Pedris, Lalini. 2018. “To Understand the Rise of Campus Activism, Listen to These Students.”  The Aspen Institute. https://www.aspeninstitute.org/blog-posts/understanding-rise- student-activism-college-campuses/ (September 25, 2018).  Petit, Pascale. 2007. “The Effects of Age and Family Constraints on Gender Hiring  Discrimination: A Field Experiment in the French Financial Sector.” Labour Economics,  14 (June): 371-391.   Pew Research Center. 2017. "Sharp Partisan Divisions in Views of National Institutions." Pew  Research Center.http://www.people-press.org/2017/07/10/sharp-partisan-divisions-in- views-of-national-institutions/ (March 20, 2018).   Pixley, Jocelyn. “Combining Work and Welfare: Arguing Against Basic Income.” Just Policy: A  Journal of Australian Social Policy 4 (September): 530-534.  Plant, Mark W. 1984. "An Empirical Analysis of Welfare Dependence." The American  Economic Review 74 (September): 673-684.  Princeton Review. 2019. “A Day in the Life of a College Administrator.” The Princeton Review.   https://www.princetonreview.com/careers/40/college-administrator (January 14, 2020).  Putnam, Robert D. 2000. Bowling Alone: The Collapse and Revival of American Community.  New York, NY: Simon and Schuster.  Rahn, Wendy M, and John E. Transue. 1998. "Social Trust and Value Change: The Decline of  Social Capital in American Youth, 1976–1995." Political Psychology 19 (September):  545-565.  Rauch, Jonathan. 1993. Kindly Inquisitors: The New Attacks on Free Thought. Chicago, IL:  University of Chicago Press.  Riach, Peter A, and Judith Rich. 2010. “An Experimental Investigation of Age Discrimination in  the English Labor Market.” Annals of Economics and Statistics/Annales d’Économie et  de Statistique 99/100 (July/December): 169-185.     236 Riach, Peter A, and Judith Rich. 2004. "Deceptive Field Experiments of Discrimination: Are  They Ethical?" Kyklos 57 (August): 457-470.  Robson, Ruthann. 2007. “A Mere Switch or a Fundamental change? Theorizing Transgender  Marriage.” Hypatia 22 (February): 58-70.  Roen, Katrina. 2001. “Transgender Theory and Embodiment: The Risk of Racial  Marginalisation.” Journal of Gender Studies 10 (November): 253-263.  Rooth, Dan-Olof. 2009. “Obesity, Attractiveness, and Differential Treatment in Hiring: A Field  Experiment.” Journal of Human Resources 44 (July): 710-735.   Rosenfeld, Richard, Eric P. Baumer, and Steven F. Messner. 2001. "Social Capital and  Homicide." Social Forces 80 (September): 283-310.  Rothman, Stanley, S. Robert Lichter, and Neil Nevitte. 2005. “Politics and Professional  Advancement Among College Faculty.” The Forum 3 (March): 1-16.  Rotter, Julian B. 1980. "Interpersonal Trust, Trustworthiness, and Gullibility." American  Psychologist 35 (January): 1-7.  Sampson, Robert J, Stephen W. Raudenbush, and Felton Earls. 1997. "Neighborhoods and  Violent Crime: A Multilevel Study of Collective Efficacy." Science 277 (August): 918- 924.  Sawhill, Isabel V. 1988. "Poverty in the US: Why is it so Persistent?" Journal of Economic  Literature 26 (September): 1073-1119.  Schmidt, Peter. 2011. "Experiment Tricked Graduate-Program Officials in Fruitless Search for  Political Bias." The Chronicle of Higher Education, March 21.  https://www.chronicle.com/article/Experiment-Tricked/126845 (June 6, 2018).  Schram, Sanford F, J. Patrick Turbett, and Paul H. Wilken. 1988. "Child Poverty and Welfare  Benefits: A Reassessment with State Data of the Claim that American Welfare Breeds  Dependence." American Journal of Economics and Sociology 47 (October): 409-422.  Simpson, Jeffry A. 2007. "Psychological Foundations of Trust." Current Directions in  Psychological Science 16 (October): 264-268.  Smith, Sandra Susan. 2010. "Race and Trust." Annual Review of Sociology 36 (August): 453- 475.  Smith, Tom W. 1997. "Factors Relating to Misanthropy in Contemporary American Society."  Social Science Research 26 (June): 170-196.  Spencer, Keith A. 2019. “Why Do Conservatives Hate Oberlin So Much?” Salon, June 18.   237  https://www.salon.com/2019/06/18/why-do-conservatives-hate-oberlin-so-much/  (January 15, 2020).  Stryker, Susan, Paisley Currah, and Lisa Jean Moore. 2008. “Introduction: Trans-, Trans, or  Transgender?” Women's Studies Quarterly 1 (October): 11-22.  Sundstrom, William. 2016. “Models with Sample Selection.” RPubs by RStudio.  Sykes, Charles J. 1988. Profscam: Professors and the Demise of Higher Education.  Washington, DC: Regnery Publishing.  Tajfel, Henri. 1970. "Experiments in Intergroup Discrimination." Scientific American 223  (November): 96-103.  Tajfel, Henri. 1979. “Individuals and Groups in Social Psychology.” British Journal of Social  and Clinical Psychology 18 (June): 183-190.  Tajfel, Henri, Michael G. Billig, Robert P. Bundy, and Claude Flament. 1971. "Social  Categorization and Intergroup Behaviour." European Journal of Social Psychology 1  (November): 149-178.  Thein, Amanda Haertling. 2013. “Language Arts Teachers’ Resistance to Teaching LGBT  Literature and Issues.” Language Arts 90 (January): 169-180.  Ting, Michael M. 2016. “Politics and Administration.” American Journal of Political Science 2  (April): 305-319.  Turner, Margery Austin, Michael Fix, and Raymond J. Struyk. 1991. Opportunities Denied,  Opportunities Diminished: Racial Discrimination in Hiring. Washington, DC: The Urban  Institute Press.  Twenge, Jean M, W. Keith Campbell, and Nathan T. Carter. 2014. "Declines in Trust in Others  and Confidence in Institutions Among American Adults and Late Adolescents, 1972– 2012." Psychological Science 25 (October): 1914-1923.  Uslaner, Eric M. 2002. The Moral Foundations of Trust. Cambridge, MA: Cambridge University  Press.  Van Dyke, Nella. 1998. “Hotbeds of Activism: Locations of Student Protest.” Social Problems  45 (May): 205-220.  Vatz, Richard. 2017. "Anti-Conservative Bias in Academe is Real." The Chronicle of Higher  Education, August 30. https://www.chronicle.com/blogs/letters/anti-conservative-bias-in- academe-is-real/ (February 1, 2018).    238 Vedder, Richard. 2018. “’Kill All the Administrators’ (Not Really).” Forbes, May 10.  https://www.forbes.com/sites/richardvedder/2018/05/10/kill-alll-the-administrators-not- really/#7216fb2c6210 (January 14, 2020).  Vinik, Danny. 2017. “80 Percent of Conservatives Think Poor People ‘Have It Easy.” The New  Republic, June 26. https://newrepublic.com/article/118427/pew-survey-conservatives- think-poor-people-have-it-easy (August 12, 2018).  Vinzant, Janet Coble, and Lane Crothers. 1998. Street-Level Leadership: Discretion and  Legitimacy in Front-Line Public Service. Washington, DC: Georgetown University Press.  Walker, Margaret Urban. 2005. “Diotima’s Ghost: The Uncertain Place of Feminist Philosophy  in Professional Philosophy.” Hypatia 20 (July): 153-164.  Wheeler, David R. 2017. “The Hot New Brand in Higher Education.” The Atlantic, February 1.  https://www.theatlantic.com/education/archive/2017/02/the-hot-new-brand-of-higher- education/515316/ (April 20, 2018).  Woessner, Matt and April Kelly-Woessner. 2009a. "Left Pipeline: Why Conservatives Don't Get  Doctorates" in The Politically Correct University: Problems, Scope, and Reforms. ed.  Maranto, Robert, and Richard E. Redding. Washington DC: The AEI Press.  Woessner, Matthew, and April Kelly-Woessner. 2009b. "I Think My Professor is a Democrat:  Considering Whether Students Recognize and React to Faculty Politics." PS: Political  Science & Politics 42 (April); 343-352.  Woessner, Matthew, Robert Maranto, and Amanda Thompson. 2019. “Is Collegiate Political  Correctness Fake News? Relationships between Grades and Ideology.” EDRE Working  Paper No. 2019-15. https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3383704   Wood, Peter W. 2017. "It's Time to Talk About Defunding Universities That Won't Protect Free Speech." The Federalist, February 8. http://thefederalist.com/2017/02/08/time-talk-defunding-universities-wont-defend-free-speech/ (March 1, 2018).  Woodward, Kerry. “The Multiple Meanings of Work for Welfare-Reliant Women.” Qualitative Sociology 31 (June): 149-168.  Wright, Bradley R.E, Michael Wallace, John Bailey, and Allen Hyde. 2013. “Religious  Affiliation and Hiring Discrimination in New England: A Field Experiment.” Research in  Social  Stratification and Mobility 34 (December): 111-126.  Wunder, Christoph and Regina T. Riphahn. 2014. “The Dynamics of Welfare Entry and Exit  Among Natives and Immigrants.” Oxford Economic Papers 66 (April): 580-604.  Yancey, George. 2011. Compromising Scholarship: Religious and Political Bias in American  Higher Education. Waco, TX: Baylor University Press.     239 Yinger, John. 1998. “Evidence on Discrimination in Consumer Markets.” Journal of Economic  Perspectives 12 (June): 23-40.   Yinger, John. 1986. “Measuring Racial Discrimination with Fair Housing Audits: Caught in the  Act.” American Economic Review 75 (December): 881-893.   Zak, Paul J, and Stephen Knack. 2001. "Trust and Growth." The Economic Journal 111 (March:  295-321. Zimman, Lal. 2009. “The Other Kind of Coming Out: Transgender People and the Coming Out  Narrative Genre.” Gender & Language 3 (May). Zimmerman, David and Phillip Levine. 1993. Wellesley College, Department of Economics,  Working Paper 93-07 (November). Zywicki, Todd and Christopher Koopman. 2017. “The Changing of the Guard: the Political  Economy of Administrative Bloat in Higher Education.” George Mason Law and  Economics Review Research Paper (March): 1-35.      240 BIOGRAPHICAL SKETCH  Jessica Parsons is a PhD Candidate in the Department of Political Science at Florida State University. She previously earned a master’s degree and a bachelor’s degree both in political science from the University of Louisville. She has won several awards, including the Florida State University Legacy Fellowship 2015-2020, the 2017 George Pruet Jr. Award for the best graduate student paper in the Department of Political Science at Florida State University, and the University of Louisville Graduate Dean’s Citation, awarded to the top 10% of the graduating class in master’s and doctoral programs for performance beyond a high grade point average.    