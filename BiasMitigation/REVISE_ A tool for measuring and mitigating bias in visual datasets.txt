REVISE: A Tool for Measuring and Mitigating Bias in Visual
Datasets
Angelina Wang Alexander Liu Ryan Zhang Anat Kleiman Leslie
Kim Dora Zhao Iroha Shirai Arvind Narayanan Olga Russakovsky
Abstract Machine learning models are known to per-
petuate and even amplify the biases present in the data.
However, these data biases frequently do not become
apparent until after the models are deployed. Our work
tackles this issue and enables the preemptive analysis
of large-scale datasets. REVISE (REvealing VIsual bi-
aSEs) is a tool that assists in the investigation of a
visual dataset, surfacing potential biases along three
dimensions: (1) object-based, (2) person-based, and (3)
geography-based. Object-based biases relate to the size,
context, or diversity of the depicted objects. Person-
based metrics focus on analyzing the portrayal of peo-
ple within the dataset. Geography-based analyses con-
sider the representation of dierent geographic loca-
tions. These three dimensions are deeply intertwined
in how they interact to bias a dataset, and REVISE
sheds light on this; the responsibility then lies with
the user to consider the cultural and historical con-
text, and to determine which of the revealed biases
may be problematic. The tool further assists the user
by suggesting actionable steps that may be taken to
mitigate the revealed biases. Overall, the key aim of
our work is to tackle the machine learning bias prob-
lem early in the pipeline. REVISE is available at https:
//github.com/princetonvisualai/revise-tool.
Keywords computer vision datasets bias mitigation
tool
1 Introduction
Computer vision dataset bias is a well-known and much-
studied problem. Torralba and Efros (2011) highlighted
the fact that every dataset is a unique slice through
Angelina Wang
E-mail: angelina.wang@princeton.edu
Fig. 1: Our tool takes in as input a visual dataset and its
annotations, and outputs metrics, seeking to produce
insights and possible actions.
the visual world, representing a particular distribution
of visual data. Since then, researchers have noted the
under-representation of object classes (Buda et al., 2017;
Liu et al., 2009; Oksuz et al., 2019; Ouyang et al., 2016;
Salakhutdinov et al., 2011; J. Yang et al., 2014), ob-
ject contexts (Choi et al., 2012; Rosenfeld et al., 2018),
object sub-categories (Zhu et al., 2014), scenes (Zhou
et al., 2017), gender (Kay et al., 2015), gender con-
texts (Burns et al., 2018; J. Zhao et al., 2017), skin
tones (Buolamwini and Gebru, 2018; Wilson et al., 2019),
geographic locations (Shankar et al., 2017) and cul-
tures (DeVries et al., 2019). The downstream eects
of these under-representations range from the more in-
nocuous, like limited generalization of car classiers (Tor-
ralba and Efros, 2011), to the much more serious, like
having deep societal implications in automated facial
analysis (Buolamwini and Gebru, 2018; Hill, 2020). Ef-
forts such as Datasheets for Datasets (Gebru et al.,
2018) have played an important role in encouraging
dataset transparency through articulating the intent
of the dataset creators, summarizing the data collec-
tion processes, and warning downstream dataset users
of potential biases in the data. However, this alone is
not sucient, as there is no algorithm to identify allarXiv:2004.07999v4  [cs.CV]  23 Jul 20212 Angelina Wang et al.
biases hiding in the data, and manual review is not a
feasible strategy given the scale of modern datasets.
Bias detection tool: To mitigate this issue, we pro-
vide an automated tool for REvealing VIsual biaSEs
(REVISE) in datasets. REVISE is a broad-purpose tool
for surfacing the under- and dierent- representations
hiding within visual datasets. For the current explo-
ration we limit ourselves to three sets of metrics: (1)
object-based, (2) person-based and (3) geography-based.
Object-based analysis is most familiar to the com-
puter vision community (Torralba and Efros, 2011), as
many of the popular visual recognition datasets are
object-centric (Everingham et al., 2010; Russakovsky
et al., 2015). Thus, these analyses focus on consider-
ing statistics about object frequency, scale, context, or
diversity of representation.
Person-based analyses began to gain attention with
research showing unequal performance for people of
dierent genders and skin tones (Gebru et al., 2018;
J. Zhao et al., 2017). This line of analysis considers
the representation of people of dierent demographics
within the dataset, and allows the user to assess what
potential downstream consequences this may have in
order to consider how best to intervene. It also builds
on the object-based analysis by considering how the
representation of objects with people of dierent demo-
graphic groups diers.
Finally, geography-based analysis considers the por-
trayal of dierent geographic regions within the dataset;
this is a relatively new but very important conversation
within the community (DeVries et al., 2019; Shankar et
al., 2017). This axis of analysis is deeply intertwined
with the previous two, as geography inuences both
the types of objects that are represented, as well as the
dierent people that are pictured.
We imagine two primary use cases for our tool: (1)
dataset builders can use the actionable insights pro-
duced by our tool during the process of dataset compi-
lation to guide the direction of further data collection,
and (2) dataset users who train models can use the tool
to understand what kinds of biases their models may
inherit as a result of training on a particular dataset.
Example Findings: REVISE automatically surfaces
a variety of metrics that highlight unrepresentative or
anomalous patterns in the dataset. To validate the use-
fulness of the tool, we have used it to analyze several
datasets commonly used in computer vision: COCO (Lin
et al., 2014), OpenImages (Krasin et al., 2017),
YFCC100m (Thomee et al., 2016), and BDD100K (Yu
et al., 2020). Some examples of the kinds of automatic
insights our tool has found include:{In the object detection dataset COCO (Lin et al.,
2014), some objects, e.g., airplane ,bedandpizza ,
are frequently large in the image. This is because
fewer images of airplanes appear in the sky (far
away; small) than on the ground (close-up; large).
This may be a problem since object size plays a key
role in recognition accuracy. One mitigation is to
query for images of airplane appearing in scenes
ofmountains, desert, sky .
{The OpenImages dataset (Krasin et al., 2017) de-
picts a large number of people who are too small in
the image for human annotators to determine their
gender; nevertheless, we found that annotators in-
fer that they are male 69% of the time, especially in
scenes of outdoor sports fields, parks . Com-
puter vision researchers might want to exercise cau-
tion with these gender annotations so they don't
propagate into the model.
{In the computer vision and multimedia dataset
YFCC100m (Yahoo Flickr Creative Commons 100
million) (Thomee et al., 2016) images come from
196 dierent countries. However, we estimate that
for around 47% of those countries | especially in
developing regions of the world | the images are
predominantly photos taken by visitors to the coun-
try rather than by locals, potentially resulting in a
stereotypical portrayal.
A benet of our tool is that a user doesn't need to
have specic biases in mind, as these can be hard to enu-
merate. Rather, the tool automatically surfaces unusual
patterns. REVISE cannot automatically say which of
these patterns, or lack of patterns, are problematic, and
leaves that analysis up to the user's judgment and ex-
pertise. We note that \bias" is a contested term, and
while our tool seeks to surface a variety of ndings that
are interesting to dataset creators and users, not all
may be considered forms of bias by everyone.
2 Related Work
Data collection: Visual datasets are constructed in
various ways, with the most common being through
keyword queries to search engines, whether singular
(e.g., ImageNet (Russakovsky et al., 2015)) or pairwise
(e.g., COCO (Lin et al., 2014)), or by scraping web-
sites like Flickr (e.g., YFCC100m (Thomee et al., 2016),
OpenImages (Krasin et al., 2017)). There is extensive
preprocessing and cleaning done on the datasets. Hu-
man annotators, sometimes in conjunction with auto-
mated tools (Zhou et al., 2017), then assign various
labels and annotations. Dataset collectors put in sig-
nicant eort to deal with problems like long-tails toREVISE: A Tool for Measuring and Mitigating Bias in Visual Datasets 3
ensure a more balanced distribution, and intra-class di-
versity by doing things like explicitly seeking out non-
iconic images beyond just the object itself in focus.
Dataset Bias: Rather than pick a single denition,
we adopt an inclusive notion of bias and seek to high-
light ways in which the dataset builder can monitor and
control the distribution of their data. Proposed ways
to deal with dataset bias include cross-dataset anal-
ysis (Torralba and Efros, 2011) and having the ma-
chine learning community learn from data collection
approaches of other disciplines (Brown, 2014; Jo and
Gebru, 2020). Recent work (Prabhu and Birhane, 2020)
has looked at dataset issues related to consent and jus-
tice; the authors advocate for enforcing Institutional
Review Board (IRB) approval for large scale datasets.
Although we have limited the scope of our work to the
contents of the dataset itself, there are much broader
questions of fairness to be considered regarding the role
that datasets play (Denton et al., 2020; Paullada et al.,
2020). Constructive solutions will need to combine au-
tomated analysis with human judgement as automated
methods cannot yet understand things like the histori-
cal context that led to an observed statistical imbalance
in the dataset. Our work takes this approach by auto-
matically supplying a host of new metrics for analyzing
a dataset along with actions that can be taken to miti-
gate these ndings. However, the responsibility lies with
the user to select next steps. The tool is open-source,
lowering the resource and eort barrier to creating eth-
ical datasets (Jo and Gebru, 2020).
Computer vision tools: Hoiem et al. (2012) built
a tool to diagnose the weaknesses of object detector
models in order to help improve them. More recently,
tools in the video domain (Alwassel et al., 2018) are
looking into forms of dataset bias in activity recognition
(Sigurdsson et al., 2017). We similarly in spirit hope to
build a tool that will, as one goal, help dataset curators
be aware of the patterns and biases present in their
datasets so they can iteratively make adjustments.
Algorithmic fairness: In addition to looking at how
models trained on one dataset generalize poorly to oth-
ers (Tommasi et al., 2015; Torralba and Efros, 2011),
many more forms of dataset bias are being increasingly
noticed in the fairness domain (Caliskan et al., 2017;
Mehrabi et al., 2019; K. Yang et al., 2020). There has
been signicant work looking at how to deal with this
from the algorithm side (Dwork et al., 2012; Dwork
et al., 2017; Khosla et al., 2012; Wang et al., 2020)
with varying denitions of fairness (Gajane and Pech-
enizkiy, 2017; Hardt et al., 2016; Kilbertus et al., 2017;Pleiss et al., 2017; Zhang et al., 2018) that are often
deemed to be mathematically incompatible with each
other (Chouldechova, 2017; Kleinberg et al., 2017), but
in this work, we look at the problem earlier in the
pipeline from the dataset side.
Automated bias detectors: Facebook's Fairness Flow
(Facebook AI, 2021) focuses on assessing model predic-
tions for their contextual fairness, though also looks
at the labels themselves. IBM's AI Fairness 360 (Bel-
lamy et al., 2018) similarly discovers biases in machine
learning models, and also looks into the datasets. How-
ever, its look into dataset biases is limited in that it
rst trains a model on that dataset, then interrogates
this trained model with specic questions. On the other
hand, REVISE looks directly at the dataset and its
annotations to discover model-agnostic patterns. The
Dataset Nutrition Label (Holland et al., 2018) is a re-
cent project that assesses machine learning datasets.
Dierently, our approach works on visual rather than
tabular data which allows us to use additional computer
vision techniques, and goes deeper in terms of present-
ing a variety of graphs and statistical results. Swinger
et al. (2019) look at automatic detection of biases in
word embeddings, but we look at patterns in visual im-
ages and their annotations. Amazon SageMaker Clar-
ify (Amazon, 2021) also works to detect bias in train-
ing data, but only along the person-based axis, and not
object nor geography. Similarly, Google's Know Your
Data (Google People + AI Research, 2021) also aims
to help mitigate bias issues in image datasets. However,
their tool currently only works on TensorFlow image
datasets, whereas REVISE will work for any local im-
age dataset. This has the benet of allowing dataset
creators to iteratively query our tool during the devel-
opment process of their dataset, as well as dataset users
to apply it to a private or proprietary dataset.
3 Tool Overview
REVISE is a general tool intended to yield insights at
varying levels of granularity. As an input, it simply re-
quires an image dataset and any available annotations.
The tool has the ability to fully automatically com-
pute a host of metrics, to be described in Sec. 4.1, 5.1,
and 6.1, broken down by the axes of object, person,
and geography. Which metrics can be computed de-
pends on the annotations available, e.g., gender labels
are required to compute statistics about dierent gen-
der representations. To perform analyses beyond just
the annotations provided, we also use external tools
and pretrained models, such as Fasttext language de-
tection (Joulin, Grave, Bojanowski, Douze, et al., 2016;4 Angelina Wang et al.
Joulin, Grave, Bojanowski, and Mikolov, 2016), Places
scene detection (Zhou et al., 2017), and automatic fea-
ture extraction (Idelbayev, 2019) to derive some of our
metrics, and acknowledge these models themselves may
contain biases. The metrics are often situated to pro-
vide a user with anomalous patterns, such as when
the size distribution of an object class is highly non-
uniform, and correspondingly provide automatic data-
driven insights on how one might correct for this dis-
tribution. However, a metric itself has no normative
claim on its own; ultimately it is up to the user to
determine whether the automatically-surfaced patterns
deviate enough from an intended distribution that this
would be a problem for the downstream application of
models trained on the dataset.
Tool: Practically, REVISE takes the form of a Jupyter
notebook interface that allows interactive exploration,
as shown in Figs. 2 and 3. For privacy reasons, all
analyses are run on a user's local machine. By default,
the code to compute the metrics are largely abstracted
away. However, all code is open-sourced such that a user
can perform any personal customization to the metrics
to t their intended use-case. After multiple rounds of
iterations with potential users, we have added a number
of features to increase adoptability and usability of the
tool. For one, we have created a video that will demon-
strate to potential users what the interface of the tool
looks like, and allow them to get a sense of whether
this would be useful for their purposes. We have also
included a feature that automatically generates a sum-
mary PDF as a result of running the tool and exploring
the notebook. This supplements the dynamic nature
of the tool with a static component that summarizes
the key ndings. Additionally, the biggest hurdle for a
user to get started with their own dataset is the pro-
cess of setting their dataset up to be in the standard-
ized format that our tool requires. To this end, we have
created a comprehensive testing script that provides in-
formative feedback to ensure a user has inputted their
dataset in the proper format before it is run through
the tool.
Axes of analyses: The analyses that can ultimately
be performed depend on the annotations available:
1.Object-based insights require instance labels and,
if available, their corresponding bounding boxes and
object category. Datasets are frequently collected
together with manual annotations, but we also use
automated computer vision techniques to infer some
semantic labels, like scenes.
2.Person-based insights require sensitive attribute
labels of the people in the images. The tool is general
Fig. 2: Example interface of a metric in our notebook.
A dropdown menus allow for sorting by p-value or ra-
tio, and a sliding bar allows adjusting the number of
examples shown in the graph. Best viewed in color.
Fig. 3: Interface for exploring datasets with geography
annotations. Interactive features allow viewing both im-
age distribution by geography, as well as a bubble show-
ing the labels of a specic image.
enough that given labels of any grouping of people,
such as racial groups, the corresponding analyses
can be performed. If the attribute labels are ordi-
nal, such as quantized age or skin tone, additional
regression analyses are available.
3.Geography-based insights exibly allow for la-
bels in two possible formats: 1) region labels as strings,
e.g., \Portugal", \Nigeria", or 2) GPS latitude and
longitude coordinates. By default the tool will use
a global map, but users can override this with their
own GeoJSON le.1The geography labels are ana-
lyzed in conjunction with the previously mentioned
object and demographic labels, as well as external
data source annotations. These can be at either the
image-level, e.g., language of an image caption or
region level, e.g., population size.
1GeoJSON is a JSON-based standard for encoding bound-
ary and region information through GPS data. GeoJSON les
for many geographic regions are easily downloadable online,
and can be readily converted from shapeles, another type of
geographic boundary le.REVISE: A Tool for Measuring and Mitigating Bias in Visual Datasets 5
In the rest of the paper we will describe some in-
sights automatically generated by our tool on various
datasets, and potential actions that can be taken. The
metrics are all run fully automatically, but based on the
statistically signicant results that are surfaced by the
tool, we pick out the interesting ndings to present in
this paper that demonstrate the avor of insight each
metric will provide.
4 Object-Based Analysis
We begin with an object-based approach to gain a ba-
sic understanding of a dataset. Much visual recognition
research has centered on recognizing objects as the core
building block (Everingham et al., 2010), and a number
of object recognition datasets have been collected e.g.,
Caltech101 (Fei-Fei et al., 2004), PASCAL VOC (Ev-
eringham et al., 2010), ImageNet (Deng et al., 2009;
Russakovsky et al., 2015). In Section 4.1 we introduce
the metrics reported by REVISE; in Section 4.2 we dive
into the actionable insights we surface as a result, all
summarized in Table 1.
4.1 Object-based Metrics
Of the metrics we will introduce, several (e.g., object
counts, duplicate annotations, object scale) are com-
monly used by dataset collectors; others (e.g., scene or
appearance diversity) are sometimes used during ad-
hoc dataset examination but rarely quantied.
When the number of labels is very large (e.g., Open-
Images contains 19,995) dataset analysis at the object
level can be intractable to interpret. This motivates the
need for higher-level supercategories : e.g., an appliance
supercategory encompasses the more granular instances
ofoven ,refrigerator , and microwave in COCO (Lin
et al., 2014). Most datasets, however, do not contain
explicit mappings from labels to supercategories like
COCO does. REVISE automatically bins labels to a
set of predened supercategories using the cosine simi-
larity of word embeddings (Honnibal et al., 2020). Re-
sults of auto-generated mappings are returned to the
user, sorted by condence, and the user is free to over-
ride any of the mappings. In a random sample of labels
from the OpenImages dataset mapped to the COCO
supercategories, human validation nds this automatic
binning strategy to be appropriate on 44 of 50 labels.
4.1.1 Object counts
Object counts in the real world tend to naturally follow
a long-tail distribution (Ouyang et al., 2016; Salakhut-
dinov et al., 2011; J. Yang et al., 2014). As for object
Fig. 4: Oven and refrigerator counts fall below the
median of object classes in COCO; however, they are
actually over-represented within the appliance category.
counts in datasets, there are two main views: reecting
the natural long-tail distribution (e.g., in SUN (Xiao
et al., 2010)) or approximately equal balancing (e.g., in
ImageNet (Russakovsky et al., 2015)). Either way, the
rst-order statistic when analyzing a dataset is to com-
pute the per-category counts and verify that they are
consistent with the target distribution. By computing
how frequently an object is represented both within its
supercategory, as well as among all objects, this allows
for a ne-grained look at frequency statistics: for ex-
ample, while the oven and refrigerator objects fall
below the median number of instances for an object
class in COCO, it is nevertheless notable that both of
these objects are around twice as frequent as the aver-
age object from the appliance class.
4.1.2 Duplicate annotations
A common issue with object dataset annotation is the
labeling of the same object instance with two names
(e.g., cupand mug), which is especially problematic in
free-form annotation datasets such as Visual Genome (Kr-
ishna et al., 2016). In datasets with closed-world vocab-
ulary, image annotation is commonly done for a single
object class at a time causing confusion when the same
object is labeled as both trumpet andtrombone (Rus-
sakovsky et al., 2015). While these occurrences are man-
ually ltered in some datasets, automatic identica-
tion of such pairs is useful for both dataset curators
(to remove errors) and to dataset users (to avoid over-
counting of either object). REVISE automatically iden-
ties such object instances. In the OpenImages dataset
(Krasin et al., 2017) some examples of automatically de-
tected pairs include bagel and doughnut ,jaguar and
leopard , and orange and grapefruit . In each case,
the two labels are distinct (although visually similar)
concepts, suggesting annotation errors.6 Angelina Wang et al.
Table 1: Object-based summary: for image content and object annotations of COCO
Metric Example insight Example action
Object counts
(Sec. 4.1.1)Within the supercategory appliance ,oven and
refrigerator are overrepresented and toaster is un-
derrepresentedQuery for more toaster images
Duplicate
annotations
(Sec. 4.1.2)The same object is frequently labeled as both doughnut
and bagelManually reconcile the duplicate annota-
tions
Object scale
(Sec. 4.1.3)Airplane is overrepresented as very large in images, as
there are few images of airplanes smaller and ying in
the skyQuery more images of airplane with kite,
since they're more likely to have a small
airplane
Object
co-occurrences
(Sec. 4.1.4)Person appears more with unhealthy food likecake or
hot dog than broccoli ororangeQuery for more images of people with a
healthier food
Scene diversity
(Sec. 4.1.5)Baseball glove doesn't occur much outside of sports
eldsQuery images of baseball glove in dier-
ent scenes like a sidewalk
Appearance
diversity
(Sec. 4.1.6)The appearance of furniture objects become more
varied when they come from scenes like water, ice,
snow and outdoor sports fields, parks rather than
predominantly from home or hotel .Query more images of furniture in
outdoor sports fields, parks , since this
scene is more common than water, ice,
snow, and still contributes appearance di-
versity
4.1.3 Object scale
It is well-known that object size plays a key role in
object recognition accuracy (Hoiem et al., 2012; Rus-
sakovsky et al., 2015), as well as semantic importance
in an image (Berg et al., 2012). While many quanti-
zations of object scale have been proposed (Hoiem et
al., 2012; Lin et al., 2014), we aim for a metric that is
both comparable across object classes and invariant to
image resolution to be suitable for dierent datasets.
Thus, for every object instance we compute the frac-
tion of image area occupied by this instance, and quan-
tize into 5 equal-sized bins across the entire dataset.
This binning reveals, for example, that rather than an
equal 20% for each size, 77% of airplanes and 73% of
pizzas in COCO are extra large ( >9:3% of the image
area).
4.1.4 Object co-occurrence
Object co-occurrence is a known contextual visual cue
exploited by object detection models (Galleguillos et
al., 2008; Oliva and Torralba, 2007), and thus can serve
as an important measure of the diversity of object con-
text. We compute all pairwise object class co-occurrence
statistics within the dataset, and use them both to
identify surprising co-occurrences as well as to gener-
ate potential search queries to diversify the dataset,
as described in Section 4.2. For example, we nd that
in COCO, person appears in 43% of images contain-
ing the food category; however, person appears in a
smaller percentage of images containing broccoli (15%),carrot (21%), and orange (29%), and conversely a
greater percentage of images containing cake (55%),
donut (55%), and hot dog (56%).
4.1.5 Scene diversity
Building on quantifying the common context of an ob-
ject, we additionally strive to measure the scene diver-
sity directly. To do so, for every object class we consider
the entropy of scene categories in which the object ap-
pears. We use a ResNet-18 (He et al., 2016) trained on
Places (Zhou et al., 2017) to classify every image into
one of 16 scene groups,2and identify objects like person
that appear in a higher diversity of scenes versus ob-
jects like baseball glove that appear in fewer kinds
of scenes (almost all baseball elds). This insight may
guide dataset creators to further augment the dataset,
as well as guide dataset users to want to test if their
models can support out-of-context recognition on the
objects that appear in fewer kinds of scenes, for exam-
ple baseball gloves in a street.
4.1.6 Appearance diversity
Finally, we consider the appearance diversity (i.e., intra-
class variation) of each object class, which is a primary
2Because top-1 accuracy for even the best model on all 365
scenes is 55.19%, but top-5 accuracy is 85.07%, we use the
less granular scene categorization at the second tier of the
dened scene hierarchy here. For example, aquarium ,church
indoor , and music studio fall into the scene group of indoor
cultural .REVISE: A Tool for Measuring and Mitigating Bias in Visual Datasets 7
challenge in object detection (Yao et al., 2017). We
use a ResNet-110 network (Idelbayev, 2019) trained on
CIFAR-10 (Krizhevsky, 2009) to extract a 64-dimensional
feature representation of every instance bounding box,
resized to 32x32 pixels. We rst validate that distances
in this feature space correspond to semantically mean-
ingful measures of diversity. To do so, on the COCO
dataset we compute the average distance with n=
500;000 between two object instances of the same class
(5:911:44), and verify that it is smaller than the av-
erage distance between two object instances belonging
to dierent classes but the same supercategory (6 :24
1:42), with a Cohen's D eect size of :23 and further
smaller than the average distance between two unre-
lated objects (6 :481:44), with a Cohen's D eect size
of:17. This metric allows us to identify individual ob-
ject instances that contribute the most to the diversity
of an object class, and informs our interventions in the
next section.
4.2 Object-based Actionable Insights
The metrics of Section 4.1 help surface biases or other
issues, but it may not always be clear how to address
them. We strive to mitigate this concern by providing
examples of meaningful, actionable, and useful steps to
guide the user.
For duplicate annotations, the remedy is straight-
forward: perform manual cleanup of the data, e.g., as in
Appendix E of (Russakovsky et al., 2015). For the oth-
ers the path is less straight-forward. For datasets that
come from web queries, following the literature (Ever-
ingham et al., 2010; Lin et al., 2014; Russakovsky et
al., 2015) REVISE denes search queries of the form
\XXand YY," where XXcorresponds to the target ob-
ject class, and YYcorresponds to a contextual term (an-
other object class, scene category, etc.). REVISE ranks
all possible queries to identify the ones that are most
likely to lead to the target outcome, and we investigate
this approach more thoroughly in Appendix C.
For example, within COCO, airplanes have low
diversity of scale and are predominantly large in the
images. Our tool identies that smaller airplanes co-
occurred with objects like surfboard and scenes like
mountains, desert, sky (which are more likely to be
photographed from afar). In other words, size matters
by itself, but a skewed size distribution could also be a
proxy for other types of biases. Dataset creators aiming
to diversify their dataset towards a more uniform distri-
bution of object scale can use these queries as a guide.
These pairwise queries can similarly be used to diversify
appearance diversity. Furniture objects appear pre-
dominantly in indoor scenes like home or hotel , so
Fig. 5: The top shows the tradeo for furniture in
COCO between how much scenes increase appearance
diversity (our goal) and how common they are (ease
of collecting this data). To maximize both, outdoor
sports fields, parks would be the most ecient
way of augmenting this category. Water, ice, snow
provides the most diversity but is hard to nd, and home
or hotel is the easiest to nd but provides little diver-
sity. On the bottom are sample images of furniture
from these scenes.
querying for furniture in scenes like water, ice, snow
would diversify the dataset. However, this combination
is quite rare, so we want to navigate the tradeo be-
tween a pair's commonness and its contribution to di-
versity. Thus, we are more likely to be successful if we
query for images in the more common outdoor sports
fields, parks scenes, which also brings a signicant
amount of appearance diversity. The tool provides a vi-
sualization of this tradeo (Fig. 5), allowing the user to
make the nal decision.
5 Person-Based Analysis
We next look into discrepancies in various aspects of
how people of diering demographic attributes are rep-
resented, summarized in Table 2. The datasets we con-
sider here are COCO (Lin et al., 2014), for which we
have gender and skin tone annotations, and OpenIm-
ages (Krasin et al., 2017), for which we have gender an-
notations. In Section 5.1 we explain some of the metrics
that we collect, and in Section 5.2 we discuss possible
actions.
Gender labels: The gender labels in COCO are from J.
Zhao et al. (2017), and their methodology in determin-
ing the gender for an image is that if at least one cap-8 Angelina Wang et al.
tion contains the word \man" and there is no mention of
\woman", then it is a male image, and vice versa for fe-
male images. We use the same methodology along with
other gendered labels like \boy" and \girl" on OpenIm-
ages' pre-existing annotations of individuals. It is im-
portant to acknowledge that the labels we are using are
those of perceived binary gender, which is not inclusive
of all gender categories. We will use the terms male and
female to refer to binarized socially-perceived gender
expression, and not gender identity nor sex assigned at
birth, neither of which can be inferred from an image.
In Appendix A.1 we consider some of the problems that
arise from using gender labels that have been inferred
in this way.
Skin tone labels: Our skin tone annotations for COCO
come from (D. Zhao et al., 2021), and are numbered 1-6
according to the Fitzpatrick scale (Fitzpatrick, 1988),
where 1 is the lightest and 6 is the darkest. We use per-
ceived skin tone as a poor proxy for race, and acknowl-
edge that this risks reifying a particular inaccurate con-
ception of race (Hanna et al., 2020). We consider skin
tone as an ordinal variable, and analyze trendlines that
result as we increase or decrease along this axis.
5.1 Person-based Metrics
In this section, we will give representative ndings for
each metric that demonstrate the kind of insight our
tool can provide. We start out by considering both gen-
der and skin tone for COCO in Sec. 5.1.1 and 5.1.2, be-
fore transitioning to gender in OpenImages in Sec. 5.1.3
and 5.1.4.
5.1.1 Person Prominence
As our rst line of analysis regarding how people of dif-
ferent demographic attributes are represented, we con-
sider the proportion of an image a person takes up, as
well as their distance from the center. We treat these
two measures as a proxy for importance (Berg et al.,
2012), where people who are larger and more to the
center of an image are the focal point. We run the anal-
ysis for COCO on people dierentiated both by gender
and by skin tone. For gender, people who are male tend
to take up more of the image ( :268:213 for male vs
:138:148 for female, with a Cohen's D eect size of
.709) and be closer to the center ( :363:218 for male
vs:510:250 for female, with a Cohen's D eect size
of .627). For people of dierent skin tones, in Fig. 6 we
see that as skin tone increases in darkness, the person
is more likely to take up less of the image, as well as be
further from the center. This indicates a bias against
Fig. 6: In the COCO dataset, as a person's skin tone
increases in darkness, that person is more likely to be
smaller and further from the center. This indicates that
people of darker skin tones are more likely to be in the
background of an image rather than featured promi-
nently. We used Jonckheere's trend test (Jonckheere,
1954) to show there is an a priori ordering to size and
distance values by skin tone with p-values of 2 :11e 7
and:014, respectively.
females and people of darker skin tones as being less
likely to be the focal point of an image.
5.1.2 Contextual representation
Looking beyond just the person themselves, we consider
the contexts that people with dierent demographic
attributes tend to be featured in through the object
groups they cooccur with, and the scenes they appear
in. We rst consider people of two dierent genders in
COCO, and in Fig. 7 see that images with females tend
to be more indoors in scenes like shopping and dining
and with object groups like furniture ,accessory , and
appliance . On the other hand, males tend to be in
more outdoors scenes like sports fields and water,
ice, snow , and with object groups like sports and
vehicle . These trends reect gender stereotypes in many
societies and can propagate into the models. While there
is work on algorithmically intervening to break these as-
sociations, there are often too many proxy features to
robustly do so. Thus it can be useful to intervene at the
dataset creation stage.
Then, we consider these analyses in COCO along
the ordinal variable of skin tone. In Fig. 8 we see sta-
tistically signicant trends according to the Wald test
on a non-zero slope of regression lines where people
with lighter skin tones are more likely to be in home or
hotel scenes and with object groups like furniture ,
and people with darker skin tones are more likely to
be in outdoor transportation scenes and with object
groups like vehicle . In the next metric we dig deeper
into these object categories by considering the particu-
lar objects themselves.REVISE: A Tool for Measuring and Mitigating Bias in Visual Datasets 9
Table 2: Person-based summary: investigating representation of people with dierent demographic attributes.
Metric Example insight Example action
Person
Prominence
(Sec. 5.1.1)As the skin tone of the person in an image increases in
darkness, the person is more likely to be smaller and
further from the center.Collect more images of people of dierent
skintones as the subject of an image rather
than in the background.
Contextual
representation
(Sec. 5.1.2)Males occur in more outdoors scenes and with sports
objects. Females occur in more indoors scenes and with
kitchen objects.Collect more images of females in outdoors
scenes with sports objects, and vice versa
for males.
Instance counts
and distances
(Sec. 5.1.3)In images with musical instrument organ , males are
more likely to be actually playing the organ .Collect more images of females playing
organs .
Appearance
dierences
(Sec. 5.1.4)Males in sports uniforms tend to be playing outdoor
sports, while females in sports uniforms are often in-
doors or in swimsuits.Collect more images of each gender with
sports uniform in their underrepresented
scenes.
Fig. 7: Contextual information of images in COCO by gender, represented by fraction that are in a scene (left)
and have an object from the category (right).
5.1.3 Instance Counts and Distances
Analyzing object instances allows a more granular un-
derstanding of biases in the dataset. For example, in the
previous metric on COCO we found vehicle objects
to occur more with people of darker skin tones, and
furniture with people of lighter skin tones. The spe-
cic vehicle objects that t this trend are motorcycle
and bus, while the specic furniture objects are bed
andcouch .
In OpenImages we nd that objects like cosmetics ,
doll , and washing machine are overrepresented with
females, and objects like rugby ball ,beer ,bicycle
are overrepresented with males. However, beyond just
looking at the number of times objects appear, we alsolook at the distance an object is from a person. We
use a scaled distance measure as a proxy for under-
standing if a particular person, p, and object, o, are
actually interacting with each other in order to derive
more meaningful insight than just quantifying a mutual
appearance in the same image. The distance measure
we dene is
dist=distance between p and o centersparea parea o(1)
to estimate distance in the 3D world, where area pis
measured on a normalized image of total area 1. In
Appendix B we validate this notion that our distance
measure can be used as a proxy interaction. We con-
sider these distances in order to disambiguate between
situations where a person is merely in an image with an10 Angelina Wang et al.
Fig. 8: We t regression lines between co-occurrences
of people with particular skin tones, and scenes and
object categories. We show in the gure example cate-
gories where the Wald test has p < : 05 that the slopes
are non-zero, revealing trends that appear in image con-
text as skin tone changes. On the left, we see that as
an individual's skin tone increases in darkness, they are
less likely to be pictured in home or hotel scenes, and
more likely to be pictured in outdoor transportation
scenes. On the right, we see that for object categories,
people with darker skin tones are less likely to be pic-
tured with furniture objects, and more likely to be
pictured with vehicle objects.
Fig. 9: 5 images from OpenImages for a person (red
bounding box) of each gender pictured with an organ
(blue bounding box) along the gradient of inferred 3D
distances. Males tend to be featured as actually playing
the instrument, whereas females are oftentimes merely
in the same space as the instrument.
object in the background, rather than directly interact-
ing with the object, revealing biases that were not clear
from just looking at the frequency dierences. For ex-
ample, organ (the musical instrument) did not have a
statistically signicant dierence in frequency between
the genders, but does in distance, or under our interpre-
tation, relation. In Fig. 9 we investigate what accounts
for this dierence and see that when a male person is
pictured with an organ, he is likely to be playing it,
whereas a female person may just be near it but not
necessarily directly interacting with it. Through this
analysis we discover something more subtle about how
an object is represented.
Fig. 10: Qualitative interpretation of what the visual
model has learned for the sports uniform andflower
objects between the two genders in OpenImages. \Con-
dent Correct" are the images with the highest con-
dence scores.
5.1.4 Appearance Dierences
We also look into the appearance dierences in images
of each gender with a particular object. This is to fur-
ther disambiguate situations where occurrence counts,
or even distances, aren't telling the whole story. This
analysis is done by (1) extracting FC7 features from
AlexNet (Krizhevsky et al., 2012) pretrained on Places
(Zhou et al., 2017) on a randomly sampled subset of
the images to get scene-level features, (2) projecting
them intopnumber of samples dimensions (as is rec-
ommended in (Hua et al., 2005; Jain and Waller, 1978))
to prevent over-tting, and then (3) tting a Linear
Support Vector Machine to see if it is able to learn
a dierence between images of the same object with
dierent genders. To make sure the female and male
images are actually linearly separable and the classier
isn't over-tting, we look at both the accuracy as well
as the ratio in accuracy between the SVM trained on
the correctly labeled data and randomly shued data.
In Fig. 10 we can see what the Linear SVM has learned
on OpenImages for the sports uniform and flower
categories. For sports uniform , males tend to be rep-
resented as playing outdoor sports like baseball, while
females tend to be portrayed as playing an indoor sport
like basketball or in a swimsuit. For flower , we see an-
other drastic dierence in how males and females are
portrayed, where males pictured with a flower are in
formal, ocial settings, whereas females are in staged
settings or paintings.
5.2 Person-based Actionable Insights
Compared to object-based metrics, the actionable in-
sights for person-based metrics are less concrete and
more nuanced. There is a tradeo between attempt-
ing to represent the visual world as it is versus as weREVISE: A Tool for Measuring and Mitigating Bias in Visual Datasets 11
think it should be. For example, in contemporary so-
cieties, gender representation in various occupations,
activities, etc. is unequal, so it is not obvious that aim-
ing for gender parity across all object categories is the
right approach. Biases that are systemic and histori-
cal are more problematic than others (Bearman et al.,
2009), and this analysis cannot be automated. Further,
the downstream impact of unequal representation de-
pends on the specic models and tasks. Nevertheless,
we provide some recommendations.
A trend that appeared in the metrics is that images
frequently fell in line with common gender and racial
stereotypes. Each group of people was under- or over-
represented in a particular way, and dataset collectors
may want to adjust their datasets to account for these
by augmenting in the direction of the underrepresenta-
tions. Dataset users may want to audit their models,
and look into to what extent their models have learned
the dataset's biases before they are deployed.
6 Geography-Based Analysis
Finally, we turn to the geography of the images. We
consider geography in the context of the object-based
and person-based analyses from before, as well as addi-
tional axes. Geography uniquely interacts with both the
types of objects that appear in images, as well as the
demographics of the people. Because of these interac-
tions, biases and problems around generalization have
been shown to appear (DeVries et al., 2019; Gebru et
al., 2017; Shankar et al., 2017).
In addition to COCO, for which we can derive ge-
ography labels on a subset of the images by querying
the source of the images, i.e., Flickr, we also consider
the global YFCC100m dataset3(Thomee et al., 2016),
and the New York-centric BDD100K (Yu et al., 2020)
self-driving car dataset.4
In Sec. 6.1 we present ndings from our metrics, and
in Sec. 6.2 we discuss what can be done about them.
6.1 Geography-based Metrics
In this section we analyze geography in the context of
objects and people appearances, but also language, in-
come, and weather. For distribution (Sec. 6.1.1), ob-
jects (Sec. 6.1.2), and language (Sec. 6.1.4) we look at
the YFCC100m dataset, for people (Sec. 6.1.3) we look
3We use dierent subsets of the YFCC100m dataset de-
pending on the particular annotations required by each met-
ric.
4We consider the subset of the BDD100K dataset with
images in New York City, which is a majority of the dataset.
Fig. 11: Geographic distribution normalized by popula-
tion in YFCC100m.
at COCO, and then for income (Sec. 6.1.5) and weather
(Sec. 6.1.6) we look at BDD100K. Additionally, our
analysis on geography by income is a case study into
what our automated analyses in conjunction with an
external data source of region-level labels may look like.
One could also imagine plugging in a dierent external
data source, e.g., region-level population size, and the
tool would automatically run the same metrics along
this axis instead.
6.1.1 Geographic distribution
The rst line of analysis is to look at the overall ge-
ographic distribution of a dataset. Researchers have
looked at OpenImages and ImageNet and found these
datasets to be Amerocentric and Eurocentric (Shankar
et al., 2017), with models dropping in performance when
being run on images from underrepresented locales. In
Fig. 11 it immediately stands out that in the global
YFCC100m dataset, the USA is drastically overrepre-
sented compared to most other countries, with the con-
tinent of Africa being very sparsely represented. This
can lead to generalization problems where a model may
perform worse on image from a region it has not seen
as much of (DeVries et al., 2019).
6.1.2 Geography by Object
In the YFCC100m dataset, we have access to image
tags, which we treat as object labels. We combine our
object-based analysis techniques with this geography
data, allowing us to discern if certain labels are over-
or under-represented between dierent areas. We then
begin by considering the frequency with which each im-
age tag appears in the set of a country's tags, compared
to the frequency that same tag makes up in the rest of
the countries. Some examples of over- and under- repre-
sentations include Kiribati with wildlife at 86x, Iran12 Angelina Wang et al.
Table 3: Geography-based summary: looking into the geo-representation of a dataset, and how that diers between
dierent regions
.
Metric Example insight Example action
Geography
distribution
(Sec. 6.1.1)Most images are from the USA, with very few from the
countries of AfricaCollect more images from the countries of
Africa
Geography
by object
(Sec. 6.1.2)Wildlife is overrepresented in Kiribati, and mosque in
IranCompare dataset frequencies to real-world
frequencies; consider collecting other kinds
of images representing these countries
Geography
by people
(Sec. 6.1.3)Underrepresented regions like Africa and South Asia
contain many of the images of people with darker skin
tonesCollect more images from underrepresented
regions to also diversify the people of dif-
ferent skin tones being represented.
Geography
by language
(Sec. 6.1.4)Countries in Africa and Asia that are already under-
represented are frequently represented by non-locals
rather than localsCollect more images taken by locals rather
than visitors in underrepresented countries
Geography
by income
(Sec. 6.1.5)Normalized by square mile, wealthier zip codes have
more images, which also contain a dierent distribu-
tion of labelsCollect more images from zip codes with
lower incomes
Geography
by weather
(Sec. 6.1.6)Northern California has signicantly less snowy images
than New York CityFinetune a model on a weather distribution
most similar to that in which it will be de-
ployed
with mosque at 30x, Egypt with politics at 20x, and
United States with safari at .92x. We note that, as
seen in the previous metric, this dataset is so skewed
in terms of representation that most statistically signif-
icant underrepresentations are in the United States, as
no other country has a high enough sample size. Addi-
tionally, whether these over- or under-representations
are problematic enough to warrant intervention is en-
tirely up to the user and their downstream task. We
have normalized these tags by number of tag occur-
rences, and notby real-world distributions of the ob-
jects they mention, e.g., perhaps there are simply more
mosques in Iran than other countries and this overrep-
resentation is innocuous and in fact a representative
depiction of the country | it is up to the user to verify
this.
We also look beyond the numbers themselves into
the appearances of how dierent subregions, as dened
by the United Nations geoscheme (United Nations Statis-
tics Division, 2019), represent certain tags. DeVries et
al. (2019) showed that object-recognition systems per-
form worse on images from countries that are not as
well-represented in the dataset due to appearance dif-
ferences within an object class, so we look into such
appearance dierences within a Flickr tag. We perform
the same analysis as in Sec. 5.1 where we run a Linear
SVM on the featurized images, this time performing
17-way classication between the dierent subregions.
In Fig. 12 we show an example of the dish tag, and
what images from the most accurately classied subre-
Fig. 12: A qualitative look at YFCC100m for what the
visual model condently and correctly classies for im-
ages with the dish tag as in Eastern Asia, and out.
gion, Eastern Asia, look like compared to images from
the other subregions. Images with the dish tag tend
to refer to food items in Eastern Asia, rather than a
satellite dish or plate, which is a more common prac-
tice in other regions. This example is telling of a more
pernicious problem than mis-identifying dishes, which
is that of dialect dierences between regions, and how
that might aect the semantic meaning of a label. Dis-
entangling homonyms will require computer vision sys-
tems to pay attention to the more subtle nuances of
linguistics (Roll et al., 2018). It may be important to
know if tags are represented dierently across subre-
gions so that models do not overt to one particular
subregion's representation of an object.
6.1.3 Geography by People
Next, we combine our COCO demographic skin tone
annotations with geography labels. In Fig. 13 we see
that images of people with darker skin tones tend to
come from South Asia and Africa, but neither of these
regions are very well-represented compared to images
from the United States and Europe. In fact, while 85.5%REVISE: A Tool for Measuring and Mitigating Bias in Visual Datasets 13
Fig. 13: Geographical distribution of COCO images
based on skin tone annotations. Images from South Asia
and Africa tend to contain people of darker skin tones,
although the majority of images are coming from the
United States and Europe.
of images of people with lighter skin tones (values 1-3)
come from North America and Europe, this number is
58.2% for people with darker skin tones (values 4-6).
Models that use this dataset may develop an under-
standing of people with darker skin tones that will be
primarily informed by people from North America and
Europe, which is a very small sample of people with
darker skin tones in the world. Cultural practices dif-
fer among people across regions, and depending on the
downstream application, it could be important that an
understanding of a group not be informed only by the
people in one geographic region.
For this particular dataset, we can additionally cus-
tomize our tool to incorporate an external data source.
Looking at the images only within the United States
and binning by urban centers, as dened by the U.S.
Census, we nd that while 84.4% of images of people
with lighter skin tones 1-3 are located in an urban area,
92.7% of images of people with darker skin tones 4-6 are
located in an urban area.
6.1.4 Geography by Language
When we looked at the global distribution of the
YFCC100m dataset, we saw an uneven distribution,
with few images coming from countries in Africa and
Asia. However, the locale of an image can be mislead-
ing, since if all the images taken in a particular country
are only by tourists, this would not necessarily encom-
pass the geo-representation one would hope for. Thus,
here we combine our geography labels with language
annotations. Fig. 14 shows the percentage of images
taken in a country and captioned in something other
than the national language(s), as detected by the fast-
Text library (Joulin, Grave, Bojanowski, Douze, et al.,
2016; Joulin, Grave, Bojanowski, and Mikolov, 2016).
Fig. 14: Percentage of tags in a non-local language in
YFCC100m. Even when underrepresented countries are
imaged, it is not necessarily by someone local to that
area.
We use the lower bound of the binomial proportion con-
dence interval in the gure so that countries with only
a few images total which happen to be mostly taken
by tourists are not shown to be disproportionately im-
aged as so. Even with this lower bound, we see that
many countries that are represented poorly in number
are also under-represented by locals. To determine the
implications in representation based on who is portray-
ing a country, we categorize an image as taken by a
local, tourist, or unknown, using a combination of lan-
guage detected and tag content as an imperfect proxy.
We then investigate if there are appearance dierences
in how locals and tourists portray a country by au-
tomatically running visual models. Although our tool
does not nd any such notable dierence, this kind of
analysis can be useful on other datasets where a lo-
cal's perspective is dramatically dierent than that of
a tourist's.
6.1.5 Geography by Income
Next, we consider how geography interacts with in-
come. For this analysis, we focus on the portion of
the BDD100K dataset in New York, and use income
statistics by ZIP code (Keeping Track Online, 2019;
The United States Census Bureau, 2019). This dataset
was collected by crowd-sourcing videos uploaded by
drivers, a collection process that has the potential to in-
troduce geographic or socioeconomic biases due to the
self-selection of drivers.
To test whether this is the case, we divide the ZIP
codes into deciles based on average income, and visual-
ize how representation varies by income decline (Figure
15). We see that there is a large dierence in the number
of images per square mile between the two wealthiest
deciles and the rest. It is possible that some of this may
be explained by the wealthier ZIP codes being in bor-14 Angelina Wang et al.
Fig. 15: ZIP codes with higher income are more repre-
sented in the BDD100K New York data.
Fig. 16: ZIP codes with higher income are more likely
to contain bicycles and pedestrians in the BDD100K
New York data.
oughs with a greater density of roads. Accordingly, we
also visualize the mean images per capita rather than
per square mile, and nd that a large dierence persists.
Such dierences in representation can introduce bi-
ases or performance disparities in models trained on
the data, because areas with dierent socioeconomic
attributes are known to have systematic appearance
dierences (Gebru et al., 2017). As evidence of such
appearance dierences in the BDD100K data, we high-
light in Fig. 16 that income correlates with the presence
of both the bicycle andpedestrian label.
6.1.6 Geography by Weather
In the BDD100K self-driving car dataset, we have ac-
cess to weather tags on each image. Weather is a very
relevant factor for this context of automated driving, as
oftentimes datasets only contain weather in clear condi-
tions (Sheeny et al., 2021), and thus have trouble gen-
eralizing to other weather conditions. Unsurprisingly,
there are discrepancies between the weather distribu-
tions of images in the Northern California and New
York City portions of this dataset, especially when look-
ing at the snowy label, which is present at 0.3% for the
former and 10% for the latter. It is important to be
aware of these dierences when deploying models in a
setting dierent from the one they were trained in. We
note that while we have distinguished between geogra-
phy analyses by object and by weather, both are auto-matically run through the same technical functionality
of the tool, as they are considering how the variation of
per-image tags, i.e., object and weather labels, vary by
region.
6.2 Geography-based Actionable Insights
Much like the demographic-based actionable insights,
those for geography-based are also more general and
dependent on what the model trained on the data will
be used for. Under- and over- representations can be
approached in ways similar to before by augmenting
the dataset, an important step in making sure we do
not have a one-sided perspective of a region. Dataset
users should validate that their models are not overt-
ting to a particular region's representation and image
distribution by testing on more geographically diverse
data, especially on that which is representative of where
a model will be deployed. The geographic distribution
of a dataset is intricately linked to the representations
of objects and the people in them. Because of this, we
note that not all instances of distribution dierences
are problematic and certain ndings of the tool, such as
an underrepresentation of safari in the United States,
may be entirely expected and not warrant any action
to be taken. This will all depend on the use-case of the
tested dataset.
It is clear that as we deploy more and more mod-
els into the world, there should be some form of either
equal or equitable geo-representation. This emphasizes
the need for data collection to explicitly seek out more
diversity in locale, and specically from the people that
live there. Technology has been known to leave groups
behind as it makes rapid advancements, and it is crucial
that dataset representation does not follow this trend
and base representation on digital availability. It re-
quires more eort to seek out images from underrepre-
sented areas, but as Jo and Gebru (2020) discuss, there
are actions that can and should be taken, such as explic-
itly collecting data from underrepresented geographic
regions, to ensure a more diverse representation.
7 Discussion
REVISE is eective at surfacing and helping mitigate
many kinds of biases in visual datasets. But we make
no claim that REVISE will identify allvisual biases.
Creating an \unbiased" dataset may not be a realis-
tic goal. The challenges are both practical (the sheer
number of categories in modern datasets; the diculty
of gathering images from parts of the world where fewREVISE: A Tool for Measuring and Mitigating Bias in Visual Datasets 15
people are online) and conceptual (how should we bal-
ance the goals of representing the world as it is and the
world as we want it to be)?
The kind of interventions that can and should be
performed in response to discovered biases will vary
greatly depending on the dataset and applications. For
example, for an object recognition benchmark, one may
lean toward removing or obfuscating people that occur
in images since the occurrence of people is largely in-
cidental to the scientic goals of the dataset (Prabhu
and Birhane, 2020; K. Yang et al., 2021). But such an
intervention wouldn't make sense for a dataset used as
part of a self-driving vehicle application. Rather, when
a dataset is used in a production setting, interventions
should be guided by an understanding of the down-
stream harms that may occur in that specic applica-
tion (Barocas et al., 2019), such as poor performance in
some neighborhoods. Making sense of which represen-
tations are more harmful for downstream applications
may require additional data sources to help understand
whether an underrepresentation is, for example, a re-
sult of a problem in the data collection eort, or sim-
ply representative of the world being imaged. Further,
dataset bias mitigation is only one step, albeit an im-
portant one, in the much broader process of addressing
fairness in the deployment of a machine learning sys-
tem (Birhane, 2021; Green and Hu, 2018).
We also note that much of our analyses necessar-
ily involves subdividing people along various socially-
constructed dimensions. By operationalizing dynamic
and non-discrete concepts such as gender and using skin
tone as a proxy for race, we reify certain conceptions of
these concepts (Hanna et al., 2020; Jacobs and Wallach,
2021) that harm certain groups, e.g., non-binary indi-
viduals (Hamidi et al., 2018; Scheuerman et al., 2020).
8 Conclusion
In conclusion, we present the REVISE tool, which auto-
mates the discovery of potential biases in visual datasets
and their annotations. We perform this investigation
along three axes: object-based, person-based, and
geography-based, and note that there are many more
axes along which biases live. What cannot be auto-
mated is determining which of these biases are prob-
lematic and which are not, so we hope that by surfacing
anomalous patterns as well as actionable next steps to
the user, we can at least bring these biases to light.9 Acknowledgments
This work is partially supported by the National Sci-
ence Foundation under Grant No. 1763642 and No.
1704444. We would also like to thank Felix Yu, Vikram
Ramaswamy, and Zhiwei Deng for their helpful com-
ments, and Zeyu Wang, Deniz Oktay, and Nobline Yoo
for testing out the tool and providing feedback.16 Angelina Wang et al.
A Appendices
A.1 Gender label inference
An additional person-based metric we consider is gender la-
bel inference. Specically, we note two especially concerning
practices of assigning gender to a person who is too small
to be identiable, or no face is detected in the image. This
is not to say that if these cases are not met it is acceptable
to assign gender, as gender cannot be visually perceived by
an annotator, but merely that assigning gender when one of
these two cases is applicable is a particularly egregious prac-
tice. For example, it's been shown that in images where a
person is fully clad with snowboarding equipment and a hel-
met, they are still labeled as male (Burns et al., 2018) due to
preconceived stereotypes. We investigate the contextual cues
annotators rely on to assign gender, and consider the gen-
der of a person unlikely to be identiable if the person is too
small (below 1000 pixels, which is the number of dimensions
that humans require to perform certain recognition tasks in
color images (Torralba et al., 2008)) or if automated face
detection (we used Amazon Rekognition (\Amazon Rekogni-
tion", n.d.), but note that any other face detection tool can
be used) fails. For COCO, we nd that among images with
a human whose gender is unlikely to be identiable, 77% are
labeled male. In OpenImages,5this fraction is 69%. Thus,
annotators seem to default to labeling a person as male when
they cannot identify the gender; the use of male-as-norm is a
problematic practice (Moulton, 1981). Further, we nd that
annotators are most likely to default to male as a gender label
inoutdoor sports fields, parks scenes, which is 2.9x the
rate of female. Similarly, the rate for indoor transportation
scenes is 4.2x and outdoor transportation is 4.5x, with the
closest ratio being in shopping and dining , where male is
1.2x as likely as female. This suggests that in the absence
of gender cues from the person themselves, annotators make
inferences based on image context. In Fig. 17 we show exam-
ples from OpenImages where our tool determined that gender
denitely should not be inferred, but was. Because attributes
like skin tone can be inferred from parts of the image, such
as a person's arm, we do not consider that attribute in this
analysis.
This metric of gender label inference also brings up a
larger question of which situations, if any, gender labels should
ever be assigned (Hamidi et al., 2018; Scheuerman et al.,
2020). However, that is outside the scope of this work, where
we simply recommend that dataset creators should give clearer
guidance to annotators, and remove the gender labels on im-
ages where gender can denitely not be determined. We note
that while we picked out two criteria of when a person is
too small and when there is no face detected to be instances
in which gender inference is particularly egregious, there are
many other situations that users may wish to delineate for
their own purposes.
A.2 Validating Distance as a Proxy for Interaction
In Section 5.1, Instance Counts and Distances, we make the
claim that we can use distance between a person and an
object as a proxy for if the person, p, is actually interact-
ing with the object, o, as opposed to just appearing in the
5Random subset of size 100,000
Fig. 17: Examples from OpenImages where annotators
assigned gender to the person, but they should not have.
The criteria used are that the person is either too small
or has no face detected.
Object #
La-
beled
Ex.'sMean
Per
Class
Acc
(%)\Yes"
Distance
meanstd\No"
Distance
meanstdThre-
shold
ball 107 67 6:162:64 8:544:15 7.63
book 27 78 2:451:99 4:842:24 3.88
car 135 71 2:943:20 4:592:97 2.74
dog 58 71 1:081:12 2:071:79 0.60
guitar 40 88 0:901:77 2:131:21 1.61
table 76 67 1:881:19 3:282:45 2.47
Table 4: Distances are classied as \yes" or \no" in-
teraction based on a threshold optimized for mean
per class accuracy. Visualization of the classication in
Fig. 18. Distances for \yes" interactions are lower than
\no" interactions in all cases, in line with our claim that
smaller distances are more likely to signify an interac-
tion.
same image with it. This allows us to get more meaning-
ful insight as to how genders may be interacting with ob-
jects dierently. The distance measure we dene is dist =
distance between p and o centersparea parea o, which is a relative mea-
sure within each object class because it makes the assump-
tion that all people are the same size, and all instances of an
object are the same size. To validate the claim we are making,
we look at the SpatialSense dataset (K. Yang et al., 2019);
specically, at 6 objects that we hope to be somewhat repre-
sentative of the dierent ways people interact with objects:
ball,book,car,dog,guitar , and table . These objects were
picked over ones such as wall orfloor , in which it is more am-
biguous what counts as an interaction. We then hand-labeled
the images where this object cooccurs with a human as \yes"
or \no" based on whether the person of interest is interacting
with the object or not. We pick the threshold by optimizing
for mean per class accuracy, where every distance below it as
classied as a \yes" interaction and every distance above it
as a \no" interaction. The threshold is picked based on the
same data that the accuracy is reported for.
As can be seen in Table 4, for all 6 categories the mean
of the distances when someone is interacting with an object
is lower than that of when someone is not. This matches our
claim that distance, although imperfect, can serve as a proxy
for interaction. From looking at the visualization of the dis-
tribution of the distances in Fig. 18, we can see that for cer-
tain objects like ball and table , which also have the lowestREVISE: A Tool for Measuring and Mitigating Bias in Visual Datasets 17
(a) Ball Distances
 (b) Book Distances
(c) Car Distances
 (d) Dog Distances
(e) Guitar Distances
 (f) Table Distances
Fig. 18: Distances for the objects that were hand-
labeled, orange if there is an interaction, and blue if
there is not. The red vertical line is the threshold along
which everything below is classied as \yes", and ev-
erything above is classied as \no".
mean per class accuracy, there is more overlap between the
distances for \yes" interactions and \no" interactions. Intu-
itively, this makes some sense, because a ball is an object
that can be interacted with both from a distance and from
direct contact, and for table in the labeled examples, people
were often seated at a table but not directly interacting with
it.
A.3 Pairwise Queries
In Section 4.2, another claim we make is that pairwise queries
of the form \[ Desired Object ] and [ Suggested Query Term ]"
could allow dataset collectors to augment their dataset with
the types of images they want. One of the examples we gave is
that if one notices the images of airplane in their dataset are
overrepresented in the larger sizes, our tool would recommend
they make the query \ airplane and surfboard " to augment
their dataset, because based on the distribution of training
samples, this combination is more likely than other kinds of
queries to lead to images of smaller airplanes.
However, there are a few concerns with this approach.
For one, certain queries might not return any search results.This is especially the case when the suggested query term
is a scene category, such as indoor cultural , in which the
query \ pizza and indoor cultural " might not be very fruit-
ful. To deal with this, we can substitute the scene category,
indoor cultural , for more specic scenes in that category,
like classroom and conference , so that the query becomes
something like \ pizza and classroom ". When the suggested
query term involves an object, there is another approach we
can take. In datasets like PASCAL VOC (Everingham et
al., 2010), the set of queries used to collect the dataset is
given. For example, to get pictures of boat, they also queried
forbarge ,ferry , and canoe . Thus, in addition to querying,
for example, \ airplane and boat", one could also query for
\airplane and ferry ", \airplane and barge ", etc.
Another concern is there might be a distribution dier-
ence between the correlation observed in the data and the
correlation in images returned for queries. For example, just
because catand dogcooccur at a certain rate in the dataset,
does not necessarily mean they cooccur at this same rate in
search engine images. However, our query recommendation
rests on the assumptions that datasets are constructed by
querying a search engine, and that objects cooccur at roughly
the same relative rates in the dataset as they do in query re-
turns; for example, that because train cooccurring with boat
in our dataset tends to be more likely to be small, in images
returned from queries, train is also likely to be smaller if
boat is in the image. We make an assumption that for an
image that contains a train and boat, the query \ train and
boat" would recover these kinds of images back, but it could
be the case that the actual query used to nd this image was
\coastal transit." If we had access to the actual query used
to nd each image, the conditional probability could then be
calculated over the queries themselves rather than the object
or scene cooccurrences. It is because we don't have these orig-
inal queries that we use cooccurrences to serve as a proxy for
recovering them.
To gain some condence in our use of these pairwise
queries in place of the original queries, we show qualitative
examples of the results when searching on Flickr for images
that contain the tags of the object(s) searched. We show the
results of querying for (1) just the object (2) the object and
query term that we would hope leads to more of the ob-
ject in a smaller size, and (3) the object and query term
that we would hope leads to more of the object in a big-
ger size. In Figs. 19 and 20 we show the results of images
sorted by relevance under the Creative Commons license. We
can see that when we perform these pairwise queries, we do
indeed have some level of control over the size of the object
in the resulting images. For example, \ pizza and classroom "
and \ pizza and conference " queries (scenes swapped in for
indoor cultural ) return smaller pizzas than the \ pizza and
broccoli " query, which tends to feature bigger pizzas that
take up the whole image. This could of course create other
representation issues such as a surplus of pizza andbroccoli
images, so it could be important to use more than one of the
recommended queries our tool surfaces. Although this is an
imperfect method, it is still a useful tactic we can use with-
out having access to the actual queries used to create the
dataset.6
6We also looked into using reverse image searches to re-
cover the query, but the \best guess labels" returned from
these searches were not particularly useful, erring on both
the side of being much too vague, such as returning \sea" for
any scene with water, or too specic, with the exact name
and brand of one of the objects.18 Angelina Wang et al.
Fig. 19: Screenshots of top results from performing
queries on Flickr that satisfy the tags mentioned. For
train , when it is queried with boat , the train itself is
more likely to be farther away, and thus smaller. When
queried with backpack , the image is more likely to show
travelers right next to, or even inside of, a train , and
thus show it more in the foreground. The same idea
applies for pizza where it's imaged from further in
the background when paired with an indoor cultural
scene, and up close with broccoli .
Fig. 20: Screenshots of top results from performing
queries on Flickr that satisfy the tags mentioned. For
bed,sink provides a context that makes it more likely
to be imaged further away, whereas catbrings bedto
the forefront. The same is the case when the object of
interest is now cat, where a pairwise query with sheep
makes it more likely to be further, and suitcase to be
closer.REVISE: A Tool for Measuring and Mitigating Bias in Visual Datasets 19
References
Alwassel, H., Heilbron, F. C., Escorcia, V., & Ghanem,
B. (2018). Diagnosing error in temporal action
detectors. European Conference on Computer
Vision (ECCV) .
Amazon. (2021). Amazon sagemaker clarify. https://
aws.amazon.com/sagemaker/clarify/
Amazon rekognition. (n.d.). https://aws.amazon.com/
rekognition/
Barocas, S., Hardt, M., & Narayanan, A. (2019). Fair-
ness and machine learning [http://www.fairmlbook.
org]. fairmlbook.org.
Bearman, S., Korobov, N., & Thorne, A. (2009). The
fabric of internalized sexism. Journal of Inte-
grated Social Sciences 1(1): 10-47 .
Bellamy, R. K. E., Dey, K., Hend, M., Homan, S. C.,
Houde, S., Kannan, K., Lohia, P., Martino, J.,
Mehta, S., Mojsilovic, A., Nagar, S., Rama-
murthy, K. N., Richards, J., Saha, D., Sattigeri,
P., Singh, M., Varshney, K. R., & Zhang, Y.
(2018). AI Fairness 360: An extensible toolkit
for detecting, understanding, and mitigating un-
wanted algorithmic bias. arXiv:1810.01943 .
Berg, A. C., Berg, T. L., III, H. D., Dodge, J., Goyal, A.,
Han, X., Mensch, A., Mitchell, M., Sood, A.,
Stratos, K., & Yamaguchi, K. (2012). Under-
standing and predicting importance in images.
Conference on Computer Vision and Pattern
Recognition (CVPR) .
Birhane, A. (2021). Algorithmic injustice: A relational
ethics approach. Patterns ,2.
Brown, C. (2014). Archives and recordkeeping: Theory
into practices. Facet Publishing .
Buda, M., Maki, A., & Mazurowski, M. A. (2017). A
systematic study of the class imbalance prob-
lem in convolutional neural networks. arXiv:1710.05381 .
Buolamwini, J., & Gebru, T. (2018). Gender shades:
Intersectional accuracy disparities in commer-
cial gender classication. ACM Conference on
Fairness, Accountability, Transparency (FAccT) .
Burns, K., Hendricks, L. A., Saenko, K., Darrell, T., &
Rohrbach, A. (2018). Women also snowboard:
Overcoming bias in captioning models. Euro-
pean Conference on Computer Vision (ECCV) .
Caliskan, A., Bryson, J. J., & Narayanan, A. (2017).
Semantics derived automatically from language
corpora contain humanlike biases. Science ,356(6334),
183{186.
Choi, M. J., Torralba, A., & Willsky, A. S. (2012). Con-
text models and out-of-context objects. Pat-
tern Recognition Letters , 853{862.Chouldechova, A. (2017). Fair prediction with disparate
impact: A study of bias in recidivism prediction
instruments. Big Data .
Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., & Fei-
Fei, L. (2009). ImageNet: A Large-Scale Hier-
archical Image Database. CVPR .
Denton, E., Hanna, A., Amironesei, R., Smart, A., Nicole,
H., & Scheuerman, M. K. (2020). Bringing the
people back in: Contesting benchmark machine
learning datasets. arXiv:2007.07399 .
DeVries, T., Misra, I., Wang, C., & van der Maaten,
L. (2019). Does object recognition work for ev-
eryone? Conference on Computer Vision and
Pattern Recognition workshops (CVPRW) .
Dwork, C., Hardt, M., Pitassi, T., Reingold, O., & Zemel,
R. (2012). Fairness through awareness. Proceed-
ings of the 3rd Innovations in Theoretical Com-
puter Science Conference .
Dwork, C., Immorlica, N., Kalai, A. T., & Leiserson,
M. (2017). Decoupled classiers for fair and ef-
cient machine learning. arXiv:1707.06613 .
Everingham, M., Gool, L. V., Williams, C. K. I., Winn,
J., & Zisserman, A. (2010). The pascal visual
object classes (voc) challenge. International Jour-
nal of Computer Vision (IJCV) .
Facebook AI. (2021). Fairness ow. https://ai.facebook.
com/blog/how- were- using- fairness- ow- to-
help-build-ai-that-works-better-for-everyone/
Fei-Fei, L., Fergus, R., & Perona, P. (2004). Learn-
ing generative visual models from few training
examples: An incremental bayesian approach
tested on 101 object categories. IEEE CVPR
Workshop of Generative Model Based Vision .
Fitzpatrick, T. B. (1988). The validity and practical-
ity of sun- reactive skin types I through VI.
Archives of Dermatology ,6, 869{871.
Gajane, P., & Pechenizkiy, M. (2017). On formalizing
fairness in prediction with machine learning.
arXiv:1710.03184 .
Galleguillos, C., Rabinovich, A., & Belongie, S. (2008).
Object categorization using co-occurrence, lo-
cation and appearance. Conference on Com-
puter Vision and Pattern Recognition (CVPR) .
Gebru, T., Krause, J., Wang, Y., Chen, D., Deng, J.,
Aiden, E. L., & Fei-Fei, L. (2017). Using deep
learning and google street view to estimate the
demographic makeup of neighborhoods across
the united states. Proceedings of the National
Academy of Sciences of the United States of
America (PNAS) ,114(50), 13108{13113. https:
//doi.org/10.1073/pnas.1700035114
Gebru, T., Morgenstern, J., Vecchione, B., Vaughan,
J. W., Wallach, H., III, H. D., & Crawford, K.20 Angelina Wang et al.
(2018). Datasheets for datasets. ACM Confer-
ence on Fairness, Accountability, Transparency
(FAccT) .
Google People + AI Research. (2021). Know your data.
https://knowyourdata.withgoogle.com/
Green, B., & Hu, L. (2018). The myth in the method-
ology: Towards a recontextualization of fair-
ness in machine learning. Machine Learning:
The Debates workshop at the 35th International
Conference on Machine Learning .
Hamidi, F., Scheuerman, M. K., & Branham, S. (2018).
Gender recognition or gender reductionism?:
The social implications of embedded gender recog-
nition systems. Conference on Human Factors
in Computing Systems (CHI) .
Hanna, A., Denton, E., Smart, A., & Smith-Loud, J.
(2020). Towards a critical race methodology in
algorithmic fairness. ACM Conference on Fair-
ness, Accountability, Transparency (FAccT) .
Hardt, M., Price, E., & Srebro, N. (2016). Equality of
opportunity in supervised learning. Advances
in Neural Information Processing Systems (NeurIPS) .
He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep
residual learning for image recognition. Euro-
pean Conference on Computer Vision (ECCV) .
Hill, K. (2020). Wrongfully accused by an algorithm.
The New York Times . https://www.nytimes.
com/2020/06/24/technology/facial-recognition-
arrest.html
Hoiem, D., Chodpathumwan, Y., & Dai, Q. (2012). Di-
agnosing error in object detectors. European
Conference on Computer Vision (ECCV) .
Holland, S., Hosny, A., Newman, S., Joseph, J., & Chmielin-
ski, K. (2018). The dataset nutrition label: A
framework to drive higher data quality stan-
dards. arXiv:1805.03677 .
Honnibal, M., Montani, I., Van Landeghem, S., & Boyd,
A. (2020). spaCy: Industrial-strength Natural
Language Processing in Python . Zenodo. https:
//doi.org/10.5281/zenodo.1212303
Hua, J., Xiong, Z., Lowey, J., Suh, E., & Dougherty,
E. R. (2005). Optimal number of features as a
function of sample size for various classication
rules. Bioinformatics ,21, 1509{1515.
Idelbayev, Y. (2019). https://github.com/akamaster/
pytorch resnet cifar10
Jacobs, A. Z., & Wallach, H. (2021). Measurement and
fairness. ACM Conference on Fairness, Account-
ability, Transparency (FAccT) .
Jain, A. K., & Waller, W. (1978). On the optimal num-
ber of features in the classication of multi-
variate gaussian data. Pattern Recognition ,10,
365{374.Jo, E. S., & Gebru, T. (2020). Lessons from archives:
Strategies for collecting sociocultural data in
machine learning. ACM Conference on Fair-
ness, Accountability, Transparency (FAccT) .
Jonckheere, A. R. (1954). A distribution-free k-sample
test against ordered alternatives. Biometrika ,
41.
Joulin, A., Grave, E., Bojanowski, P., Douze, M., J egou,
H., & Mikolov, T. (2016). Fasttext.zip: Com-
pressing text classication models. arXiv preprint
arXiv:1612.03651 .
Joulin, A., Grave, E., Bojanowski, P., & Mikolov, T.
(2016). Bag of tricks for ecient text classi-
cation. arXiv preprint arXiv:1607.01759 .
Kay, M., Matuszek, C., & Munson, S. A. (2015). Un-
equal representation and gender stereotypes in
image search results for occupations. Human
Factors in Computing Systems , 3819{3828.
Keeping Track Online. (2019). Median incomes. https:
//data.cccnewyork.org/data/table/66/median-
incomes#66/107/62/a/a
Khosla, A., Zhou, T., Malisiewicz, T., Efros, A. A.,
& Torralba, A. (2012). Undoing the damage
of dataset bias. European Conference on Com-
puter Vision (ECCV) .
Kilbertus, N., Rojas-Carulla, M., Parascandolo, G., Hardt,
M., Janzing, D., & Sch olkopf, B. (2017). Avoid-
ing discrimination through causal reasoning. Ad-
vances in Neural Information Processing Sys-
tems (NeurIPS) .
Kleinberg, J., Mullainathan, S., & Raghavan, M. (2017).
Inherent trade-os in the fair determination of
risk scores. Proceedings of Innovations in The-
oretical Computer Science (ITCS) .
Krasin, I., Duerig, T., Alldrin, N., Ferrari, V., Abu-El-
Haija, S., Kuznetsova, A., Rom, H., Uijlings,
J., Popov, S., Veit, A., Belongie, S., Gomes,
V., Gupta, A., Sun, C., Chechik, G., Cai, D.,
Feng, Z., Narayanan, D., & Murphy, K. (2017).
Openimages: A public dataset for large-scale
multi-label and multi-class image classication.
Dataset available from https://github.com/openimages .
Krishna, R., Zhu, Y., Groth, O., Johnson, J., Hata, K.,
Kravitz, J., Chen, S., Kalanditis, Y., Li, L.-J.,
Shamma, D. A., Bernstein, M., & Fei-Fei, L.
(2016). Visual genome: Connecting language
and vision using crowdsourced dense image an-
notations. https://arxiv.org/abs/1602.07332
Krizhevsky, A. (2009). Learning multiple layers of fea-
tures from tiny images. Technical Report .
Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012).
Imagenet classication with deep convolutionalREVISE: A Tool for Measuring and Mitigating Bias in Visual Datasets 21
neural networks. Advances in Neural Informa-
tion Processing Systems (NeurIPS) , 1097{1105.
Lin, T.-Y., Maire, M., Belongie, S., Bourdev, L., Gir-
shick, R., Hays, J., Perona, P., Ramanan, D.,
Zitnick, C. L., & Dollar, P. (2014). Microsoft
COCO: Common objects in context. European
Conference on Computer Vision (ECCV) .
Liu, X.-Y., Wu, J., & Zhou, Z.-H. (2009). Exploratory
undersampling for class-imbalance learning. IEEE
Transactions on Systems, Man, and Cybernet-
ics.
Mehrabi, N., Morstatter, F., Saxena, N., Lerman, K., &
Galstyan, A. (2019). A survey on bias and fair-
ness in machine learning. arXiv:1908.09635 .
Moulton, J. (1981). The myth of the neutral 'man'. Sex-
ist Language: A Modern Philosophical Analy-
sis, 100{116.
Oksuz, K., Cam, B. C., Kalkan, S., & Akbas, E. (2019).
Imbalance Problems in Object Detection: A
Review. arXiv e-prints , arXiv:1909.00169.
Oliva, A., & Torralba, A. (2007). The role of context
in object recognition. Trends in Cognitive Sci-
ences .
Ouyang, W., Wang, X., Zhang, C., & Yang, X. (2016).
Factors in netuning deep model for object de-
tection with long-tail distribution. Conference
on Computer Vision and Pattern Recognition
(CVPR) .
Paullada, A., Raji, I. D., Bender, E. M., Denton, E., &
Hanna, A. (2020). Data and its (dis)contents:
A survey of dataset development and use in
machine learning research. NeurIPS Workshop:
ML Retrospectives, Surveys, and Meta-analyses .
Pleiss, G., Raghavan, M., Wu, F., Kleinberg, J., & Wein-
berger, K. Q. (2017). On fairness and calibra-
tion. Advances in Neural Information Process-
ing Systems (NeurIPS) .
Prabhu, V. U., & Birhane, A. (2020). Large image datasets:
A pyrrhic win for computer vision? arXiv:2006.16923 .
Roll, U., Correia, R. A., & Berger-Tal, O. (2018). Using
machine learning to disentangle homonyms in
large text corpora. Conservation Biology .
Rosenfeld, A., Zemel, R., & Tsotsos, J. K. (2018). The
elephant in the room. arXiv:1808.03305 .
Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh,
S., Ma, S., Huang, Z., Karpathy, A., Khosla,
A., Bernstein, M., Berg, A. C., & Fei-Fei, L.
(2015). ImageNet Large Scale Visual Recogni-
tion Challenge. International Journal of Com-
puter Vision (IJCV) ,115(3), 211{252. https:
//doi.org/10.1007/s11263-015-0816-y
Salakhutdinov, R., Torralba, A., & Tenenbaum, J. (2011).
Learning to share visual appearance for mul-ticlass object detection. Conference on Com-
puter Vision and Pattern Recognition (CVPR) .
Scheuerman, M. K., Wade, K., Lustig, C., & Brubaker,
J. R. (2020). How we've taught algorithms to
see identity: Constructing race and gender in
image databases for facial analysis. Proceedings
of the ACM on Human-Computer Interaction .
Shankar, S., Halpern, Y., Breck, E., Atwood, J., Wilson,
J., & Sculley, D. (2017). No classication with-
out representation: Assessing geodiversity is-
sues in open datasets for the developing world.
NeurIPS workshop: Machine Learning for the
Developing World .
Sheeny, M., Pellegrin, E. D., Mukherjee, S., Ahrabian,
A., Wang, S., & Wallace, A. (2021). RADIATE:
A radar dataset for automotive perception in
bad weather. IEEE International Conference
on Robotics and Automation (ICRA) .
Sigurdsson, G. A., Russakovsky, O., & Gupta, A. (2017).
What actions are needed for understanding hu-
man actions in videos? International Confer-
ence on Computer Vision (ICCV) .
Swinger, N., De-Arteaga, M., IV, N. H., Leiserson, M.,
& Kalai, A. (2019). What are the biases in my
word embedding? Proceedings of the AAAI/ACM
Conference on Articial Intelligence, Ethics, and
Society (AIES) .
The United States Census Bureau. (2019). American
community survey 1-year estimates, table s1903
(2005-2019). https://data.census.gov/
Thomee, B., Shamma, D. A., Friedland, G., Elizalde,
B., Ni, K., Poland, D., Borth, D., & Li, L.-J.
(2016). Yfcc100m: The new data in multimedia
research. Communications of the ACM .
Tommasi, T., Patricia, N., Caputo, B., & Tuytelaars, T.
(2015). A deeper look at dataset bias. German
Conference on Pattern Recognition .
Torralba, A., & Efros, A. A. (2011). Unbiased look at
dataset bias. Conference on Computer Vision
and Pattern Recognition (CVPR) .
Torralba, A., Fergus, R., & Freeman, W. T. (2008). 80
million tiny images: A large dataset for non-
parametric object and scene recognition. IEEE
Transactions on Pattern Analysis and Machine
Intelligence ,30(11), 1958{1970.
United Nations Statistics Division. (2019). United Na-
tions statistics division - methodology. https:
//unstats.un.org/unsd/methodology/m49/
Wang, Z., Qinami, K., Karakozis, Y., Genova, K., Nair,
P., Hata, K., & Russakovsky, O. (2020). To-
wards fairness in visual recognition: Eective
strategies for bias mitigation. Conference on
Computer Vision and Pattern Recognition (CVPR) .22 Angelina Wang et al.
Wilson, B., Homan, J., & Morgenstern, J. (2019). Pre-
dictive inequity in object detection. arXiv:1902.11097 .
Xiao, J., Hays, J., Ehinger, K. A., Oliva, A., & Torralba,
A. (2010). Sun database: Large-scale scene recog-
nition from abbey to zoo. Conference on Com-
puter Vision and Pattern Recognition (CVPR) .
Yang, J., Price, B., Cohen, S., & Yang, M.-H. (2014).
Context driven scene parsing with attention to
rare classes. Conference on Computer Vision
and Pattern Recognition (CVPR) .
Yang, K., Qinami, K., Fei-Fei, L., Deng, J., & Rus-
sakovsky, O. (2020). Towards fairer datasets:
Filtering and balancing the distribution of the
people subtree in the imagenet hierarchy. ACM
Conference on Fairness, Accountability, Trans-
parency (FAccT) .
Yang, K., Russakovsky, O., & Deng, J. (2019). Spa-
tialsense: An adversarially crowdsourced bench-
mark for spatial relation recognition. Interna-
tional Conference on Computer Vision (ICCV) .
Yang, K., Yau, J., Fei-Fei, L., Deng, J., & Russakovsky,
O. (2021). A study of face obfuscation in ima-
genet. arXiv:2103.06191 .
Yao, Y., Zhang, J., Shen, F., Hua, X., Xu, J., & Tang,
Z. (2017). Exploiting web images for dataset
construction: A domain robust approach. IEEE
Transactions on Multimedia , 1771{1784.
Yu, F., Chen, H., Wang, X., Xian, W., Chen, Y., Liu, F.,
Madhavan, V., & Darrell, T. (2020). Bdd100k:
A diverse driving dataset for heterogeneous mul-
titask learning. IEEE Conference on Computer
Vision and Pattern Recognition (CVPR) .
Zhang, B. H., Lemoine, B., & Mitchell, M. (2018). Mit-
igating unwanted biases with adversarial learn-
ing.Proceedings of the 2018 AAAI/ACM Con-
ference on AI, Ethics, and Society .
Zhao, D., Wang, A., & Russakovsky, O. (2021). Under-
standing and evaluating racial biases in image
captioning. CoRR ,abs/2106.08503 .
Zhao, J., Wang, T., Yatskar, M., Ordonez, V., & Chang,
K.-W. (2017). Men also like shopping: Reduc-
ing gender bias amplication using corpus-level
constraints. Proceedings of the Conference on
Empirical Methods in Natural Language Pro-
cessing (EMNLP) .
Zhou, B., Lapedriza, A., Khosla, A., Oliva, A., & Tor-
ralba, A. (2017). Places: A 10 million image
database for scene recognition. IEEE Transac-
tions on Pattern Analysis and Machine Intel-
ligence .
Zhu, X., Anguelov, D., & Ramanan, D. (2014). Cap-
turing long-tail distributions of object subcat-egories. Conference on Computer Vision and
Pattern Recognition (CVPR) .