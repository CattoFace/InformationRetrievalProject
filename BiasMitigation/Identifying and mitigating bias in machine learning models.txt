Technical University of Denmark
Department of Management Engineering
Master Thesis
Identifying and mitigating bias in
machine learning models
June 25, 2021
Students:
Daniel Juh√°sz Vigild, s161749
Lau Johansson, s164512Supervisor:
Aasa FeragenTable of Contents
1 Abstract 1
2 Introduction 2
3 Research Questions 4
4 Terminology 5
4.1 Machine learning model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
4.2 ClassiÔ¨Åcation rates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
4.3 What is machine learning bias? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
5 Literature Review 7
5.1 IdentiÔ¨Åcation of bias . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
5.2 Bias mitigation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
6 Case description: The AIR project 21
6.1 Purpose . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
6.2 Organisational context . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
6.3 Political context . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
6.4 Flow diagram of the AIR project . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
6.5 The AIR data set . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
6.6 The AIR classiÔ¨Åcation model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
7 Method 25
7.1 Quantitative method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
7.2 Interview and online meetings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
7.3 Obtaining data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
8 Theory 28
8.1 Bias identiÔ¨Åcation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
8.2 Bias mitigation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
8.3 Machine learning models built on the AIR data set . . . . . . . . . . . . . . . . . . . . . . 32
9 Descriptive Analysis 37
10 IdentiÔ¨Åcation of bias 42
10.1 ClassiÔ¨Åcation rates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42
10.2 Relation between classiÔ¨Åcation rates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45
10.3 Accuracy and predicted probabilities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46
10.4 Sub-conclusion: IdentiÔ¨Åcation of bias in AIR . . . . . . . . . . . . . . . . . . . . . . . . . 47
11 Mitigation of bias 48
11.1 Dropping the protected variable . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48
11.2 Gender swapping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52
11.3 Disparate impact removal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56
11.4 Learning fair representations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60
11.5 Changes in overall classiÔ¨Åcations after mitigating bias . . . . . . . . . . . . . . . . . . . . 64
11.6 Sub-conclusion: Mitigation of bias in AIR . . . . . . . . . . . . . . . . . . . . . . . . . . . 64
12 Discussion 66
12.1 Challenges regarding the mitigation techniques . . . . . . . . . . . . . . . . . . . . . . . . 66
12.2 Technical aspects and modelling choices . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66
12.3 Theoretical and methodological approach . . . . . . . . . . . . . . . . . . . . . . . . . . . 69
13 Conclusion 71
13.1 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71
13.2 Recommendations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71
iTABLE OF CONTENTS
References 73
A Bias metrics for the Ô¨Åve models built on AIR data set 77
B Assessment of classiÔ¨Åcation thresholds 91
C ROC curves 95
D SHAP values 100
E Comparing probabilities across the models 103
F Linear regression analysis 104
G Qualitative theory of bias in machine learning 106
H The COMPAS algorithm 116
I COMPAS - Descriptive Analysis 118
J COMPAS - Reproduction 129
K COMPAS - Reproduction metrics 131
L Model architectures and hyperparameters 132
M Internal AIR report 134
ii1 ABSTRACT
1 Abstract
This thesis examines bias in machine learning models used for decision support by identifying and
mitigating bias in the AIR (AI Rehabilitation) project from Aalborg Municipality. The AIR project
explores the possibility of using citizens‚Äô data to predict their risk of falling and thereby support decisions
regarding the provision of fall prevention training. The thesis utilizes a data set with 2144 records
that contains information regarding citizens from Aalborg Municipality and their fall incidents. Bias
is identiÔ¨Åed by training Ô¨Åve machine learning models on the AIR data set, estimating gender-speciÔ¨Åc
classiÔ¨Åcation rates, and assessing the relation of the gender-speciÔ¨Åc rates. Bias is mitigated by applying
four diÔ¨Äerent pre-processing techniques ( dropping gender ,gender swap ,disparate impact removal , and
learning fair representations ) on the AIR data set and re-estimating the gender-speciÔ¨Åc classiÔ¨Åcation
rates. Gender bias is identiÔ¨Åed in all models built on the original AIR data set. Most noticeably, the
results show that the models misclassify females who fall at a higher rate than males who fall (false
negatives). All four pre-processing techniques successfully mitigate the identiÔ¨Åed gender bias. For future
work on bias mitigation, the thesis recommends that the applied identiÔ¨Åcation and mitigation techniques
could be a part of a two-step system, where the pre-processing mitigation eÔ¨Äorts are separated from the
process of building classiÔ¨Åers.
12 INTRODUCTION
2 Introduction
In 2019, the Danish government published "National strategi for kunstig intelligens" , which describes the
government‚Äôs ambition for responsible development and use of artiÔ¨Åcial intelligence (AI). The technology
is evolving rapidly, for example, 3 % of Danish municipalities used AI for solving tasks in 2018 - and 55
% of the municipalities expect to implement AI before the end of 2021 [1, p. 52]. In the public sector,
AI can support employees in solving tasks, making decisions and give citizens a smarter and better
user experience. However, the government notes that rapid evolution of technology can make citizens
feel insecure about the future. They further state that algorithms must ensure equal treatment and be
objective. The Danish government has developed six ethical principles for setting a common framework
for developing and using AI [1, pp. 27-29]. One of the six principles for artiÔ¨Åcial intelligence is "Equality
and Justice":
"Kunstig intelligens m√• ikke reproducere fordomme, der marginaliserer befolkningsgrupper.
Der skal arbejdes aktivt for at forhindre u√∏nsket bias og fremme designs, der undg√•r
kategorisering, som diskriminerer p√• baggrund af fx etnicitet, seksualitet og k√∏n.
DemograÔ¨Åsk og faglig diversitet b√∏r v√¶re en rettesnor i arbejdet med kunstig intelligens."
[ArtiÔ¨Åcial intelligence must not reproduce prejudices that marginalize population groups.
EÔ¨Äorts must be made to prevent unwanted bias and promote designs that avoid
categorizations that discriminate on e.g., ethnicity, sexuality, and gender. Demographic and
professional diversity should be a guideline in the work on artiÔ¨Åcial intelligence.] [1, p. 58]
Through "National strategi for kunstig intelligens" , the government initiates signature AI projects in the
public sector to accommodate the lack of experience related to the use of AI. To Ô¨Ånance the signature
projects, the Danish government has granted 60 Mio. DKK for 2019-2029, which complements 295 Mio.
DKK prioritized in the Finance Act [1, p. 20]. The signature projects will test technology in areas where
there is a potential to increase quality and productivity in core public tasks [1, p. 54].
One of the above mentioned signature projects, the AIR (AI Rehabilitation) project , will be the
focal point of analysis in our thesis. The AIR project is owned by Aalborg Municipality, with Aarhus
University and the company DigiRehab as collaborators. The project pursues the possibilities of using
citizens‚Äô data regarding registered aids to calculate probabilities of falling, and through this, support
decisions on who should be oÔ¨Äered fall prevention training [2].
We will analyse the data and algorithm used in the AIR project. Our analysis focuses on the iden-
tiÔ¨Åcation and mitigation of bias. In this sense, we adhere to the words of the principle of "Equality
and Justice" from "National strategi for kunstig intelligens" . By testing techniques for identiÔ¨Åcation and
mitigation of bias in the AIR project, the thesis will attempt to contribute to the general Ô¨Åeld of research
on bias in AI.
Bias in machine learning
To show why biased machine learning models could be problematic, we brieÔ¨Çy present three cases of bias
in machine learning.
Bias in natural language processing
When modeling human language in a machine learning context, typically called natural language process-
ing (NLP), researchers have discovered that bias and prejudices related to protected groups (e.g. gender
or race) can be identiÔ¨Åed in the mathematical representations of language learned by models [3]. Latanya
Sweeney [4] showed that ads on search results for names typically associated with African-Americans
were more likely to suggest criminal activity than names associated with Caucasians. In this example,
the NLP algorithms used by Google reproduce bias and prejudice found in society.
Bias in facial recognition systems
The research study, "Gender Shades" , evaluated three commercial gender classiÔ¨Åcation systems by Mi-
crosoft, IBM, and Face++. The study shows that dark-skinned females are the most misclassiÔ¨Åed group
with accuracy as low as 65.3%, compared to accuracy for light-skinned males of up to 100% [5]. The
authors suggest that the disparity in accuracy is due to an over-representation of lighter-skinned sub-
jects within the data sets. If directly implemented in commercial products that use facial recognition
22 INTRODUCTION
(e.g. phones, security systems), the performance of these would be substantially worse for dark-skinned
females.
Bias in risk assessment tools
In an alleged attempt to get defendants through the US legal system as eÔ¨Éciently as possible, the court-
roomshaveturnedtopredictiveriskassessmenttools[6]. Onetoolforsuchriskassessmentisthesoftware
program called Correctional OÔ¨Äender Management ProÔ¨Åling for Alternative Sanctions (COMPAS). This
tool predicts a recidivism score: a number estimating the likelihood of re-oÔ¨Äending. The judges use
the risk assessment as a decision support tool when determining pretrial release and sentencing [6]. In
a 2016 investigation of the COMPAS algorithm, ProPublica criticized the predictive tool for yielding
biased results by showing how "(...) blacks are almost twice as likely as whites to be labeled a higher risk
but not actually re-oÔ¨Äend" [7]. This sparked a public debate regarding racial bias in algorithms used in
the judicial system [8].
33 RESEARCH QUESTIONS
3 Research Questions
As stated in the introduction, we intend to test techniques for identiÔ¨Åcation and mitigation of bias in
the AIR project. Therefore, we aim to answer the following research questions in our thesis:
‚Ä¢RQ1: How can bias be identiÔ¨Åed in the AIR project?
‚Ä¢RQ2: How can bias be mitigated in the AIR project?
By answering the research questions, we intend to provide Aalborg Municipality and collaborators with
actionable recommendations that can be used to improve the classiÔ¨Åcations of the AIR algorithm in
terms of bias. Our vision is that the result of our thesis can be helpful to practitioners in the public
sector in their work on bias identiÔ¨Åcation and mitigation. In this light, we prioritize the accessibility and
reproducibility of our models and implementations. Therefore, we will use standard implementations of
algorithms and test a range of simple techniques to identify and mitigate bias. Furthermore, we envision
that these techniques come from pre-existing libraries or are easy-to-use methods so practitioners in the
public sector can apply the techniques presented in our thesis.
In section 10, we answer research question 1 by assessing the classiÔ¨Åcation rates of model predictions.
In section 11, we answer research question 2 by testing four diÔ¨Äerent techniques for mitigating bias.
44 TERMINOLOGY
4 Terminology
In the following section, we clarify some central concepts that will be applied in the thesis. Initially,
we describe what is meant by the term machine learning model . Then, we introduce metrics used for
measuringperformanceofclassiÔ¨Åers. Finally, ashortintroductiontobiasinmachinelearningisprovided.
4.1 Machine learning model
Machine learning is a subÔ¨Åeld of AI where models can learn structures in data. When learning, the
model Ô¨Ånds a mapping from input to output that is not explicitly programmed. The models can then
be applied on new data where the output can be utilized by a user [9, p. 197]. Machine learning models
can be used for many purposes, but in the context of our thesis, we examine the role of machine learning
models when used as decision support tools [10].
The terms AI model ,machine learning algorithms ,machine learning model , andalgorithm are used
interchangeably, and all refer to the same thing unless it is clearly speciÔ¨Åed.
4.2 ClassiÔ¨Åcation rates
According to Verma and Rubin [11], who have made a collection of deÔ¨Ånitions of bias related to algorith-
mic classiÔ¨Åcation problems, most statistical measures of bias use a combination of the deÔ¨Ånitions from
table 1. In machine learning, the term classiÔ¨Åcation refers to the process of developing a model that can
predict categorical class labels. The confusion matrix in table 1 categorizes the predictions of a model. In
a binary classiÔ¨Åcation setting, the class label can take two possible values: positive class or negative class.
The number of positive and negative observations that are correctly classiÔ¨Åed are called true positives
(TP) and true negatives (TN), respectively. MisclassiÔ¨Åed observations are called false positives (FP) and
false negatives (FN). If the class labels are, for example, boolean, then 1 or truelabeled observations are
assigned as the positive class [9, p. 203-204].
True Class
P N
PTrue Positives False PositivesPredicted ClassNFalse Negatives True Negatives
Table 1: Confusion matrix
From the confusion matrix in Table 1, several metrics can be calculated where the most common are [12,
p. 862][13]:
The true positive rate (TPR):
TPR =TP
TP+FN(1)
The false positive rate (FPR):
FPR =FP
FP+TN(2)
The true negative rate (TNR):
TNR =TN
TN+FP(3)
The false negative rate (FNR):
FNR =FN
FN+TP(4)
54 TERMINOLOGY
4.3 What is machine learning bias?
4.3.1 Bias and fairness
In this thesis, the term biasrefers to cases where a machine learning model (a classiÔ¨Åer) systematically
makes disparate predictions between certain individuals or groups of individuals in terms of classiÔ¨Åca-
tion metrics [14]. In the literature, the terms bias and fairness are used frequently, both together and
separately, to describe the same issues in machine learning [15]. In this thesis we will primarily use the
term bias. However, some of the papers in the literature review use the word fairness to describe this
concept.
4.3.2 Inductive bias
To avoid confusion, we brieÔ¨Çy state that when we use the word bias, we do not refer to inductive bias .
Inductive bias is a design parameter associated with model performance where not enough bias leads to
over-Ô¨Åtting, and too much bias leads to under-Ô¨Åtting. Inductive bias is necessary in all machine learning
techniques. When building a machine learning model, there is a bias-variance trade-oÔ¨Ä in order to make
a well-performing predictor, which is able to generalize well [9].
65 LITERATURE REVIEW
5 Literature Review
This section will review literature from the research area on bias in machine learning. The purpose of
this review is to Ô¨Ånd identiÔ¨Åcation and mitigation techniques that can be used to answer the research
questions. After reviewing each paper, we reÔ¨Çect on how the techniques or theories relate to the purpose
of our thesis.
Some of the papers reviewed have an exclusive focus on either bias identiÔ¨Åcation or mitigation, while
others present techniques for both identiÔ¨Åcation and mitigation. To structure the review, we will Ô¨Årst
present papers that focus mainly on identiÔ¨Åcation and then present papers that focus mainly on mitiga-
tion. This structure serves a purpose of building a vocabulary for bias identiÔ¨Åcation and mitigation in a
meaningful way since bias identiÔ¨Åcation, to a certain extent, is a necessary precursor of bias mitigation
and because being able to identify bias plays a central role in knowing whether bias has been mitigated.
5.1 IdentiÔ¨Åcation of bias
The papers reviewed in the following sections present techniques for bias identiÔ¨Åcation.
5.1.1 Fairness Through Awareness
In their 2011 research paper, Dwork et al. study fairness in classiÔ¨Åcation. They describe the challenge
of fairness as "achieving utility in classiÔ¨Åcation for some purpose, while at the same time preventing dis-
crimination against protected population subgroups" [16]. They assess fairness on two levels, individual
fairness and group fairness.
Individual fairness
They capture individual fairness "by the principle that two individuals who are similar with respect to
a particular task should be classiÔ¨Åed similarly" [16, p. 1]. They state that a mapping M:V!(A),
which maps individuals to a probability distribution over outcomes (A), satisÔ¨Åes the Lipschitz property
for everyx;y(pair of individuals) 2V, if:
D(M(x);M(y))d(x;y) (5)
This ensures that the distance between predicted labels of any two individuals is less than or equal to
the distance between the features that were used for prediction where D()andd()are some measures of
similarity.
If incorporated into a classiÔ¨Åer by adding it as a constraint in the optimization, individual fairness
can be achieved by minimizing some arbitrary loss subject to the individual fairness constraint. In this
case, the machine learning model output is a mapping that has to satisfy the Lipschitz property.
Group fairness
The authors capture group fairness or statistical parity as "the property that the demographics of those
receiving positive (negative) classiÔ¨Åcation are identical to the demographics of the population as a whole"
[16, p. 2]. In other words, that the group receiving positive classiÔ¨Åcation resembles the group receiving
negative classiÔ¨Åcation to a high degree. A mapping M:V!(A), satisÔ¨Åes group fairness between
distributions S and T up to bias if:
D(M(s);M(t)) (6)
Wheres2S,t2TandDis the statistical distance. The lower is, the more the groups receiving
positive (negative) classiÔ¨Åcations should resemble each other.
75 LITERATURE REVIEW
InsuÔ¨Éciency of individual fairness and group fairness
Dwork et al. illustrate the insuÔ¨Éciency of only imposing either individual fairness or group fairness using
a hypothetical distribution between two groups T and S illustrated in Ô¨Ågure 1:
Figure 1: Hypothetical distribution of two groups [16]
In the Ô¨Ågure, S and T denote two diÔ¨Äerent groups; G denotes some treatment, for example, loan approval,
while the dashed line is a threshold. Individuals in S0 and T0 are not approved for a loan (G0), while
individuals in S1 and T1 are approved (G1). Dwork et al. show that imposing group fairness in this
situation would either entail denying or approving all individuals in S and T (hereby treating groups in
the same way) or denying/approving loans for S and T with the same probability. Dwork et al. also
show that only imposing individual fairness could enable discrimination of S since the threshold could
be set at a level so that no individuals in S are approved for a loan.
Furthermore, Dwork et al. highlight three ways in which group fairness is an inadequate notion of
fairness:
‚Ä¢Reduced utility: By maintaining parity between two distributions, the utility of some classiÔ¨Åcation
can be reduced if there exists a desirable property that is very diÔ¨Äerently allocated between the
two distributions.
‚Ä¢Self-fulÔ¨Ålling prophecy: By deliberately choosing an unqualiÔ¨Åed subset of a particular group,
decision-makers can "justify" discrimination against this particular group in the future while main-
tainingstatisticalparity. ThispracticeisusedbysomeÔ¨Årmsthathaveauditedtheirhiringprocesses
to ensure enough interviews with minority candidates.
‚Ä¢Subset targeting: Group fairness for distributions does not imply group fairness for subsets of
the same distributions. Decision-makers can target a subset within the two distributions that
is not equally represented in each of the distributions, hereby targeting a speciÔ¨Åc demographic
deliberately while maintaining group fairness. For example, exposing an advertisement equally to
two groups, does not inhibit a target subset (those who are expected to click on the add) from
being very unevenly distributed between the two groups.
Relevance to the thesis
TheDworketal. paperishighlycitedandrepresentsanearlyattemptatcreatingatheoreticalframework
that can deal with algorithmic fairness. The mathematical notions of individual and group fairness are
beneÔ¨Åcial concepts used in many of the papers in this review. Furthermore, the authors show the dangers
of solely relying on satisfying individual or group fairness and how discrimination can still exist in perfect
accordance within both notions of fairness.
85 LITERATURE REVIEW
5.1.2 Learning ClassiÔ¨Åcation without Disparate Mistreatment
Zafar et al. [17] introduce three notions of unfairness. The Ô¨Årst two, disparate treatment anddisparate
impact, are applicable in scenarios where there is no available or reliable ground truth. The third, dis-
parate mistreatment , is applicable when ground truth is available and reliable.
Disparate treatment
Disparate treatment arises when a model creates diÔ¨Äerent outputs for groups that are similar on all
characteristics except a sensitive attribute. Regarding gender bias, disparate treatment could correspond
tothenotionthattwosimilarindividualsshouldnotbetreateddiÔ¨ÄerentlyonlybecausetheyhavediÔ¨Äerent
genders [17]. For avoiding disparate treatment in a binary classiÔ¨Åer, the following must apply:
P(^yjx;z) =P(^y;x) (7)
Where x are non-sensitive attributes, y is the output, and z is a sensitive attribute, for example, gender.
Disparate impact
Disparate impact arises when a model‚Äôs output beneÔ¨Åts a group of people with the same values of a
sensitive attribute more frequently than other groups. Concerning gender, disparate impact is when the
fraction of males and females that beneÔ¨Åt from a system is diÔ¨Äerent [17]. To avoid disparate impact in
a binary classiÔ¨Åer, the following must apply:
P(^y= 1jz= 0) =P(^y= 1;z= 1) (8)
Where z is a binary sensitive attribute.
Disparate mistreatment
The authors state that a model suÔ¨Äers from disparate mistreatment concerning a given sensitive attribute
if the misclassiÔ¨Åcation rates diÔ¨Äer for groups of people with diÔ¨Äerent values of that sensitive attribute.
A binary classiÔ¨Åer does not suÔ¨Äer from disparate mistreatment if the misclassiÔ¨Åcation rates are the
same across groups with diÔ¨Äerent values of the sensitive feature z. The misclassiÔ¨Åcation rate can be
described in several ways. Zafar et al. investigate disparate mistreatment in terms of the overall mis-
classiÔ¨Åcation rate (OMR), false-positive rate (FPR), and false-negative rate (FNR). To avoid disparate
mistreatment in a binary classiÔ¨Åer, the following must apply for each of the above terms, respectively:
Overall misclassiÔ¨Åcation rate (OMR)
P(^y6=yjz= 0) =P(^y6=y;z= 1) (9)
False positive rate (FPR)
P(^y= 1jz= 0;y= 0) =P(^y= 1jz= 1;y= 0) (10)
False negative rate (FNR)
P(^y= 0jz= 0;y= 1) =P(^y= 0jz= 1;y= 1) (11)
Relevance to the thesis
The paper by Zafar et al. provides a bias identiÔ¨Åcation technique. It does so in two scenarios: 1) when
there is no reliable ground truth and 2) when there is reliable ground truth. In our thesis we assume
that the ground truth is reliable (registered falls). Therefore, disparate mistreatment could be a relevant
bias identiÔ¨Åcation technique.
95 LITERATURE REVIEW
5.1.3 The Blinder‚ÄìOaxaca decomposition for linear regression models
The Blinder-Oaxaca decomposition method divides an observed diÔ¨Äerence in an outcome between groups
into distinct components when using linear regression models. This decomposition can be used to identify
potential discrimination or bias. In [18], Blinder-Oaxaca decomposition is used to study the diÔ¨Äerences
in wages between men and women. The author, Jann, decomposes the diÔ¨Äerence into three components,
seen from the viewpoint of women. To explain the decomposition method, we use the wage diÔ¨Äerences
between men and women as an example. The diÔ¨Äerences regarding mean wage between men and women,
R, can be written as:
R=E+C+I (12)
The Ô¨Årst component, E, relates to the part of the diÔ¨Äerential in outcome that is due to group diÔ¨Äerences
in the predictors. Jann calls this the "endowments eÔ¨Äect". Edescribes how the wage of the average
woman would change if she had the predictors of the average man:
E=fE(Xm) E(Xf)gf (13)
The second component, C, is the contribution of the diÔ¨Äerences in the coeÔ¨Écients. Cdescribes how the
wage of the average woman would change if she was "treated" in the same way as a man or, in other
words, had the same coeÔ¨Écients [18]:
C=E(Xf)(m f) (14)
The third component, I, is an interaction term. It accounts for the diÔ¨Äerence in endowments and
coeÔ¨Écients that exist concurrently between the two groups:
I=fE(Xm) E(Xf)g(m f) (15)
The three components together compose the outcome diÔ¨Äerence, R. All the equations above take the
viewpoint of females. The diÔ¨Äerence, R, could also have been applied to the viewpoint of males.
Relevance to the thesis
The method could be used for bias identiÔ¨Åcation since the decomposition explains the mean diÔ¨Äerence
of the outcome variable, which in the AIR case can be translated to the mean diÔ¨Äerence in predicted
risk of falling between, for example, male and female citizens. The decomposition technique is primarily
used in a linear regression setting and could thus be relevant if we chose to use a linear regression model
on the AIR data.
105 LITERATURE REVIEW
5.2 Bias mitigation
When reviewing bias mitigation techniques, we use a distinction from Calmon et al. [19] who deÔ¨Åne three
areas of interest that can be used when assessing techniques to mitigate bias, namely pre-processing, in-
processing, and post-processing. This distinction provides a meaningful way to relate the techniques to
one another.
Pre-processing
5.2.1 Learning Fair Representations
In their 2013 paper Zemel et al. [20] propose a learning algorithm that attempts to achieve both group
fairness and individual fairness. They formulate an optimization problem with the goal of Ô¨Ånding an
alternative representation that encodes the data as well as possible while obfuscating information about
membership of protected groups (e.g. race or gender). In a way that "lose[s] any information that can
identify whether the person belongs to the protected subgroup, while retaining as much other information
as possible" [20, p. 2].
The original data, X, is represented by kvectors,vk, that have the same dimensionality as x (the
number of features). The vector, vk, is called a prototype, and the set of k vectors is called Z.
The original outcome, Y, is also represented by kvectors,wk, with the same dimensionality as y (one-
dimensional). The algorithm attempts to learn values of vkandwkthat achieve three explicit goals,
which are that:
1. the mapping from XtoZsatisÔ¨Åes group fairness
2. the mapping to Z-space retains as much information from X (except for membership of protected
groups)
3. the induced mapping from XtoY(throughZ) is as close to the original mapping between Xto
Yas possible.
vkandwkare the only two parameters that need to be learned.
When calculating the new values of an observation, ^xn, the learned vkis multiplied with a probability
mappingMn;k:
^xn=KX
k=1Mn;kvk (16)
Mn;kcontains a value related to each vector in the set of vk, governing the mapping between the inter-
mediateZ-space representation of xand the new ^x. Each row in Mn;ksums to 1 and can be interpreted
as a probability mapping of a given observation to the kvectors.
If k=2, a combination of v1andv2is theZ-space representation of the original data. When calcu-
lating ^xnfor the Ô¨Årst person in the data set, then n=1. Then the new data for the person is found
by:
^x1=2X
k=1M1;kvk=M1;1v1+M1;2v2 (17)
When calculating ^yn, the learned wkis multiplied with the same probability mapping Mn;k:
^yn=KX
k=1Mn;kwk (18)
In order to achieve group fairness, a probability mapping is created for the protected group, M+
k, and a
mapping for the non-protected group, M 
k.
The setup yields a learning system that attempts to minimize the following loss function:
115 LITERATURE REVIEW
L=AzLz+AxLx+AyLy (19)
Az;AxandAyare hyper parameters governing trade-oÔ¨Äs.
Lzis used to achieve group fairness by ensuring that the diÔ¨Äerence between the mapping of individ-
uals in/not in protected groups to the set of prototypes ( M+
k;M 
k) is as small as possible:
Lz=KX
k=1jM+
k M 
kj (20)
Lxis used to achieve that the mapping of xinto^xnthroughZis a good description of x, quantiÔ¨Åed by
a squared error measure.
Lx=NX
n=1(xn ^xn)2(21)
Lyrequires that the predictions are as accurate as possible:
Ly=NX
n=1 ynlog ^yn (1 yn) log(1 ^yn) (22)
By Ô¨Ånding an intermediate representation of the data, the approach can be used for any classiÔ¨Åcation
task by applying a classiÔ¨Åer to the intermediate representation of the data instead of the original data.
In this way, the approach can be used to turn any classiÔ¨Åcation algorithm into a fair classiÔ¨Åer without
having access to the algorithm itself. This is in line with the philosophy of the authors, which revolves
around achieving fairness through establishing a "two-step system construction by two parties: an im-
partial party attempting to enforce fairness, and a vendor attempting to classify individuals" [20, p. 4].
The authors compare their model to the performance of other models with respect to accuracy,dis-
crimination , andconsistency .Accuracy measures the accuracy of model predictions, and discrimination
measures bias with respect to the classiÔ¨Åcations and the sensitive feature(s) in the data. Consistency
compares the model‚Äôs classiÔ¨Åcation of a given data point to its k-nearest neighbors. The discrimination
measure is a form of groups fairness, while the consistency measures is a form of individual fairness. Re-
sults from tests on three diÔ¨Äerent data sets show that their technique removes discrimination (achieves
group fairness), maintains accuracy, achieves individual fairness, and successfully obfuscates information
about membership of protected groups.
Relevance to the thesis
Zemel et al. present a pre-processing mitigation technique through learning fair representations of the
data on which any classiÔ¨Åer can be trained. Mitigating bias through the technique presented here seems
relevant in the AIR case.
5.2.2 Optimized Pre-Processing for Discrimination Prevention
Calmon et al. [19] presents an optimization technique for producing transformations that can mitigate
bias (which they call discrimination). The goal is to determine a mapping P(^X;^YjX;Y;D ), where Y is
an outcome, X denotes non-protected variables and D denotes protected variables. The mapping has the
following goals:
1. Discrimination control, which governs the disparity in predictions between groups.
2. Distortion control, that ensures similarity between the original data and the transformed data.
3. Utility preservation, which controls that a model learned from the transformed data set is not too
diÔ¨Äerent from the one learned from the original data set.
According to Calmon et al. discrimination is : "(...) the prejudicial treatment of an individual based on
membership of a legally protected group such as a race or gender" [19, p. 1]. Direct discrimination occurs
when protected variables are used explicitly in decision-making, whereas indirect discrimination occurs
when protected variables are not used, but variables correlated with them lead to diÔ¨Äerent outcomes
125 LITERATURE REVIEW
for diÔ¨Äerent groups. Simply removing a protected variable is not enough, because it does not address
indirect discrimination - and can in fact conceal it.
Discrimination control
Discrimination control serves the purpose of limiting dependence of the transformed outcome ^Yon the
protected variable D. In other words, that the distribution of the transformed outcome for any value of
D,p(^YjD), resembles the distribution of the outcome for all groups, P(Y). This is ensured by adding a
constraint to the optimization, where the ratio between the two distributions cannot be larger than a
certain value that is controlled by a parameter .
Distortion control
Distortion control ensures that an observation (x) and its corresponding outcome (y) for a given person
in the original data (x,y) should have values in the transformed data set ( ^x,^y) that are as close to the
original values as possible. This constraint restricts the mapping to avoid certain larger changes - for
example, that a very low credit score is mapped to a very high credit score.
Utility preservation
Utility preservation ensures that the model from the transformed data set is not too diÔ¨Äerent from the
one learned on the original data. Utility preservation is achieved through the deÔ¨Ånition of the objective
function in the optimization that learns the transformed data set. The objective function is deÔ¨Åned by
a dissimilarity between the original distribution of (X, Y) and the new learned distribution of ( ^X,^Y).
Therefore, minimizing the objective function will preserve the utility of the data set.
Experimental results
The paper has applied their approach to both the COMPAS data set and the UCI Adult data set. The
approach has successfully decreased the discrimination but with a decrease in accuracy.
Figure 2: Top row relates results on the COMPAS data set and bottom row for UCI Adult data set.
First column is logistic regression (LR), and second column is random forests (RF) [19].
The models without mitigation in both the LR and RF have the highest AUC but also the highest
discrimination.
Dropping the protected variable (dropping D) reduces the discrimination of the models. In the COM-
PAS case, the AUC does not change, while it decreases the discrimination. In the UCI Adult case, the
discrimination decreases almost to the same level as the approach of the authors, but with a higher AUC
than Calmon et al.
135 LITERATURE REVIEW
Learning fair representation (LFR) (see section 5.2.1) reduces the discrimination the most on the COM-
PAS data set while preserving a higher AUC than Calmon et al.‚Äôs approach. However, on the UCI Adult
data set, the eÔ¨Äect of LFR is minimal.
Across the two cases, Calmon et al.‚Äôs approach for mitigating discrimination performs well in decreasing
the discrimination but does so with a cost in AUC.
Relevance to the thesis
Calmon et al. present a technique to learn a transformation of the data that preserve utility and controls
discrimination and distortion. They compare their approach to the approach of Zemel et al. (LFR) and
to a simple mitigation technique of dropping the protected variable from the model. The techniques
presented by Calmon et al. could be relevant for our thesis. Since Calmon et al. learns a transformed
data set, which any classiÔ¨Åer can use, hereby mitigating bias, the approach has much in common with
the two-step strategy presented in Zemel et al.
5.2.3 Certifying and removing disparate impact
Feldman et al. [21] propose a method for identifying and removing bias. Their methods are rooted in
the notion of the 80% rule . The 80% rule is advocated by the US Equal Employment Opportunity Com-
mission and states that a selection procedure violates the 80% rule if the group with lowest passing rate
has a passing rate that is less than 80% of the passing rate of the group with the highest passing rate [22].
In a scenario where gender is the only relevant protected group characteristic, and we are assessing
a hiring process, and men have the highest employment rate, then the 80% rule would be violated, if
the employment rate of women is lower than 80% of the employment rate of men. This can be written
mathematically as:
Pr(C=HiredjX=Female )
Pr(C=HiredjX=Male )0:8 (23)
Where,
D = (X,Y,C), is a data set.
X: The protected attribute (for example sex, race, religion)
Y: The remaining attributes
C: Binary class (hired/not hired)
Feldman et al. adopt the same concept to be used on classiÔ¨Åcation algorithms, in the sense that the
predictions of an algorithm on groups with protected attributes must also comply with the 80% rule. If
the algorithm does not, the authors state that it exhibits disparate impact .
Removing Disparate Impact
The paper by Feldman et al. [21] proposes a disparate impact removal algorithm that has the following
goals:
‚Ä¢It should not be possible to predict the protected variable (X) from the remaining features (Y)
‚Ä¢Widely diÔ¨Äerent outcomes for diÔ¨Äerent groups related to a protected classes (disparate impact)
should be avoided
The removal algorithm is run on the non-protected features Y and returns ^Y. Combining ^Ywith the
original X and C gives an unbiased data set, ^D=(X, ^Y,C), also called a repaired version of D.
The repaired value of a non-protected variable, ^Y, is found in the following way:
145 LITERATURE REVIEW
‚Ä¢Calculate the cumulative density function, F, and quantile function, F 1, of a variable Y for every
value of x2X.
‚Ä¢Find the percentile score of an observation by putting Y into the Fxof the group x that the
observation is in.
‚Ä¢Find the new repaired ^Yfor the observation by taking the median of the results from putting the
percentile score into the quantile functions for every x 2X. This "median" quantile function is
calledF 1
A.
Repairing a non-protected variable in this way both preserves the within group ranking of observations,
while it makes it diÔ¨Écult to separate the protected groups based on their values of the repaired non-
protected variable.
Example: Finding repaired SAT scores with Disparate Impact Removal
We intend to Ô¨Ånd a repaired SAT score (entrance exam scores for universities and colleges), ^Y, for fe-
males and males. We start by calculating the cumulative distribution functions and quantile functions
for females and males that map SAT scores with percentiles and vice-versa:
Ffemale (SAT ) =percentile (24)
Fmale(SAT ) =percentile (25)
F 1
female(percentile ) =SAT (26)
F 1
male(percentile ) =SAT (27)
A male who originally had a SAT score of 500 has an associated percentile of:
Fmale(500) = 0:95 (28)
His repaired SAT score is calculated as follows:
^y=median (F 1
female(0:95); F 1
male(0:95)) = 625 (29)
Figure 3 shows the distributions of hypothetical SAT scores for females and males, where the black curve
represents the repaired SAT score distribution.
Figure 3: Hypothetical distributions of SAT scores for females (blue) and males (red). The fully
repaired data is represented by the distribution in black [21].
155 LITERATURE REVIEW
Partial repair
Feldman et al. [21] also tested out partial repair techniques. A partial repair governs how much the
distributions move towards each other. The repair ranges from 0 to 1, and the distributions are shifted
towards the median distribution as the repair level increases. Repair level = 1 is the same as the repair
method shown above.
Results
Feldman et al. [21] have tested their disparate impact removal technique on three data sets (Adult In-
come, German Credit, and Ricci data sets) that all have protected attributes in the data. Results show
that across all data sets, using repair levels above 0.75 ensure that the tested classiÔ¨Åers comply with 80%
rule.
Relevance to the thesis
Feldman et al. [21] propose a notion of fairness, disparate impact, that uses the relation between the
positive predictions for two groups (for example, females and males). Furthermore, the authors use a
threshold based on legally founded considerations about disparate impact, the so-called 80% rule. The
notiondisparate impact canbeusedforbiasidentiÔ¨ÅcationintheAIRdataset, andthe80%rulecouldalso
be a tool for discussing the fairness of a data set. The authors‚Äô method for removing disparate impact can
be used as a mitigation technique on the AIR data set. The use of disparate impact as a notion of fairness
also relates to equal opportunity by Hardt et al. [23]. Hardt et al. use the notion when postprocessing a
predictor, whereas Feldman et al. use the notion when preprocessing data for removing disparate impact.
5.2.4 Gender Bias in Contextualized Word Embeddings
Zhao et al. [3] examine the quantiÔ¨Åcation and mitigation of gender bias in the Ô¨Åeld of NLP. In NLP, a
model has learned vector representations of words. These vector representations, also called word embed-
dings, are the focal point of the Zhao et al. paper. They examine how two word embeddings, GloVe and
ELMo, reproduce gender bias in their embedding values. They propose a mitigation technique, gender
swap, that attempts to mitigate bias by data augmentation where gender related words are swapped. To
evaluate whether mitigation has worked, they train an SVM on both the original and gender swapped
versions of the data and assess its performance.
Experimental setup
The authors used the WinoBias data set (a word corpus), which is a data set consisting of sentences
with entities corresponding to people, referred to by their occupation. For example, ‚Äúthe engineer went
back to his home‚Äù is such a sentence, where the entity engineer is the occupation. WinoBias is created
to explore gender bias and is divided into two subsets: pro-stereotype and anti-stereotype. Sentences in
the pro-stereotype data set have pronouns associated with gender stereotypes (for example, "the nurse
rides her bike"). In the anti-stereotype data set, the opposite is true (for example, "the nurse rides his
bike").
To identify gender bias, the authors train an SVM to classify the gender of an entity in a sentence, based
on the embedding values of the words used in the sentence. For example, "the nurse rides her bike"
should be classiÔ¨Åed as "female". They test an SVM on both the pro-stereotypical and anti-stereotypical
data sets. If the performance of the SVM tested on the pro-stereotypical data set is substantially better
than the performance of the SVM tested on the anti-stereotypical data set, then the authors identify
that gender bias has been propagated into the word embeddings.
To test whether gender swap mitigates bias, they train an SVM on the original data and on the gender
swapped data set. If the performances of the SVM trained on the gender swapped data have similar
performances on the pro-stereotypical and anti-stereotypical, then bias has been mitigated.
Gender swap
Gender swap is done by replacing gender-revealing entities in the data set with words indicating the
opposite gender. For example, the sentence ‚Äúthe engineer went back to her home‚Äù has a swapped ver-
sion: ‚Äúthe engineer went back to his home‚Äù. The gender swapped version of the original data set is then
concatenated with the original data, creating the gender swapped data set (illustrated in Fig. 4).
165 LITERATURE REVIEW
Figure 4: Gender swapped data set: The union of a gender-swapped version of the original data set
and the original data [3]. Illustration made by Daniel Vigild Juh√°sz and Lau Johansson.
Results
Table2showstheF1-scoresoftheSVMstrainedonrespectivelytheoriginaldataandthegenderswapped
data, when tested on the pro-stereotypical and anti-stereotypical WinoBias sentences.
Data Pro. (F1) Anti. (F1) |DiÔ¨Äerence|
Original data set 79.1 49.5 29.6
Gender swapped data set 65.9 64.9 1.0
Table 2: The F1-score of SVM predictions on the pro-stereotype and anti-stereotype data.
Data augmentation reduces the diÔ¨Äerence in F1-score percentage points between the two data sets (pro-
and anti-stereotype) from 29.6 to 1.0. The results show that gender swap is largely eÔ¨Äective at mitigating
the bias found in the embeddings of GloVe and ELMo [3].
Relevance to the thesis
The mitigation technique performed by Zhao et al. shows that swapping genders in the word corpus as
data augmentation could reduce gender bias. Since the methods tested in the paper relate to NLP, and
the bias is expressed in terms of how well the SVM performs in classifying the gender of an entity, the
approach cannot directly be applied in the thesis. However, the data augmentation technique (swapping
genders) could be tested on the AIR data set to examine if bias can be mitigated.
175 LITERATURE REVIEW
In-processing
5.2.5 Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word
Embeddings
Intheir2016paper, Bolukbasietal. [24]pointtotheriskofamplifyingbiasespresentinwordembeddings
and oÔ¨Äer a solution to debiasing word embeddings. They Ô¨Ånd evidence of gender biases in word embed-
dings. For example, MAN is to COMPUTER-PROGRAMMER as WOMAN is to HOMEMAKER. In
their paper, they distinguish between two types of bias: direct bias andindirect bias :
‚Ä¢Direct bias: The authors describe direct bias as "an association between a gender-neutral word and
a clear gender pair" [24, p. 3]. Here, they make an important distinction between gender-neutral
words(for example, nurse, doctor, football, softball) and gender-speciÔ¨Åc words (for example, man,
woman, uncle, aunt). Gender-speciÔ¨Åc words form gender pairs, for example, he-she and king-queen.
An example of direct bias is that they Ô¨Ånd that the word embedding of nurseis much closer to
womanthan toman. In other words, there is direct gender bias when a gender-neutral word, e.g.,
nurse, has an unequal distance to a clear gender pair.
‚Ä¢Indirect bias: The authors describe indirect bias as "associations between gender neutral words
that are clearly arising from gender" [24, p. 4]. An example of this is that the embedding of the
wordreceptionist is closer to softballthan tofootball. Here,receptionist andsoftballare typically
associated with females, while footballis associated with males.
To illustrate the potential harms of these biases, they oÔ¨Äer a hypothetical example of an algorithm with
the task of retrieving relevant web pages for some query. If word embeddings are used to improve the
order of web page results, then the biased embeddings could have real-world outcomes. For example,
when searching for qualiÔ¨Åed candidates for a job opening as a computer programmer, the embeddings
will lead to gender-biased search results that disadvantage female candidates.
Debiasing
The authors present debiasing as a way to reduce gender bias in the word embeddings. The goals of
debiasing are as follow:
‚Ä¢Reduce bias: a) "ensure that gender-neutral words such as nurse are equidistant between gender
pairs such as she-he" [24, p. 4] and b) reduce biased gender associations between gender-neutral
words.
‚Ä¢Maintain embedding utility: a) maintenance of meaningful associations that are not gender related
between gender-neutral words, and b) to maintain deÔ¨Ånitional gender associations, for example,
between manandfather.
The Ô¨Årst step towards debiasing is identiÔ¨Åcaiton of the gender subspace , which is a direction in the em-
bedding space that describes the diÔ¨Äuse female-male notion. By aggregating over multiple diÔ¨Äerences in
word pair directions, for example, !she   !heand     !woman   !man, and computing their principle compo-
nents, the authors identify a single direction, which explains the majority of the variance.
This direction gis deÔ¨Åned as a general gender direction that can be used to calculate both direct and
indirect bias and to debias word embeddings. Here, the authors present two ways of debiasing: hard
debiasing and soft debiasing.
‚Ä¢Hard debiasing: Ensures that all gender-neutral words have value zero in the gender direction
gand that the words are equidistant to all words in the gender speciÔ¨Åc word sets, for example,
man-woman, grandmother-grandfather etc.
‚Ä¢Soft debiasing: Neutralizes embeddings and equalizes to a degree, using a trade-oÔ¨Ä parameter,
while maintaining as much similarity to the original embedding as possible.
When evaluating the performance of hard debiased and soft debiased embeddings for diÔ¨Äerent tasks, the
authors Ô¨Ånd that the new embeddings and transformations do not negatively impact the performance.
They test the embeddings by assessing the characteristics of their generated analogies.
185 LITERATURE REVIEW
InÔ¨Ågure5,theoriginalembeddingsgeneratethemoststereotypicalanalogies(blue),andbyapplyinghard
debiasing (green), the analogies become less stereotypical. Consider the analogy puzzle, HE to DOCTOR
is as SHE to X. The original embedding returns X = NURSE, while the hard-debiased embedding Ô¨Ånds
X = PHYSICIAN.
Figure 5: Debiasing word embeddings [24].
Relevance to the thesis
The paper by Bolukbasi et al. [24] relates to bias mitigation in the Ô¨Åeld of NLP. The natural structure
of language has (in some cases) incorporated gender associations (for example fatherandqueen). The
modeling task of the AIR case is not NLP related, and therefore, the technique cannot be applied to
the AIR model. However, the review of this article gives insight into how bias is mitigated within other
machine learning areas.
Post-processing
5.2.6 Equality of Opportunity in Supervised Learning
Hardt et al. [23] propose two ways of bias identiÔ¨Åcation with respect to a protected attribute and a
method for post-processing bias mitigation by learning a fair predictor from the predictions of a model.
Hardt et al. introduce two notions related to bias: equalized odds andequal opportunity .
Equalized odds
"A predictor ^YsatisÔ¨Åes equalized odds with respect to protected attribute, A, and outcome, Y, if ^Yand
A are independent conditional on Y." [23, p. 2]. The paper focuses on binary targets, and in that case,
equalized odds is equivalent to:
Pr(^Y= 1jA= 0;Y=y) =Pr(^Y= 1jA= 1;Y=y);y2f0;1g (30)
Equalized odds requires that a classiÔ¨Åer generates equal true positive rates and equal false positive rates
across two demographics (A=0 and A=1), e.g., females and males. According to the authors, if a pre-
dictor satisÔ¨Åes equalized odds, then it would be deemed "unbiased".
Equal opportunity
Equal opportunity is a relaxation of equalized odds since it only requires equal true positive rate between
two demographics:
Pr(^Y= 1jA= 0;Y= 1) =Pr(^Y= 1jA= 1;Y= 1) (31)
The notion comes from the emphasis of Y=1 being the advantageous outcome, for example, receiving
a loan or promotion. The notion then requires that both groups have an equal opportunity for the
advantageous outcome. The relaxation is weaker and therefore less "unbiased". However, it allows for
better utility.
195 LITERATURE REVIEW
Deriving the unbiased predictor
The authors mitigate bias as a post-processing step, which means that the data and the model is left
unchanged. They construct an unbiased predictor, ~Y, by deriving it from ^Y.
The unbiased predictor can be derived by a linear program where the objective is to optimize the
accuracy between ~YandY. The program has two constraints: 1) the TPR for females and males should
be equal, and 2) the FPR for females and males should be equal. The two constraints together form the
equalized odds constraint in equation 30.
The unbiased predictor can be derived from a model‚Äôs probability output ^Y. The goal is to Ô¨Ånd an
optimal threshold that satisÔ¨Åes the equalized odds constraint. The optimal solution can Ô¨Ånd diÔ¨Äerent
thresholds, ta, one for each diÔ¨Äerent value of A (e.g. females and males). However, if the ROC curves do
not intersect, then the optimal solution may require randomizing between two thresholds for each group.
The unbiased predictor can also be derived by only using the equal opportunity constraint.
Figure6: Left: Findingoptimalequalizedoddspredictor. Right: Findingoptimalequalizedopportunity
predictor. First axis is false positives rates and second axis true positives rates. [23]
Figure 6 shows the trade-oÔ¨Ä between false positives and true positives for derived predictor ~Y. The blue
area is an achievable region for group A=0 and the green for A=1. If equalized odds is required for a
predictor, then a solution to the linear program is exactly at the intersection between the blue and the
green region (the red star). Since equal opportunity is a relaxation, only the true positive rates should
be equal. This is shown in Ô¨Ågure 6, where the blue and green dot are horisontally aligned.
Results
Hardt et al. test the unbiased predictor on a case study of FICO scores. These scores are used for classi-
Ô¨Åcation purposes in the US to predict creditworthiness. The authors train a classiÔ¨Åer with no constraints
and use the performance of this model as a benchmark. A classiÔ¨Åer subject to the equal opportunity
constraint achieves 92.8 % of the utility of the benchmark classiÔ¨Åer, while a classiÔ¨Åer subject to equal
odds achieves 90.2 % [23].
Relevance to the thesis
Hardt et al. present two measures for identifying bias: equalized odds and equal opportunity. Equalized
odds is equivalent to the notion of disparate impact from the Zafar et al. paper in section 5.1.2 although
Hardt et al. assume a ground truth (y) to be known. In their notion of equal opportunity, they relax the
equalized odds expression, so it only has to be valid for observations where the ground truth is y= 1.
The techniques presented by Hardt et al. could be relevant in the AIR project, since once can assume
that the ground truth y= 1is known.
206 CASE DESCRIPTION: THE AIR PROJECT
6 Case description: The AIR project
This section describes the purpose of the AIR project, the organisational and political context and
presents a Ô¨Çow diagram that explicitly shows where and how the AI (AIR classiÔ¨Åcation algorithm) is
expected to support the caseworkers in their decision-making.
The AIR project (AI Rehabilitation) is a project focusing on using AI for decision support during
the rehabilitation process of elderly citizens in Aalborg Municipality [25]. It is one of several signature
projects that are initiated by the Danish government, Local Government Denmark and Danish Regions.
The signature projects will test the usage of artiÔ¨Åcial intelligence in the Danish public sector [26]. The
project is in a development phase and therefore not implemented as of 2021.
6.1 Purpose
The AIR project seeks to create a "solution based on models and statistics that can support the individual
caseworker‚Äôs assessment of municipality services regarding rehabilitation, particularly training, better use
of aids and fall prevention training" [25]. We focus on a part of the AIR project that explores developing
an AI that can help caseworkers answer the following:
‚Ä¢Who should be oÔ¨Äered fall prevention training?
A key component in answering the question above is the data regarding the granted aids of each citizen.
Aalborg Municipality registers all aids granted to each citizen, and these form, in the words of the project
owner, an "aid-DNA that tells a story about the impairments of the individual citizen" [2, translated from
Danish]. It is this aid-DNA (combinations of aids provided) coupled with data regarding the citizen that
are expected to contain patterns which the AI can use to make predictions that can help the case workers
determine if the citizen should be provided with fall prevention training [2].
6.2 Organisational context
The AIR project is placed within the Referral Unit - Support and Care of Aalborg Municipality, with
Aarhus University and DigiRehab as collaborators. Aarhus University is responsible for developing the
AI,andDigiRehabisaprivatecompanythatAalborgMunicipalityhashiredtoprovideadigitalplatform
which their physiotherapists and homecare workers can use. The Referral Unit deals with the care of
citizens who have "long lasting and continuous needs for rehabilitation, enabling them to - in a strength-
ened way - live a partially independent and meaningful everyday life with assistance from compensated
services and aids" [27, translated from danish]. The unit has two departments, each with 20 employ-
ees: one department deals with referring citizens to rehabilitation, and the other department assesses
what type of service or aid will help the citizen [2]. As of 2021, the unit manages the care of 2.600 citizens.
A key outcome of the AIR project is that the AI will, hopefully, help the caseworkers by identifying
citizens in an automated fashion that are likely to beneÔ¨Åt from services. Since manually handling each
case is very laborious, algorithmic monitoring of the citizens under the care of the Referral Unit could
lead to even more eÔ¨Äective identiÔ¨Åcation of the citizens‚Äô needs [2]. For example, in the case of predicting
the probability of falling, using the AI could help caseworkers identify an aid-DNA that correlates with
a high risk of falling and oÔ¨Äer fall prevention training before the falls occurs. In this light, successful
development and implementation of the AIR project would lead to a signiÔ¨Åcant leap both in terms of
eÔ¨Éciency and eÔ¨Äectiveness for the Referral Unit. The AI can be used to support identifying which citi-
zens to contact and save time in terms of monitoring the 2.600 citizens they have under their care [2].
6.3 Political context
In the Consolidation Act on Social Services of 2015 by The Danish Ministry of Social AÔ¨Äairs and the
Interior, article 83a states that the "municipal council shall oÔ¨Äer a brief and time-limited rehabilitation
program to individuals with functional impairment, if the rehabilitation program is assessed to be able to
improve their functional impairment and thus reduce the need for assistance" [28]. The project owner
conÔ¨Årms that the AIR project can be viewed as a part of Aalborg Municipality‚Äôs eÔ¨Äort towards complying
216 CASE DESCRIPTION: THE AIR PROJECT
with article 83a [2]. The reasoning behind oÔ¨Äering brief and time-limited rehabilitation is two-fold: 1) it
can improve the individual‚Äôs functional impairment and 2) reduce the need for future assistance. In this
light, the AI will guide the Referral Unit in their search for citizens who are expected to beneÔ¨Åt from
a brief and time-limited rehabilitation program. From the article text and interview with the project
owner, it is clear that the political intent of AIR project is to deliver more welfare to citizens, not to
optimize within the current budget.
Moreover, an AIR project team member states that they are more cautious with mistakes regarding
not providing fall prevention training when it is needed than mistakes regarding providing training to
citizens who might not need it [29]. In terms of classiÔ¨Åcation rates, they are more willing to accept higher
false positive rates (FPR) than higher false negative rates (FNR); the consequences of a false positive
and false negative classiÔ¨Åcation support this idea. While a false positive classiÔ¨Åcation would result in the
potential waste of a fall prevention training program, a false negative classiÔ¨Åcation would mean with-
holding valuable training to a citizen who has a high risk of falling. Since falling can have very severe
consequences for elderly citizens, it is clear why the AIR project team chooses to value misclassiÔ¨Åcations
in the way described here.
Finally, citizens have the rights to access records and documents that describe a decision or case regard-
ing the citizen [30]. Therefore, the municipality could be requested by citizens to provide information
regarding bias identiÔ¨Åcation and mitigation eÔ¨Äorts. In this light, simple and intuitive methods might be
easier to communicate to citizens and therefore preferred over more complex methods.
6.4 Flow diagram of the AIR project
This section describes the process of a citizen entering the Referral Unit‚Äôs area of authority related to
the use of the AI model. A citizen can come from multiple sources in the welfare system, for example,
after an accident or functional impairment due to aging. The AI will typically screen citizens who have
been given an aid, although, this is not strictly mandatory in order to be screened and provided fall
prevention training. Figure 7 shows how the typical process of screening and predicting the risk of falling
is envisioned.
Figure 7: A process Ô¨Çow diagram of the AIR project related to assessing whether citizens should be
provided fall prevention training.
A:The Referral Unit receives a case concerning a citizen. Typically, the citizen applies for one or more
aids that the Municipality can provide.
226 CASE DESCRIPTION: THE AIR PROJECT
B:If needed, the Referral Unit decides which aid(s) the citizen should be provided e.g., a walking stick
or a wheelchair.
C:The provision of the aid is registered, and the data is sent to the AI, including information about
e.g., the age of the citizen. After receiving the information, the AI predicts a risk score which is used by
the Referral Unit to assess, whether this aid-DNA represents a well-known pattern, implying that the
citizen has a high risk of falling. This risk score is expected to be based on the probability of falling.
D:TheoutputoftheAI(riskscore)isusedasatoolfordecisionsupportwhenassessingwhetheracitizen
should be provided with fall prevention training. The AI does not make any automated decisions. The
risk score is included as an element in an overall assessment of the individual citizen, where other factors
than the risk score are considered. Each citizen is assessed individually. If it is decided that a citizen
should be provided fall prevention training, the training is ordered from either another municipal service
or DigiRehab. The training program typically lasts for three months, where data regarding training and
potential fall incidents are registered on the digital platform provided by DigiRehab.
6.5 The AIR data set
This section describes the data generating process and the content of the AIR data set. All information
regarding the data set comes from a yet to be published internal document describing the AIR project
and meetings held with Aarhus University [31][29].
6.5.1 Data collection process
The AIR classiÔ¨Åcation model is trained on a data set containing 2144 records with information regarding
citizens, their aids, and whether they have fallen. All data points are collected at the screening time,
except whether or not the citizens have fallen, which refers to whether a fall has occurred within three
months after the screening date. The screening is done by DigiRehab. Some of the observations in the
data set are screenings of citizens who have been in other municipally subsidized programs before or
have been in a DigiRehab program before. They could therefore be in the data set more than once. The
data are collected during 2019 and 2020.
6.5.2 Features of the AIR data set
The features in the AIR data set are described in the table below:
Variable Description Type
Gender The gender of the citizen (0: female, 1: male). Binary
Age Age of the citizen. Integer
Cluster0-19 Result of k-modes clustering of aids into
20 clusters. Cluster information is one-hot-
encoded (e.g. Cluster15 = 1 if observation is
in cluster 15).Binary
LoanPeriod Average number of days that the citizen has
borrowed aids from Aalborg Municipality.Integer
NumberAts Number of aids that the citizen has borrowed
from Aalborg Municipality.Integer
Ats_* One-hot-encoded features for each of the 114
unique aids in the data set (e.g. if a citizen
has a walking stick then Ats_walking_stick
= 1).Binary
Fall Whether the citizen has fallen within three
months after the DigiRehab screening. (0:
"No fall", 1: "Fall")Binary
Table 3: Description of AIR data set
236 CASE DESCRIPTION: THE AIR PROJECT
Most of the features are self-explanatory, but a more detailed description of the features Cluster0-19 is
provided in the following.
6.5.3 The aid cluster values
The aid cluster values are calculated by clustering the Ô¨Årst 50 aids of each citizen into 20 clusters.
The clusters are calculated using K-modes since the aid information is categorical data. K-modes uses a
distancemeasure, forexample, Hammingdistance, tocalculatethediÔ¨Äerencesbetweentheclustercenters
and a given observation. These distances are used when clustering the observations. An important note
is that not all citizens in the data set have 50 aids, but the algorithm uses the 50 Ô¨Årst aids of each citizen,
at the most. The sequence of the aids is also taken into account by the clustering algorithm. The clusters
are Ô¨Ånally one-hot-encoded.
6.5.4 Protected features in the AIR data set
From table 3 we identify two potential protected features: gender and age. Both are frequently used in
the literature as examples of protected characteristics. However, we choose to move forward with
gender as the protected variable since it has a more obvious group distinction (females/males).
Finally, consideration regarding the scope and time limits of the thesis constrains us to choose only one
protected characteristic for further analysis.
6.6 The AIR classiÔ¨Åcation model
Aarhus University has developed the AIR classiÔ¨Åcation model, which is intended to be used to predict
the risk of falling, as shown in Ô¨Ågure 7. They have chosen to build the AI using an XGBoost model. See
section 8.3 for an explanation of the XGBoost model.
247 METHOD
7 Method
The thesis examines how bias can be identiÔ¨Åed and mitigated in models used for decision support in
the public sector, with the AIR project as the primary subject of analysis. This section describes our
method for answering the research questions (see section 3). Followed by a description of our process
regarding the literature review, and a presentation of how we obtained the data used in the AIR project.
7.1 Quantitative method
Mathematical modeling is a method where mathematics is applied to elucidate problems in the real
world. Furthermore, the method consists of analysing models as well as developing mathematical models
[32]. Mathematical modeling can help answering the research questions since building machine learning
models on the AIR data set and analysing the results can elucidate how bias can be identiÔ¨Åed and miti-
gated in the AIR project.
Throughout the thesis, the AIR data set is used to build machine learning models and make descriptive
analysis for understanding the data related to the AIR project‚Äôs algorithm. We have not been a part
of the data collection process but received a pre-processed AIR data set ready for analysis. The data
is owned by Aalborg Municipality and is, therefore, public register data, which can be considered a
reliable data source [33]. Quantitative method requires that results are measurable (quantiÔ¨Åable) [33].
Using quantiÔ¨Åed register data (AIR data set) and building machine learning models on the data provides
quantiÔ¨Åable results, which is why we use quantitative method.
7.2 Interview and online meetings
We conducted a semi-structured interview [34] with AIR project owner Camilla Fibiger Smed (the
respondent), from Aalborg Municipality, to obtain general information about the AIR project. Prior to
theinterview, wepreparedsomequestionsthatweanticipatedwouldhelpgettingagreaterunderstanding
of the AIR project. Since our knowledge regarding the AIR project was limited, we let the respondent
answer the questions openly. Furthermore, we were able to ask in-depth questions as we gained more
insight into the AIR project. The semi-structured interview is a qualitative method [35] and was used in
the thesis in order to gain knowledge about the AIR project that otherwise would not be available. The
interview is referenced as [2]. Christian Marius Lillelund is a member of the AIR project and is one of
Aarhus University‚Äôs employees responsible for building the AIR machine learning model. We have been
in close contact with Christian through several online meetings. These have not been recorded, but we
refer to [29], when information is obtaining through the meetings. We have primarily used Christian
to answer questions regarding the data generation process and the AIR machine learning model. In
the thesis, Christian is often mentioned as the AIR project member from Aarhus University. Christian
prepared a document with a summary of the information regarding the AIR data set that he has relayed
through our meetings. The document is called The internal AIR report and is referenced as [31]. The
report can be found in appendix, section M.
7.3 Obtaining data
7.3.1 Overall process
The master thesis project period runs between ultimo January 2021 and ultimo June 2021. In December
2020, the Ô¨Årst virtual meeting with Aarhus University was held.
Our expectation was that we could obtain the data used in the AIR project in the early stages of the
project period, but due to circumstances regarding data protection and permissions outside of our con-
trol, the data transfer happened the 10th of May.
The data set contains personal information about citizens in Aalborg Municipality (pseudonymized).
Initially, a data processing agreement between Aalborg Municipality and DTU was made, where DTU
Compute‚Äôs Head of Department had to sign. Then, we had to enter into a contract with DTU Compute.
Furthermore, DTU Compute had to create a server where the data could be stored in a secure manner.
To access the data, a virtual machine was created by DTU, which we could access through SSH (secure
257 METHOD
socket shell) from our own computers.
While waiting for the data, we had to Ô¨Ånd a way to continue the project without the AIR data in a
fruitful manner, so that the master thesis would not be delayed. We therefore chose to Ô¨Ånd a placeholder
data set on which we could test identiÔ¨Åcation and mitigation of bias. Furthermore, we also reviewed
literature related to qualitative theory regarding bias in algorithms. This part of the literature review
was however not included as a part of the Ô¨Ånal literature review presented in the thesis but can be found
in the appendix, section G.
As the placeholder data set, we chose to use the COMPAS data set, related to the COMPAS algorithm
used for decision support in pretrial release rulings in the US. The COMPAS case is widely referenced in
the literature as a case of bias in algorithmic decision support. The COMPAS data set contains approx-
imately 7000 records of defendants who have committed a crime. The defendants have been assigned a
COMPAS score; a score which represents the risk of the defendant re-oÔ¨Äending. When used in practice,
a judge will use the predicted score as a decision support tool in her/his ruling regarding pretrial release
of a defendant [36]. Our analysis of the COMPAS data can be found in appendices H, I and J.
7.3.2 Detailed timeline of obtaining the AIR data
December 2020 - before the oÔ¨Écial beginning
At the beginning of December 2020, before the oÔ¨Écial start of the master thesis period, we held a meeting
with two members of the AIR steering group who work at Aarhus University. They have developed the
machine learning model which is intended to be used in the AIR project. They anticipated that we could
get the data as a CSV Ô¨Åle since the data was anonymized. However, we needed to get acceptance from
the rest of the steering group. We were therefore invited to a meeting with the rest of the steering group,
which was to be held in February.
February 2021
The steering group decided to postpone the steering group meeting with us since a group member could
not attend. The meeting was postponed to 9th of March. Since the meeting was postponed, we chose
to review literature related to bias in machine learning. Furthermore, we chose to reach out to the AIR
project owner in order to get information about the project. The interview with the project owner was
conducted on the 8th of February.
March 2021
On the 9th of March, the steering group meeting was held, where we presented what we anticipated
to examine. The steering group found our thesis relevant for the AIR project. DigiRehab contacted a
lawyer from Aalborg Munipacility who was expected to form a contract with us. We wrote an email to
the lawyer, who replied that the data set contained personal information [ personhenf√∏rbar in Danish].
We had to describe our project, what data we required, and how we intended to use it. Furthermore, the
lawyer wrote that the contract would formally be agreed upon between Aalborg Municipality and DTU
- not us as students. A general description of the data was then added to the contract. On the 19th of
March, the contract was sent to DTU‚Äôs legal department for proofreading.
April 2021
On the 14th of April, DTU‚Äôs lawyer evaluated the contract descriptions. They had some legal questions
to the lawyer from Aalborg Municipality. Especially, the lawyers from both DTU and Aalborg Munici-
pality agreed that the data did indeed contain personal information. Therefore, a secure server had to
be created to enable us to work securely with the data. Furthermore, the DTU lawyers wrote that the
head of DTU Compute should sign the contract. This was done on the 26th of April. Ten days before,
on the 16th of April, we had made a contract with DTU Compute to gain access to the data when it
was transferred to the secure server.
May 2021
On the 3rd of May, we signed an agreement to get access to the secure server with an associated virtual
machine. The same day, the server and virtual machine were running. On the 10th of May, we Ô¨Ånally
received the actual AIR data set. On the 12th of May, data was downloaded on the secure server. We
contacted an AIR project member from Aarhus University that had been responsible for transforming
267 METHOD
the data. On the 16th of May, with the help of the project member, we managed to clone the AIR
project GitLab repository to the secure server. We Ô¨Ånally had the fully transformed AIR data in our
possession.
278 THEORY
8 Theory
The following sections describes our bias identiÔ¨Åcation approach, the mitigation techniques, and the
models we use to create estimates of the classiÔ¨Åcation rates on the AIR data set.
8.1 Bias identiÔ¨Åcation
In the following, we describe how classiÔ¨Åcation metrics are used for bias identiÔ¨Åcation, and how the re-
lation between classiÔ¨Åcation rates grouped by genders can be exploited for identifying gender bias. The
comparisons between classiÔ¨Åcation rates of genders will be used to answer RQ1.
Bias classiÔ¨Åcation metrics
In the literature review related to bias identiÔ¨Åcation, we found two overall ways of deÔ¨Åning bias. The
approaches depend on whether the ground truth is available and reliable. If the ground truth is not
available and reliable, then bias must be identiÔ¨Åed using only the predicted outcome. If the ground truth
is available and reliable, bias can be identiÔ¨Åed using both the ground truth and predicted outcome. Since
the actual falls of the citizens in the AIR data set are known, it is possible to examine the predicted falls
of the classiÔ¨Åcation models and compare them to the actual falls. For identifying bias the papers in the
literature review use combinations of classiÔ¨Åcation metrics of the deÔ¨Ånitions from table 1 (TP, FP, TN,
and FN) [17] [21] [23]. When identifying bias, the papers in the literature review compare the actual and
predicted outcomes between demographics, sometimes called protected groups [11], for example, females
and males. We choose to focus on the both correct classiÔ¨Åcations (TP, TN) and misclassiÔ¨Åcations (FP,
FN). Furthermore, since the analysis focus on gender bias, the classiÔ¨Åcation metrics are compared be-
tween females and males. To ensure the gender speciÔ¨Åc metrics are comparable, we use the classiÔ¨Åcation
rates: TPR, TNR, FPR, and FNR. This approach follows [17] [23], who also use classiÔ¨Åcation rates for
identifying bias.
Bias identiÔ¨Åcation technique
The bias identiÔ¨Åcation approach of the thesis consists of two steps. First, we estimate the classiÔ¨Åcation
rates (TPR, FPR, TNR, and FNR) grouped by females and males, respectively. Then, the conÔ¨Ådence
intervals of the estimates of classiÔ¨Åcation rates are compared between the genders to identify
if there is a diÔ¨Äerence. If the conÔ¨Ådence intervals do not overlap, we deem it suÔ¨Éciently probable, that
the diÔ¨Äerence between the genders are large enough to merit further identiÔ¨Åcation of bias.
Second, the relations of the metrics between the genders are assessed using the 80% rule from Feld-
man et al. [21] who use the rule to identify bias regarding disparate impact . In the thesis, we do not
assess disparate impact since we assume that the ground truth is reliable, and if males fall at a higher
rate, a classiÔ¨Åer where bias has been mitigated should still reÔ¨Çect this fact. If men fall at a higher rate
than women, we do not consider this as bias. In stead we choose to use the 80% rule on the
classiÔ¨Åcation metrics: TPR, TNR, FPR, and FNR because the 80% rule provides a region where
the relation between the estimates could be problematic in relation to bias. To calculate the relation of
a classiÔ¨Åcation rate, the estimate of the rate for females is divided by the estimate of the rate for males.
The relation should comply with the 80% rule, which gives a lower boundary of 0.8 and upper boundary
of 1.25. These boundaries should be seen as "guidelines" and not a strict threshold when assessing bias.
The region between the boundaries is called the 80% region . In this light, if the relation between the
genders‚Äô classiÔ¨Åcation rates are clearly outside the 80% region, a classiÔ¨Åer could be biased in terms of
the speciÔ¨Åc metric.
8.2 Bias mitigation
To answer RQ2, we have chosen four mitigation techniques from the literature review that we intend
to test on the AIR data set. These four are: dropping the gender variable, gender swapping,
disparate impact removal, and learning fair representations . After using a mitigation technique
on the AIR data set, we will train a model on the resulting data set and test the model on the original
data. The classiÔ¨Åcation rates of the model built on the bias mitigated data set will be compared with
the original classiÔ¨Åcation rates. The mitigation techniques are chosen on the basis of two considerations:
288 THEORY
varying the level of complexity and relevance for the AIR project and the future work on bias mitigation.
We have deliberately chosen techniques on diÔ¨Äerent levels of complexity to test if adding complexity
will lead to better results. In the AIR context, simple and explainable methods are preferred since it
eases the strain for non-technical employees in the municipality who are expected to work on the project,
and makes it easier to communicate the processes behind a given decision regarding a citizen in Aalborg.
When making recommendations to the AIR project team regarding identifying and mitigating bias, both
the level of complexity and the eÔ¨Äect of the mitigation techniques will be taken into account.
Furthermore, we have chosen only to test pre-processing mitigation techniques. The Zemel et al. paper
[20] presents a philosophy of a two-step system construction where one party attempts to mitigate bias
and another party attempts to maximize their utility of classiÔ¨Åcations on the data set. Practically, this
would imply that one party performs some mitigation eÔ¨Äorts on a data set, and then sends the resulting
data set to another party that learns a model on the data. We are inspired by this system since it makes
the incentives clear for both parties, where one party only wishes to mitigate bias and another party
solely focuses on the speciÔ¨Åc classiÔ¨Åcation task at hand. In this way, the potential trade-oÔ¨Ä between
bias mitigation and utility of the classiÔ¨Åcation is no longer placed on a single party. This approach
will also work if Aalborg Municipality wishes to collaborate with private organizations and companies,
where the algorithms used for prediction might be proprietary. If a two-step system is desired, then only
pre-processing techniques are applicable, since the mitigation eÔ¨Äorts can only be made on the data set
before it is sent to the other party and fed into a given model.
Dropping the gender variable
The idea behind dropping the gender variable comes from the Calmon et al. paper [19], where they use
dropping gender as a benchmark model on which to compare their own mitigation approach and Zemel
et al.‚Äôs learning fair representation technique. The authors note that dropping the protected variable
might not always be an eÔ¨Äective strategy since other variables that are not dropped could be highly
correlated with the protected attribute, which would still enable bias.
We Ô¨Ånd the simple nature of the technique compelling, which also has merit in the context of the
AIR project, where explainability is important. However, we acknowledge the potential issues with the
technique, particularly regarding other attributes highly correlated with gender. Despite these issues, we
Ô¨Ånd it relevant to test the technique on the AIR data set. Figure 8 shows the process of implementing
the dropping gender technique and training/testing the models.
Figure 8: Dropping gender
As can be seen in the Ô¨Ågure, we remove gender from the training set. Afterwards, a model is trained on
the training data set without gender and tested on the test set.
298 THEORY
Gender swap
The idea behind gender swap comes from the Zhao et al. paper [3], where they use the technique on
sentences from a word corpus (WinoBias) that refer to females and males. In the AIR case, the data is
in tabular form, and gender swapping the original data set is done by taking all rows for females and
changing their gender to male and vice-versa. In the Zhao et al. paper, gender swap eÔ¨Äectively mitigated
the biased prediction of a classiÔ¨Åer trained on the embeddings of GloVe and ELMo. The data structure
and learning setup is diÔ¨Äerent in the AIR case, but the technique is still relevant to test. Again, the
mitigation technique is simple and intuitive, which is a compelling characteristic in the AIR context.
Figure 9 shows the process of implementing gender swap and training/testing the models.
Figure 9: Gender swap
As the Ô¨Ågure shows, we take the training set, copy it, swap the genders, and train the model on the
union of the original training set and the gender swapped training set. The model is then tested on the
test set.
Disparate impact removal
The idea behind disparate impact removal comes from the Feldman et al. paper [21], where they use the
technique to ensure that is not possible to predict a protected variable from the remaining features. This
is intended to have the eÔ¨Äect that widely diÔ¨Äerent outcomes for diÔ¨Äerent groups related to a protected
class is avoided. Feldman et al. operationalizes "widely diÔ¨Äerent outcomes" by the 80% rule. It is
important to note that our intended goal of the disparate impact removal technique is notto remove
diÔ¨Äerences in the outcomes between genders since we do not consider it biased if there is a diÔ¨Äerence in
the ground truth between females and males as described above. However, if the models built on the
original AIR data set yield biased classiÔ¨Åcation rates, then one could imagine that augmenting the data
in a way that makes the distributions of variables between females and males as similar as possible might
mitigate the biased classiÔ¨Åcations. Finally, we highlight that the disparate impact removal algorithm can
only be used to change the numerical values, which in the AIR case is Age,Loan period , andNumber
of aids. The remaining features are one-hot-encoded and will therefore be left unchanged when the
technique has been used on the data set. Figure 10 shows the process of implementing disparate impact
removal and training/testing the models.
308 THEORY
Figure 10: Disparate Impact Removal
As can be seen in the Ô¨Ågure, we drop gender from the training set and run the disparate impact removal
algorithm on the training set. Then we train a model on the resulting data set and test the model on a
disparate impact removed test set.
We use an implementation of disparate impact removal from AIF360‚Äôs pre-processing algorithm package
[37].
Learning fair representations
The idea behind learning fair representations comes from the paper of Zemel et al. [20], where they
Ô¨Ånd an alternative representation of the data that encodes the original data as well as possible while
obfuscating information regarding the membership of protected groups. The purpose of the technique
resembles the disparate impact removal algorithm, in the sense that it should not be possible to guess
whether an observation is female or male based on the non-protected attributes. However, learning fair
representations is a more complex model since it allows for an alternative representation of the data that
attempts to mitigate bias, retain as much of the original information as possible, and retain the mapping
between the covariates and the outcome. However, in the same way as disparate impact removal, learning
fair representations can only change the numerical attributes of the AIR data set. Figure 11 shows the
process of implementing learning fair representations and training/testing the models.
Figure 11: Learning Fair Representations
As can be seen in the Ô¨Ågure, we drop gender from the training set and learn a fair representation of
the training set. We use this learned representation to also transform the test into the new represen-
tation space. Then we train a model on the LFR training data set and test the model on the LFR test set.
318 THEORY
We use an implementation of learning fair representations from the AIF360‚Äôs pre-processing algorithm
package [38].
Accuracy and output probabilities
When attempting to mitigate bias, one typically experiences a decrease in model performance since bias
mitigation can be seen as a constraint to a model [19]. Therefore, when implementing a bias mitigation
technique, we will compare the accuracy of the resulting models with models built on the original data
set. This enables us to assess the impact that the bias mitigation technique has had on model perfor-
mance.
We identify a discrepancy between our bias identiÔ¨Åcation method and the intended use of the AIR
model. When implemented, caseworkers in Aalborg Municipality will use the AIR model as a decision
support tool. The model output will most likely be converted to some risk score (e.g., 0-100), directly
mapped to the predicted probabilities of the model. The risk score will then be used to assess each citizen
individually as described in the AIR case presentation in section 6. However, our method for identifying
bias uses the predicted binary classes (fall and no fall), not the predicted probabilities. Therefore, it is
relevanttoassesswhateÔ¨Äectthebiasmitigationtechniquehashadintermsofthepredictedprobabilities.
Because of this, we will compare accuracy and predicted probabilities of models built on data sets
where bias mitigation has been applied, with original models built on the original data set.
8.3 Machine learning models built on the AIR data set
We implement Ô¨Åve machine learning models on the AIR data set: support vector machine (SVM), logistic
regression (LR), random forest (RF), feed-forward neural network (FFNN), and XGBoost. The Ô¨Årst four
(SVM, LR, RF, and FFNN) are chosen by us, while XGBoost is the model that the AIR project team
has chosen to build. We choose to test the bias identiÔ¨Åcation and mitigation techniques across all Ô¨Åve
models. This is done to ensure robustness of our results.
The four models (SVM, LR, RF, and FFNN) are chosen since all of them are frequently used for clas-
siÔ¨Åcation. They are highly accessible, in the sense that many open-source programming languages have
libraries with implementations of them. This is desirable, since our ambition is that our results can be
useful for other projects that evaluate bias in the public sector. Finally, they represent four diÔ¨Äerent ap-
proaches to building machine learning models. Logistic regression represents a regression model, support
vector machine represents instance-based algorithms, random forest is a tree-based ensemble method,
and the feed forward neural network represents a deep learning model [39]. The fact that they come
from diÔ¨Äerent machine learning approaches can be seen as a way to test the identiÔ¨Åcation and mitigation
techniques in a more robust fashion.
This section brieÔ¨Çy describes each model, including the underlying theory and our speciÔ¨Åc implementa-
tion of the models. In the AIR project, the outcome of interest (Fall) is a binary variable. Therefore,
the section covers theory related to binary classiÔ¨Åcation.
Logistic Regression
Logistic regression is a classiÔ¨Åcation algorithm, where the output can be interpreted as posterior proba-
bilities of belonging to a positive class.
Logistic regression uses a function h(x)that is restricted to output values between 0 and 1, which
sum to 1 over the diÔ¨Äerent classes, in this case two classes, by using the sigmoid function [40, p. 119]:
h(x) =1
1 +e Tx(32)
Theare learned parameters governing the mapping from input variables to the predicted output, and
are used to predict the class a of new observation. In this sense, the probability of belonging to a class
given the values of the input variables and parameterized by is:h(x) =P(y= 1jx;). Logistic
regression models are Ô¨Åtted by maximizing the log-likelihood [40, p. 120]. In the binary setting, when
optimizing the model, logistic regression uses the binary cross-entropy loss to evaluate its performance,
328 THEORY
which is equivalent to maximizing the log-likelihood:
Cost (h(x);y) = ylog(h(x)) (1 y)log(1 h(x)) (33)
To optimize the parameters , gradient descent is used to Ô¨Ånd the global minimum of the convex loss
function. Logistic regression is often used as a data analysis tool to understand the role of the inde-
pendent variables in explaining the outcomes. Since each input variable can be given an individual
-parameter, the contribution of input features can be compared to one another [40, p. 121].
For the logistic regression, we use the Scikit-learn library [41] implementation and choose to run the
models with the default settings, except for the maximum number of iterations, where 1000 is used to
achieve convergence [41]. We also set the parameter "probability" to True in order to generate probabil-
ity outputs. Finally, the parameter class weight is set to "balanced" since it is an unbalanced problem
(see section 9). We chose default values to ensure reproducibility and generality of our tests.
Support Vector Machine
The Support Vector Machine algorithm [42] can be used to classify binary data. The algorithm uses a
Support Vector ClassiÔ¨Åer to separate the classes. The Support Vector ClassiÔ¨Åer is a soft margin classi-
Ô¨Åer, meaning that it Ô¨Ånds an optimal separating hyperplane that allows misclassiÔ¨Åcations [40, p. 419].
Because the classiÔ¨Åer allows misclassiÔ¨Åcations, the algorithm is robust to outliers and overlapping clas-
siÔ¨Åcations. The optimal margin is found through cross-validation.
The Support Vector Machine algorithm takes a relatively lower-dimensional data set and calculates
the relationship between observations in a relatively higher dimension and Ô¨Ånds a Support Vector Clas-
siÔ¨Åer that can separate the classes in this higher dimensional representation [40, p. 423]. It does this
since Ô¨Ånding an optimal separator for some distributions is not possible in lower dimensions. To predict
the class of a new observation, SVM maps the observation into the relatively higher space, and the class
is given by where the observation lies in relation to the learned optimal separating hyperplane. To ease
computation, the data is not actually transformed into higher dimensions, but the diÔ¨Äerence between
data points are calculated "as if" the observations were in a higher dimension. This is called the "kernel
trick". The Support Vector Machine uses diÔ¨Äerent kernels to calculate the relationship between observa-
tions in higher dimensions [40, p. 424]. The Scikit-learn implementation uses the radial kernel as default
[42]. The radial kernel is deÔ¨Åned by the following:
e (a b)2(34)
Whereaandbare the coordinates of observations in the dataset and scales the inÔ¨Çuence that observa-
tions have on each other when calculating the relationships. The radial kernel calculates the relationship
in inÔ¨Ånite dimensions.
For the Support Vector Machine, we use the Scikit-learn library implementation and choose to run
the models with the default settings, except for the parameter ‚Äôprobability‚Äô, which we set to True. Fi-
nally, the parameter class weight is set to "balanced".
Random Forest
Random Forest is a tree-based classiÔ¨Åcation algorithm. The algorithm classiÔ¨Åes a new observation by
running the observation through a large number of decision trees and uses the aggregate classiÔ¨Åcation to
classify the observation. Each decision tree in the random forest is created in two steps: 1) creating a
bootstrapped dataset by sampling with replacement from the original dataset, 2) creating a decision tree
using a random subset of variables from the bootstrapped dataset [40, p. 588]. The number of decision
trees in the random forest is a hyperparameter. The Out-Of-Bag dataset is used as a test set to calculate
the Out-Of-Bag error, which estimates the accuracy of the random forest on new data [40, p. 593]. The
accuracy is used to Ô¨Ånd the best number of variables to be used at each step when creating the decision
trees in the random forest.
We use the Scikit-learn library implementation for the Random Forest and choose to run the mod-
els with the default settings. The default number of random variables at each step is the square root of
338 THEORY
the total number of features, while the default number of decision trees in a random forest is 100 [43].
Feed Forward Neural Network
The goal of the FFNN is to approximate any function f*. The function, f*(x), maps the input x to a
category, y. Information Ô¨Çows from x through multiple layers, which together compose many functions.
The number of layers constitutes the depth of network. Each unit in the network can be interpreted
as playing the role of a neuron, which receives an input (e.g., an element of a vector) and compute an
activation value using an activation function [44, p. 164-165].
The forward information Ô¨Çow (forward propagation) produces a scalar cost using a cost-function (C).
Back propagation is the backward Ô¨Çow of information about the cost, which the network uses in order
to compute gradients [44, p. 189]. Back propagation works by Ô¨Ånding the partial derivatives C=w
(gradients), where wis any weight in the network. The gradients are then used for updating the weights
in the network and the network is optimized using gradient descent [45].
Figure 12: An example of a neural network from [45]
The linear transformation of each neuron‚Äôs input is activated by an activation function, which yields a
nonlinear transformation. We use the rectiÔ¨Åed linear activation function (ReLU) as activation function,
since it is recommended as a default activation function [44, p. 170]. The advantage of ReLU is that the
function eases optimization with gradient-based methods. The reason for this is that half of the domain
outputs zero, which implies a more sparse network representation. Furthermore, whenever the unit is
active, the ReLU function is linear and the gradient has a large constant value [44, p. 189]. The ReLU
function is deÔ¨Åned as:
g(z) =max(0;z) (35)
where z=WTx+band W describes the mapping (weights) of the the previous layer‚Äôs output to the
neuron. Each weight is represented by a edge in the graph. b is a bias term [44, p. 189].
The FFNN can be used for binary classiÔ¨Åcation when the last output layer consists of one node where
sigmoid is used as activation function. This gives output values between 0 and 1 [45], which can be
interpreted as probabilities. The FFNN implemented using the AIR data is a fully connected network
with an input layer, an output layer, and four hidden layers. The input layer and hidden layer use ReLU
as an activation function. The input layer and hidden layers also use dropout and batch-normalization
for regularization of the model. The output layer uses sigmoid as an activation function for binary clas-
siÔ¨Åcation. The loss function is the binary cross-entropy loss. The performance of the model is evaluated
using cross-validation. After training, the FFNN can be used for prediction, by sending the features of
a new observation to the input layer and propagating information forward to the output layer. PyTorch
built-in libraries have been applied for the implementation of the model. The model architecture and
the chosen hyper-parameters can be found in appendix, section L.
XGBoost
XGBoost is a library that contains an implementation of gradient-boosted decision trees [46]. Boosting
involves training multiple models in a sequence and then letting the loss of a given model depend on the
previous model‚Äôs performance in the sequence [40, p. 653]. In Figure 13, boosting starts by building a
model and then sequentially boost the performance by building new models [47]. Decision trees can be
used as a boosting framework, where each model in the sequence is a tree [40, p. 654] [48].
348 THEORY
Figure 13: Sequential combination of multiple models [47].
XGBoost (Extreme Gradient Boosting) is a machine learning system for tree boosting [48]. Tree models
can be seen as a model that partition the feature (input) space into regions [49]. To classify a new
observation, XGBoost uses Knumber of functions to predict the output, where each kfunction represent
an independent tree [48]. The predicted outcome ^yof the input xiis found by:
^yi=KX
k=1fk(xi) (36)
wherefkcorresponds to an independent tree. In other words, to make a classiÔ¨Åcation, the input ( xi)
enters each of the Ktrees, uses the decision rules of the tree to classify it into the leaves. The Ô¨Ånal
prediction is calculated by summing up the score in the corresponding leaves.
In order to learn the fttree, two elements are used: 1) the losses of the previous tree (t-1), and 2)
the prediction of the fttree itself [50]. First, the structure of the fttree is learned by considering every
split on each feature. Then, the fttree predicts an output and adds the previous tree‚Äôs output. A
prediction at learning tstep is then deÔ¨Åned by:
^y(t)
i= ^y(t 1)
i +ft(xi) (37)
The structure that is chosen for the t‚Äôth tree, is the one that minimizes the learning problem‚Äôs objective
function [49] [50]:
obj(t)=l(^yi;^y(t 1)
i) +tX
a=1
(fa) (38)
Wherelis a loss function. In a binary classiÔ¨Åcation setting, the loss function is the binary cross-entropy
loss [50]. The 
is a regularization term that regularizes the complexity of the tree, for example, the
number terminal nodes of each individual tree [49]. The objective function is optimized using gradient
descent algorithm [51].
We use the settings as deÔ¨Åned in the AIR XGBoost model found on the AIR project‚Äôs GitLab [52]
which also is found in appendix section L.
Cross-validation
In order to achieve estimates of the classiÔ¨Åcation rates that are as robust as possible, we perform 5-fold
cross-validation when training each classiÔ¨Åcation model on the AIR data set. The 5-fold cross-validation
is repeated ten times, each time with a new random seed. Therefore, each classiÔ¨Åcation rate and accuracy
has been calculated 50 times. For assessing the uncertainty of the models, the 95% conÔ¨Ådence intervals
are shown. The central limit theorem states that for a large number of model samples, the distribution of
sampled means will approach a normal distribution [40, pp. 78-79]. We assume that the distributions of
the metrics are normal and therefore, the conÔ¨Ådence intervals are calculated accordingly. The conÔ¨Ådence
intervals show the range of values for each classiÔ¨Åcation rate that with 95% conÔ¨Ådence contains the true
mean. If the conÔ¨Ådence intervals of the estimate of the classiÔ¨Åcation rates between females and males do
not overlap, we deem it relevant to assess their relation and assess whether it is problematic according
to the 80% rule.
Thresholds
The LR, RF, FFNN, and XGBoost use a classiÔ¨Åcation threshold of 0.5, meaning that if the predicted
probability of a citizen falling is less than 0.5, the binary prediction is "No Fall"; if above 0.5, the binary
prediction is "Fall". Since the SVM does not output probabilities, the Scikit-learn implementation uses
Platt scaling to produce probabilities by training a sigmoid function to map the SVM output to proba-
358 THEORY
bilities [53] [54]. Therefore, SVM has no threshold on the probability outputs as the other four models.
The optimal hyperplane is trained to divide the data into two groups: citizens who fall and not fall. The
logistic regression is, after that, Ô¨Åtted to SVM outputs to generate predicted probabilities.
369 DESCRIPTIVE ANALYSIS
9 Descriptive Analysis
Before identifying bias in the classiÔ¨Åcations of the AIR model and our model implementations on the AIR
data set, we will perform an initial descriptive analysis of the AIR data set to gain some domain knowl-
edge that could be useful when assessing the classiÔ¨Åcation rates and how they diÔ¨Äer between groups. We
will start by showing the distributions and averages of central variables to get an overall understanding
of the data and try to understand how the variables might inÔ¨Çuence the probability of falling. We brieÔ¨Çy
go through descriptive statistics on the variables of the AIR data set. When showing the histograms, we
have truncated the span of values shown since some of the bars in the histograms could be attributed to
one single observation and showing this would not be in compliance with the data protection contracts
we have signed.
Table 4 below shows how many of the citizens in the data set have experienced a fall within the Ô¨Årst three
months of their DigiRehab program. In the table, one can see that approximately 1 out of 5 citizens
experienced a fall. This makes the prediction of falling an unbalanced problem, in the sense that a naive
model which only predicts no citizens will fall could achieve fairly high accuracy of 77.7%.
Fall Count (%)
Has not fallen 1666 (77.7%)
Has fallen 478 (22.3%)
Table 4: Distribution of Fall
Table 5 shows how many females and males there are in the data set. Females account for 2/3 of the
data set, while males account for 1/3. The uneven distribution of gender could be because women on
average live longer than men [55] and that the AIR data population is primarily elderly citizens.
Gender Count (%)
Female 1365 (63.6%)
Male 779 (36.4%)
Table 5: Distribution of Gender
Table 6 shows that a higher proportion of the males experience falling compared to females. However,
284 females in the data set have fallen, compared to 194 males, which is explained by the fact that
women are over-represented, and therefore the proportion of females that fall is less than the proportion
of males that fall. We do not imagine a direct relationship between gender and falling, but rather that
gender is correlated with other variables (observed and unobserved) that have a causal impact on falling,
for example, physical impairments and inactive lifestyles.
Gender Count (%)
Females falling 284 (20.8%)
Males falling 194 (24.9%)
Table 6: Distribution of citizens that falls grouped by gender
Figure 14 shows a histogram of the variable Age. As expected, the data set contains elderly citizens,
most of whom are 70-100 years old, The mean age is 83.1. This validates our previous assumption of
an elderly population, where the consequences of falling, and therefore also the consequences of a false
negative, are quite severe. Finally, we imagine that the probability of falling increases with age.
379 DESCRIPTIVE ANALYSIS
Figure 14: Histogram of age
Figure 15 shows the distribution of the number of aids. The distribution is right-skewed with a top of
around Ô¨Åve aids and a mean of 11 aids. When looking at the raw data, we observe that citizens do not
need to have an aid to be part of the data; in fact, 6% do not have any aids. We expect that number
of aids is positively correlated with the probability of falling since it implies some underlying physical
impairment, which, all things equal, most likely heightens the risk of falling.
Figure 15: Histogram of number of aids
Figure 16 shows the distribution of loan period, which is the number of days a citizen (on average) has
had aids on loan. The mean number of loan days is 1152. The distribution is right-skewed with a spike
for very short loan periods and a plateau for loan periods between 1-3 years, after which the distribution
is monotonically decreasing. The loan period is expected to be positively correlated with the probability
of falling since it shows a lasting need for some aid.
389 DESCRIPTIVE ANALYSIS
Figure 16: Histogram of loan period (days)
Figure 17 shows the number of citizens that belong to each of the clusters. The two clusters with most
citizens are cluster 0 and 10.
Figure 17: Count of citizens in each cluster
Table 7 shows the top Ô¨Åve most loaned aids respectively for the citizens who do not fall and those who
fall. The Ô¨Åve most frequently loaned aids are the same for both groups, while a slightly higher percentage
of those who fall have loaned the top Ô¨Åve most loaned aids.
Aid Count "No Fall" (%) Count "Fall" (%)
Walker 1312 (78.8%) 406 (84.9%)
Shower chair 1193 (71.6%) 361 (75.5%)
Emergency alarm 855 (51.3%) 314 (65.7%)
Toilet seat riser 823 (49.4%) 246 (51.5%)
Bedsore prevention 652 (39.1%) 204 (42.7%)
Table 7: Top Ô¨Åve most loaned aids.
Distribution of variables by gender
The purpose of the thesis is to identify and mitigate bias in relation to gender in the AIR project (see
section 6.5.4). To learn more about potential diÔ¨Äerences in the distribution of central variables between
females and males, we assess boxplots of age,loan period , andnumber of aids .
The distributions of age are in Ô¨Ågure 18 plotted as box plots. The group of citizens who fall (the
two rightmost distributions) have higher median ages than the group of citizens who do not fall (the two
399 DESCRIPTIVE ANALYSIS
leftmost). The group with the lowest median age is males who do not fall, implying that age might to a
higher degree have an eÔ¨Äect on the probability of falling for men.
Figure 18: Box plot of the age grouped by gender and whether group members have fallen.
In Ô¨Ågure 19, the median loan period is shorter for the citizens that fall for each gender. Furthermore,
for the citizens who fall, males have shorter loan periods than females. These insights contradict our
initial expectation, where we imagined that a long loan period would be related to a higher probability
of falling. The opposite seems to be the case since 1) males (who fall at higher rates) have shorter loan
periods and 2) those who fall have shorter loan periods than those who do not fall. If a short loan period
maps to a high probability of falling, and males, in general, have shorter loan periods, this could impact
that males are predicted as falling at a higher rate than females.
Figure 19: Box plot of loan period grouped by gender.
The distributions of the number of aids in Ô¨Ågure 20 are more similar between the four groups in terms of
medians and interquartile ranges. This contradicts our expectation that a higher number of aids leads
to a higher probability of falling. The box plot in Ô¨Ågure 20 therefore leaves a somewhat unclear relation
between the number of aids and the probability of falling, where neither a positive or negative relation
can be read from the plot.
409 DESCRIPTIVE ANALYSIS
Figure 20: Box plot of number of aids grouped by gender.
Correlation between variables by gender
We brieÔ¨Çy present a correlation matrix for each gender between the numerical variables and the fall-
outcome. The information from these matrices could provide an additional understanding of the relation
between the variables for females and males and how these might diÔ¨Äer. Table 8 shows the correlations
for females, and table 9 shows the correlations for males.
Variable Age Loan period Number of aids Fall
Age 1.0 - - -
Loan period 0.0637 1.0 - -
Number of aids -0.1522 0.1767 1.0 -
Fall 0.0235 -0.0268 0.0034 1.0
Table 8: Correlation matrix of numerical features and fall - females
Variable Age Loan period Number of aids Fall
Age 1.0 - - -
Loan period -0.0937 1.0 - -
Number of aids -0.1977 0.3043 1.0 -
Fall 0.1114 -0.1113 -0.0843 1.0
Table 9: Correlation matrix of numerical features and fall - males
When looking at the diÔ¨Äerences between genders regarding the relation between the variable Falland the
other variables, the main diÔ¨Äerence is that the correlations seem to be stronger for males than females,
which can be seen in the diÔ¨Äerence between the correlations of FallandAge/Loan period . This could
imply that the variables Loan period andAgeaÔ¨Äect the probability of falling for men to a higher degree
than for women. Furthermore, Fallis positively correlated with Number of aids for females, while it is
negatively correlated for males. However, the correlation for females is quite close to zero.
When looking at the diÔ¨Äerence between genders regarding the correlation between the variables other
theFall, one can see that Loan period andAgeare positively correlated for females. In contrast, they
are negatively correlated for males. This indicates that the relationship between age and loan period is
diÔ¨Äerent between females and males.
4110 IDENTIFICATION OF BIAS
10 IdentiÔ¨Åcation of bias
For identifying bias in the models trained on the AIR data, we use a two-step strategy. First, we assess
the classiÔ¨Åcation rates across the Ô¨Åve diÔ¨Äerent algorithms and see if the estimates of the rates between
females and males have overlapping conÔ¨Ådence intervals. Second, we calculate the relation between the
estimates of the classiÔ¨Åcation rates and evaluate them in relation to the 80% rule. In other words, for
identifying gender bias, we must both Ô¨Ånd that the conÔ¨Ådence intervals of the classiÔ¨Åcation rates do not
overlap and that the relation between them is problematic according to the 80% rule.
10.1 ClassiÔ¨Åcation rates
In order to evaluate bias in the models used for classiÔ¨Åcation, the four classiÔ¨Åcation rates TPR, FPR,
TNR, and FNR are estimated. These are shown in Ô¨Ågure 21, while the values can also be assessed in the
appendix, table 15 in section A. From Ô¨Ågure 21, we can assess whether the conÔ¨Ådence intervals of the
classiÔ¨Åcation rates for females and males overlap.
4210 IDENTIFICATION OF BIAS
Figure 21: ClassiÔ¨Åcation rates of the Ô¨Åve classiÔ¨Åcation models grouped by gender.
4310 IDENTIFICATION OF BIAS
Support Vector Machine (SVM)
In general, the SVM has higher positive classiÔ¨Åcation rates for men (TPR and FPR) and higher negative
classiÔ¨Åcation rates (TNR and FNR) for women. This shows that the model accurately classiÔ¨Åes males
who actually have experienced a fall as "fallen" at a higher rate than women (TPR) and accurately
classiÔ¨Åes women who do not fall as "not fallen" at a higher rate than men (TNR). Similarly, the model
misclassiÔ¨Åes males who do not fall at a higher rate than females who do not fall (FPR), while also
misclassifying females who fall at a higher rate than males who fall (FNR). As discussed previously,
a misclassiÔ¨Åcation in the form of a false negative has more severe consequences than a false positive
misclassiÔ¨Åcation. In this light, the classiÔ¨Åer‚Äôs potential bias is disadvantageous for women.
Logistic Regression (LR)
The LR exhibits the same pattern between the classiÔ¨Åcation rates as the SVM. Again, LR wrongly
classiÔ¨Åes men at a higher rate in the positive "fall" class and wrongly classiÔ¨Åes women in the negative
"will not fall" class. Although for logistic regression, the false classiÔ¨Åcation rates (FPR and FNR) are
somewhat larger than for SVM.
Random Forest (RF)
For the RF implementation, there is a noteworthy change. First of all, the classiÔ¨Åer almost makes no
mistakes regarding the actual condition negative observations (TNR and FPR), that is, those citizens
who do not fall, with an FPR around 0.05 and a TNR around 0.95. However, when assessing the TPR
and the FNR, random forest has the same characteristics regarding gender as SVM and LR while having
a slightly lower TPR and slightly higher FNR. This means that the classiÔ¨Åer has worse performance on
the actual condition positives, in other words, those who fall. The TPR and FNR show the same pattern
regarding gender as SVM and LR.
Feed Forward Neural Network (FFNN)
The classiÔ¨Åcation rates of the FFNN implementation resemble the RF regarding FPR and TNR. Fur-
thermore, the FFNN model has the highest FNR and lowest TPR of all the implementations. Again,
the same gender misclassiÔ¨Åcation patterns exhibited by the other models can be found in the FFNN.
XGBoost
The XGBoost model exhibits a classiÔ¨Åcation rate pattern somewhat in between that of the RF and
FFNN, which are alike, and the SVM and the LR, which are alike. The FPR and TNR values are closer
to the edge values (0 and 1) than SVM and LR but not as close to the edge as RF and FFNN. The TPR
is higher than the FNR but not as exaggerated as is the case for SVM. Again, in the same way as the
previous models, there is a gender misclassiÔ¨Åcation pattern, where the FPR is higher for men and the
FNR is higher for women.
We note here the fact that our implementations (SVM, LR, RF, and FFNN) "surround" the Aarhus
University‚Äôs implementation (XGBoost) in terms of classiÔ¨Åcation rate pattern characteristics, which to a
certain degree validates our attempt to choose algorithms for our implementations that cover a wide area
of algorithms and potential classiÔ¨Åcation and bias patterns to ensure the robustness of our experiments
and results.
Overall diÔ¨Äerences in classiÔ¨Åcation rate patterns for gender
When assessing the true classiÔ¨Åcations, the TPR and TNR, the TPR for all Ô¨Åve models are lower for
females than for males. This means that of the citizens who actually fall, the models‚Äô predictions are
more accurate for male citizens than female citizens. The opposite is true in the case of TNR, which is
higher for females than for males. This means that of the citizens who actually do not fall, the models‚Äô
predictions are more accurate for female citizens than male citizens.
When assessing the false classiÔ¨Åcations, the FPR and FNR, we also identify a gender diÔ¨Äerence. Here, the
FPR is higher for males than for females for each model. This means that the models more often suggest
that males should be provided fall prevention training - even though they will not fall. The same is clear
for the FNR, which is higher for females than for males, and shows that the models tend to wrongly
predict that females do not fall - even though they actually will fall. As previously mentioned, a false
negative has more severe consequences than a false positive in this particular case since not providing
fall prevention training to someone who will fall is worse than providing the training to a citizen who
4410 IDENTIFICATION OF BIAS
does not need it. Again, the classiÔ¨Åcation rates reÔ¨Çect a disadvantage for females.
All of the estimates of classiÔ¨Åcation rates for all models have non-overlapping conÔ¨Ådence intervals be-
tween males and females, except for the FFNN model. For the FFNN model, the conÔ¨Ådence intervals
on FPR and TNR do not overlap, but do overlap on FNR and TPR. Since most of the models have
non-overlappingconÔ¨ÅdenceintervalsbetweenthegendersfortheclassiÔ¨Åcationrates, wemoveontoexam-
ine whether the gender-speciÔ¨Åc diÔ¨Äerences imply that the classiÔ¨Åers could be biased in relation to gender.
10.2 Relation between classiÔ¨Åcation rates
To evaluate whether the non-overlapping estimates of the classiÔ¨Åcation rates found above imply biased
classiÔ¨Åers, we show the relation between the classiÔ¨Åcation rates and assess them using the notion of the
80% rule. To calculate the relation of a classiÔ¨Åcation rate, the estimate of the rate for females is divided
by the estimate of the rate for males. This is shown in Ô¨Ågure 22. If a classiÔ¨Åcation rate is equal for both
genders, the point lies on the red line (relation = 1.0). If a classiÔ¨Åcaion rate is higher for males than
females, the point lies under the red line. If a classiÔ¨Åcation rate is higher for females than males, the
point lies above the red line. The grey lines represent the 80% region (0.8 and 1.25). If it is assumed that
the 80% rule applies for all of the rates, a model could be biased if a point lies either above the upper
grey boundary or below the lower grey boundary. This method enables us to compare the gender-speciÔ¨Åc
diÔ¨Äerences across all classiÔ¨Åcation rates.
Figure 22: Relation between females and males in respectively TPR, FPR, TNR and FNR. The Ô¨Årst
axis is categorical. However, to avoid overlapping data points within the rates jitter is added to the plot.
Figure 22 shows what was already established in Ô¨Ågure 21, which is that males have higher TPR and
FPR and that females have higher TNR and FNR. However, Ô¨Ågure 22 reveals insights into the relation
between the classiÔ¨Åcation rates and whether or not this entails a biased classiÔ¨Åer.
ForTPR, four out of Ô¨Åve of the models lie within the 80% region, while the remaining, the LR, lies just
outside.
4510 IDENTIFICATION OF BIAS
ForFPR, four models lie outside of the 80% region, while XGBoost is inside the region although
close to the margin. Regarding FPR, the SVM implementation is the most problematic classiÔ¨Åer.
ForTNR, all relations between men and women lie within the 80% region. The SVM and LR are
closer to the upper boundary than the rest of the models.
ForFNR, the FFNN implementation is within the 80% region, while RF is closer to the margin but still
within. Furthermore, the SVM and the LR implementations are clearly outside of the 80% region, and
the XGBoost is also outside but closer to the boundary.
When looking across all rates, the results lead us to conclude that there could be potential issues with
bias regarding the FPR and FNR when implementing models on the AIR data set. This bias is dis-
advantageous for females in the sense that males who do fall are more likely to be provided with fall
prevention training (FPR) and that females who fall are less likely to be provided with fall prevention
training (FNR). For TPR, the picture is less clear where the implementations are close to the margin of
the 80% region, while it seems that the TNR is not biased.
When looking at the XGBoost model, which is the algorithm that Aalborg Municipality and Aarhus
University intend to implement in the real world, it appears to be one of the less biased classiÔ¨Åers since
the relation of the classiÔ¨Åcation rate estimates are within or close to the acceptable 80% region for all
rates. We have identiÔ¨Åed bias in the AIR classiÔ¨Åcation algorithm (XGBoost) and seen that among four
other algorithms trained on the AIR data the XGBoost method is one of the less biased. Furthermore,
the XGBoost model‚Äôs challenges with bias are limited to FPR and FNR, where it exhibits a gender bias
that is close to the margins of the 80% region.
10.3 Accuracy and predicted probabilities
In order to assess the eÔ¨Äect of a mitigation technique on the accuracy and predicted probabilities of the
models, we show the values for the models trained on the original data set, to enable comparison.
The estimate of accuracy for each model is shown in Ô¨Ågure 23.
Figure 23: Accuracy of the Ô¨Åve models.
In Ô¨Ågure 23 we observe that the accuracies lie between 0.62 and 0.87. RF has the highest accuracy,
followed by XGBoost and FFNN with a slight drop in accuracy. SVM is somewhat lower, while LR is
clearly the lowest. The estimates can be found in table 16 in appendix A.
4610 IDENTIFICATION OF BIAS
Table 10 shows the predicted probabilities for each model conditioned by gender and ground truth
outcome.
Model Females (Fall) Males (Fall) Females (No Fall) Males (No Fall)
SVM 31.0 36.3 18.0 22.2
(30.5-31.6) (35.7-36.9) (17.8-18.3) (21.8-22.5)
LR 50.3 56.8 42.4 47.2
(49.7-50.9) (56.1-57.5) (42.0-42.7) (46.68-47.66)
RF 50.0 55.4 14.3 15.4
(49.0-51.0) (54.2-56.6) (14.0-14.6) (15.0-15.9)
FFNN 34.9 38.6 17.2 19.9
(34.0-35.7) (37.6-39.6) (16.9-17.4) (19.5-20.4)
XGBoost 54.9 59.5 20.7 24.0
(53.8-56.0) (58.2-60.8) (20.2-21.1) (23.3-24.6)
Table 10: Mean and 95% conÔ¨Ådence interval for predicted probabilities conditioned by actual fall and
gender.
As expected, since men fall more on average than women, males generally have higher predicted proba-
bilities than females. We identiÔ¨Åed a potential bias, where the FNR for females is higher than males, and
where the FPR for males is higher than females. Therefore, when assessing the predicted probabilities
in the tested mitigation techniques, we hope to Ô¨Ånd, that the probabilities for females who fall increase
(lower FNR), while the probabilities for males who do not fall decrease (lower FPR). Furthermore, we
are less focused on changing the probabilities for males who fall and for females who do not fall, since
the issues with bias are not directly related to the probabilities of these two groups.
10.4 Sub-conclusion: IdentiÔ¨Åcation of bias in AIR
On the basis of section 10 we have answered research question 1. We have identiÔ¨Åed bias in the AIR
project in the following way:
In section 10.1, we identiÔ¨Åed bias by estimating the gender speciÔ¨Åc diÔ¨Äerences in classiÔ¨Åcation rates
across all Ô¨Åve models. Here, we found non-overlapping conÔ¨Ådence intervals, indicating gender speciÔ¨Åc
diÔ¨Äerences regarding TPR, TNR, FPR, and FNR.
In section 10.2, we identiÔ¨Åed bias by assessing the relation between the classiÔ¨Åcation rates of females
and males and evaluated them in the light of the 80% rule. Here, we found that the relations between
females and males in terms of FPR and FNR are biased. This is particularly the case for FNR, where
females who fall are wrongly classiÔ¨Åed as not falling. This misclassiÔ¨Åcation can have severe consequences.
In section 10.3, we assessed the predicted probabilities of all Ô¨Åve models. Here, we observed, that
males have higher predicted probabilities of falling than females.
4711 MITIGATION OF BIAS
11 Mitigation of bias
In this section we test four bias mitigation techniques. These are:
‚Ä¢Dropping the gender variable
‚Ä¢Gender swap
‚Ä¢Disparate impact removal
‚Ä¢Learning fair representations
We implement each of them, train the Ô¨Åve models on the resulting data set, and assess if the identiÔ¨Åed
bias regarding FPR and FNR has been mitigated.
First, classiÔ¨Åcation rates of the new models are compared with the original rates from Ô¨Ågure 21. Second,
the relations of the new classiÔ¨Åcation rates are compared with the relations of the original rates from
Ô¨Ågure 22. Third, the average predicted probabilities are compared with the originals‚Äô found in table 10.
11.1 Dropping the protected variable
We remove the gender variable from the AIR data set and train the models again.
11.1.1 ClassiÔ¨Åcation rates
Figure 24 shows estimates of the classiÔ¨Åcation rates from models trained on gender swapped AIR data
set. These are compared to the original plots from Ô¨Ågure 21. The new estimates of classiÔ¨Åcation rates
can also be found in appendix, section A, table 20.
4811 MITIGATION OF BIAS
Figure 24: Comparing classiÔ¨Åcation rates between models built on original data (left) and data where
gender is dropped (right).
4911 MITIGATION OF BIAS
SVM
FPR decreases and FNR increases for males. The conÔ¨Ådence intervals for FPR are still not overlapping,
while the FNR conÔ¨Ådence intervals overlap.
LR
Dropping the gender variable increases FPR for females and decreases FPR for males. Furthermore, it
slightly decreases FNR for females and increases FNR for males. However, the conÔ¨Ådence intervals of
the estimates are non-overlapping for both FNR and FPR.
RF
For males, there is a decrease in FPR and an increase in FNR. The conÔ¨Ådence intervals for both FPR
and FNR overlap.
FFNN
Dropping gender decreases FPR and increases FNR for males. The conÔ¨Ådence intervals for both FPR
and FNR overlap.
XGBoost
FPR decreases for males and FPR increases for females, while there is an increase in FNR for males and
a decrease in FNR for females. The conÔ¨Ådence intervals for FPR overlap, while FNR conÔ¨Ådence intervals
do not overlap.
General comments
It seems that for all models, the diÔ¨Äerence between men and women regarding the identiÔ¨Åed FNR and
FPR bias is decreased.
For SVM, RF, and FFNN, the FNR-related biases are mitigated by only increasing FNR for males.
This implies that the performance of the models is worsened. Simply increasing FNR for males is an
undesirable avenue for bias mitigation since, as previously stated, FNR is a costly misclassiÔ¨Åcation. In
the LR and XGBoost, dropping the gender variables lowers the FNR for females and increases the FNR
for males. This is a somewhat more desirable result.
For all of the models, FPR for males is decreased, and FPR for females is mostly unchanged or slightly
increased. This is a desirable mitigation result.
11.1.2 Relation between classiÔ¨Åcation rates
In Ô¨Ågure 25, the left pane shows the relations of metrics of the models built on the original data. The
right pane shows the relations of the metrics for the models built on the data where gender is dropped.
The original models exhibited bias in FPR and FNR. This bias is now mitigated since all metric re-
lations are located within the 80% region. Furthermore, the TPR and TNR are located closer to the red
line (relation=1) than was the case with the models trained on the original.
5011 MITIGATION OF BIAS
Figure 25: Relation of classiÔ¨Åcation rates between males and females. Left: Models built on original
data. Right: Models built on data where gender is dropped.
Thus, when dropping the gender variable and training the Ô¨Åve models on the resulting data set, no bias
related to the 80% rule is identiÔ¨Åed when using our approach for bias identiÔ¨Åcation. This means that
dropping the gender variable successfully mitigated the identiÔ¨Åed gender bias.
11.1.3 Accuracy and predicted probabilities
Figure 26 shows the accuracy of each model compared to the model trained on the original data set.
Figure 26: Accuracy of the Ô¨Åve models when mitigating bias by dropping gender.
From Ô¨Ågure 26 it is clear that dropping gender did not have a substantial impact on the accuracy of the
models. The estimates can be found in table 18 in appendix A
Finally, we assess the averages of the predicted probabilities to see if dropping the gender variable
has had the desired eÔ¨Äect here also. Table 11 shows the changes in probabilities grouped by gender and
actual outcome, compared with the original probabilities. As stated previously, we focus on increasing
the probabilities for the group "Females (Fall)" and decreasing the probabilities for the group "Males
(No Fall)".
5111 MITIGATION OF BIAS
Model Females (Fall) Males (Fall) Females (No Fall) Males (No Fall)
SVM +0.6 -3.6 +1.4 -2.1
LR +1.7 -2.7 +1.6 -2.8
RF +0.2 -2.1 +0.7 -0.9
FFNN +0.6 -2.2 +0.8 -0.9
XGBoost +0.9 -1.5 +2.4 -0.1
Table 11: Mean change of predicted probabilities conditioned by actual fall and gender. From models
trained on data where gender is dropped compared with original models.
We Ô¨Ånd a clear eÔ¨Äect, where the probabilities of females are increased and the probabilities of males are
decreased. This is a positive result. However, it seems that the probabilities of females who do not fall
are increased more than probabilities of females who do fall. Similarly, the probabilities of males who
fall, are decreased more than the probabilities of males who do not fall. This is unwanted, since the eÔ¨Äect
of the mitigation technique is skewed in an undesired direction, where we impact the "wrong" groups to
a higher degree, in this case "Males (Fall)" and "Females (No Fall)".
11.2 Gender swapping
We concatenate the original data set with a gender swapped version of the data and train each model
again. The resulting models are then tested on a test set from the original data.
11.2.1 ClassiÔ¨Åcation rates
Figure 27 shows estimates of the classiÔ¨Åcation rates from models trained on gender swapped AIR data
set. These are compared to the original plots from Ô¨Ågure 21. The new estimates of classiÔ¨Åcation rates
can also be found in appendix, section A, table 20.
5211 MITIGATION OF BIAS
Figure 27: Comparing classiÔ¨Åcation rates between models built on original data (left) and gender
swapped data (right).
5311 MITIGATION OF BIAS
SVM
When training on a gender swapped data set, FPR decreases and FNR increases for males, while FNR
slightly decreases for females. The conÔ¨Ådence intervals for FNR do not overlap, while they overlap for
FNR.
LR
FPR decreases for males and slightly increases for females. FNR decreases for females and increases for
males. The conÔ¨Ådence intervals overlap for FPR and do not overlap for FNR.
RF
For FPR, there is a slight increase for females and a slight decrease for males. FNR decreases for females,
while it increases for males. The conÔ¨Ådence intervals for both FPR and FNR overlap.
FFNN
There is no notable change in the levels of the classiÔ¨Åcation rates after implementing gender swap. The
conÔ¨Ådence intervals overlap for FPR and FNR.
XGBoost
When mitigating bias using gender swap, FPR increases for females and decreases for males. FNR de-
creases for females and increases for males. The conÔ¨Ådence intervals for FPR overlap and do not overlap
for FNR.
General comments
It seems that for most of the models, the diÔ¨Äerence between men and women regarding the identiÔ¨Åed
FNR and FPR bias is decreased. This is primarily done by decreasing FPR and increasing FNR for
males and increasing FPR and decreasing FNR for females. In other words, more males are predicted as
not falling, and more females are predicted as falling, compared to the original models. When comparing
the eÔ¨Äect of bias mitigation through gender swapping with dropping gender, we identify an important
diÔ¨Äerence regarding FNR. For gender swapping, the FNR values for females and males move towards
each other instead of simply increasing the FNR values for males only. Gender swapping is a better way
to mitigate bias since the overall number of false-negative is lower for gender swap than dropping gender.
This is a desirable result since false negatives are costly for elderly citizens.
The conÔ¨Ådence intervals for FPR and FNR overlap for eight out of ten model-speciÔ¨Åc classiÔ¨Åcation
rates (FPR and FNR for each model).
11.2.2 Relation between classiÔ¨Åcation rates
In Ô¨Ågure 28, the left pane shows the relations of metrics of the models built on the original data. The
right pane shows the relations of the metrics for the models built on the gender swapped data.
The original models exhibited bias in FPR and FNR. This bias is now mitigated since all metric re-
lations are located within the 80% region. Furthermore, the TPR and TNR are located closer to the red
line (relation=1) than was the case with the models trained on the original.
5411 MITIGATION OF BIAS
Figure 28: Relation of classiÔ¨Åcation rates between males and females. Left: Models built on original
data. Right: Models built on data where gender swap is implemented.
Thus, when mitigating bias using gender swapping, no bias related to the 80% rule is identiÔ¨Åed. This
means that gender swapping successfully mitigated the identiÔ¨Åed gender bias.
11.2.3 Accuracy and predicted probabilities
Figure 29 shows the accuracy of each model trained on the gender swapped data set, compared with the
original models.
Figure 29: Accuracy of the Ô¨Åve models when mitigating bias using gender swapping.
From Ô¨Ågure 29 it can be seen that gender swapping does not have any noteworthy eÔ¨Äect on the accuracy
of the models. The estimates can be found in table 21 in appendix A.
Finally, we assess the averages of the predicted probabilities to see if gender swap has had the desired
eÔ¨Äect here also. Table 12 shows the changes in probabilities grouped by gender and actual outcome,
compared with the original probabilities.
5511 MITIGATION OF BIAS
Model Females (Fall) Males (Fall) Females (No Fall) Males (No Fall)
SVM +7.4 +3.4 -1.1 -4.2
LR +1.8 -2.4 +1.2 -3.3
RF +5.1 +2.0 -0.6 -2.3
FFNN +0.2 -1.9 +0.6 -1.3
XGBoost +1.0 -1.6 +0.7 -2.2
Table 12: Mean change of predicted probabilities conditioned by actual fall and gender. From models
trained on gender swapped data compared with original models.
When assessing the predicted probabilities, we Ô¨Ånd that the eÔ¨Äects are as desired. The females who
fall experience the largest increase, while the males who do not fall experience the largest decrease in
predicted probabilities. Furthermore, the impact on the groups "Males (Fall)" and "Females (No Fall)"
is unclear, where the direction of changes in probabilities depends on the model used for prediction.
Overall, the changes in predicted probabilities are desired.
11.3 Disparate impact removal
We perform disparate impact removal (DI removal) on the numerical features in the training data set,
which are age,loan period , andnumber of aids . The models are trained on the training data set and
tested on data set where disparate impact removal has also been performed.
11.3.1 ClassiÔ¨Åcation rates
Figure 30 shows estimates of the classiÔ¨Åcation rates from models trained on disparate impact removed
AIR data set. These are compared to the original plots from Ô¨Ågure 21. The new estimates of classiÔ¨Åcation
rates can also be found in appendix, section A, table 23.
5611 MITIGATION OF BIAS
Figure 30: Comparing classiÔ¨Åcation rates between models built on original data (left) and data that
has been DI removed (right).
5711 MITIGATION OF BIAS
SVM
FPR decreases and FNR increases for males. The conÔ¨Ådence intervals for FPR do not overlap, while the
FNR conÔ¨Ådence intervals overlap.
LR
DI removal increases FPR for females and decreases FPR for males. Furthermore, it slightly decreases
FNR for females and increases FNR for males. FPR conÔ¨Ådence intervals overlap. However, the conÔ¨Å-
dence intervals of the estimates are non-overlapping for FNR.
RF
For males, there is a decrease in FPR and an increase in FNR. For females the FNR increases. The
conÔ¨Ådence intervals for FPR overlap, but FNR does not overlap.
FFNN
DI removal decreases FPR and increases FNR for males. The conÔ¨Ådence intervals for FPR do not over-
lap, while they do overlap for FNR.
XGBoost
FPR increases for females, while there is an increase in FNR for males and for females. The conÔ¨Ådence
intervals for FPR and do not overlap for FNR.
General comments
It seems that for all models, the diÔ¨Äerence between men and women regarding the identiÔ¨Åed FNR and
FPR bias is decreased. However, for the RF and XGBoost, FNR increases for both females and males,
which is not a desired result. Five out of ten conÔ¨Ådence intervals for FPR and FNR do not overlap.
11.3.2 Relation between classiÔ¨Åcation rates
In Ô¨Ågure 31, the left pane shows the relations of metrics of the models built on the original data. The
right pane shows the relations of the metrics for the models built on the disparate impact removed data.
The original models exhibited bias in FPR and FNR. This bias has been mitigated since all metric
relations are located inside the 80% region. Furthermore, the TPR and TNR are located closer to the
redline (relation=1) than was the case with the models trained on the original.
Figure 31: Relation of classiÔ¨Åcation rates between males and females. Left: Models built on original
data. Right: Models built on data where DI removal is implemented.
5811 MITIGATION OF BIAS
Thus, when creating a DI-removed version of the data set and training the Ô¨Åve models on the resulting
data set, the bias is mitigated successfully.
11.3.3 Accuracy and predicted probabilities
Figure 32 shows the accuracy of each classiÔ¨Åcation algorithm, compared with the original.
Figure 32: Accuracy of the Ô¨Åve models when mitigating bias with DI removal.
From Ô¨Ågure 32 it can bee seen that training on a DI-removed data set, did not have any noteworthy
impact on the accuracy of any model.
Finally, we assess the averages of the predicted probabilities to see if disparate impact removal has had
the desired eÔ¨Äect here also. Table 13 shows the changes in probabilities grouped by gender and actual
outcome, compared with the original probabilities.
Model Females (Fall) Males (Fall) Females (No Fall) Males (No Fall)
SVM +0.5 -3.3 +1.4 -2.0
LR +2.0 -2.7 +2.0 -3.2
RF -5.0 -5.7 +1.5 -0.2
FFNN +0.5 -2.9 +1.4 -0.9
XGBoost -4.3 -5.3 +3.2 0
Table 13: Mean change in predicted probabilities conditioned by actual fall and gender. From models
trained on DI removal data compared with original models.
Whenassessingthetable, weÔ¨ÅndtheclearesteÔ¨Äectforthegroups"Males(Fall)"(decrease)and"Females
(NoFall)"(increase). Thisisnotadesiredresult. For"Females(Fall)", theresultsaresomewhatunclear,
where three models indicate a slight increase in probabilities, while the remaining two indicate a notable
decrease. For "Males (No Fall)", the picture is clearer, where four of Ô¨Åve models indicate a decrease (as
desired). However, the decrease is less substantial than for "Males (Fall)", which is not a desired result.
5911 MITIGATION OF BIAS
11.4 Learning fair representations
We Ô¨Ånd a fair representation of the numerical features of the data set using the training data. The same
transformation is afterward applied to the test set. Each model is trained on the LFR training data and
tested on the LFR test data.
11.4.1 ClassiÔ¨Åcation rates
Figure 33 shows estimates of the classiÔ¨Åcation rates from models trained on LFR data set. These are
compared to the original plots from Ô¨Ågure 21. The new estimates of classiÔ¨Åcation rates can be found in
appendix, section A, table 26.
6011 MITIGATION OF BIAS
Figure 33: Comparing classiÔ¨Åcation rates between models built on original data (left) and the LFR
data (right).
6111 MITIGATION OF BIAS
SVM
When training on an LFR data set, FPR decreases and FNR increases for males, while FNR slightly
decreases for females. The conÔ¨Ådence intervals for FNR overlap, while they do not overlap for FPR.
LR
FPR decreases for males and increases for females. FNR decreases for females and increases for males.
The conÔ¨Ådence intervals overlap for both FPR and FNR.
RF
FNR increases for females and males. The conÔ¨Ådence intervals for FPR overlap, but not for FNR.
FFNN
FPR decreases for females and males. FNR increases for both groups. The conÔ¨Ådence intervals overlap
for FPR and FNR.
XGBoost
When mitigating bias using LFR, FPR increases for females and decreases for males. FNR increases for
females and males. The conÔ¨Ådence intervals for FPR overlap and do not overlap for FNR.
General comments
For most of the models, the diÔ¨Äerence between men and women regarding the identiÔ¨Åed FNR and FPR
bias is decreased. This is primarily done by decreasing FPR and increasing FNR for males. The changes
are less consistent for females, where we identify mostly unchanged FPR and examples of both an in-
crease and a decrease in FNR. However, as the only model, LR exhibits the desired results, where the
FPR and FNR of males and females move towards each other. Using LFR seems to decrease the FNR
diÔ¨Äerence between genders, but does so by increasing the FNR for both genders. This is an undesirable
mitigation result since FNR in the setting of the AIR case is the most costly misclassiÔ¨Åcation.
11.4.2 Relation between classiÔ¨Åcation rates
In Ô¨Ågure 34, the left pane shows the relations of metrics of the models built on the original data. The
right pane shows the relations of the metrics for the models built on the LFR data.
Figure 34: Relation of classiÔ¨Åcation rates between males and females. Left: Models built on original
data. Right: Models built on LFR data.
The original models exhibited bias in FPR and FNR. This bias is now mitigated since all metric relations
are located within the 80% region - all but the FPR for RF. However, the conÔ¨Ådence intervals for the
6211 MITIGATION OF BIAS
RF‚Äôs FPR did overlap. Furthermore, the TPR and TNR are located closer to the red line (relation=1)
than was the case with the models trained on the original.
Thus, when mitigating bias using LFR, no bias related to the 80% rule is identiÔ¨Åed. This means that
LFR successfully mitigated the identiÔ¨Åed gender bias. However, we identify an undesired increase in
FPR and FNR.
11.4.3 Accuracy and predicted probabilities
Figure 35 shows the accuracy of each model trained on the LFR data set, compared with the original
models.
Figure 35: Accuracy of the Ô¨Åve models when mitigating bias with LFR.
From Ô¨Ågure 35 it can be seen that there is no noteworthy aÔ¨Äect on the accuracy of the models, when
LFR is used to mitigate bias. The estimates can be found in table 27 in appendix A.
Finally, we assess the averages of the predicted probabilities to see if learning fair representations has
had the desired eÔ¨Äect here also. Table 14 shows the changes in probabilities grouped by gender and
actual outcome, compared with the original probabilities.
Model Females (Fall) Males (Fall) Females (No Fall) Males (No Fall)
SVM +0.2 -4.5 +1.6 -1.8
LR +2.1 -3.9 +2.2 -2.8
RF -2.8 -2.8 +0.7 +0.5
FFNN -2.1 -4.7 +2.1 +0.3
XGBoost -2.8 -2.9 +1.9 +0.5
Table 14: Mean change in predicted probabilities conditioned by actual fall and gender. From models
trained on LFR data compared with original models.
When assessing the table, the conclusions regarding the eÔ¨Äect on predicted probabilities are similar to
disparate impact (see table 13). The clearest eÔ¨Äect is on the probabilities for "Males (Fall)" (decreases)
and "Females (No Fall)" (increases). Furthermore, it seems that the models - despite not agreeing -
indicate a decrease in probabilities for "Females (Fall)", while the picture is less clear for "Males (No
Fall)". Overall, the impact on the predicted probabilities is not as desired.
6311 MITIGATION OF BIAS
11.5 Changes in overall classiÔ¨Åcations after mitigating bias
Whencommentingoneach mitigationtechnique, we assessedchanges forthegender-speciÔ¨ÅcclassiÔ¨Åcation
rates. However, to arrive at a conclusion on the eÔ¨Äect of the mitigation techniques, we must also look
at the eÔ¨Äect on overall classiÔ¨Åcations. If we do not do so, we run the risk of missing important changes
in classiÔ¨Åcations, for example, an overall increase in false negatives, which cannot be read only from the
gender speciÔ¨Åc FNR. In this section, we assess what eÔ¨Äects the diÔ¨Äerent bias mitigation techniques have
had on the classiÔ¨Åcations of each model. To this end, we show the values from a normalized confusion
matrix in the appendix A, table 30. The values of each category (TP, FP, TN, and FN) are normalized,
so they sum to 100. As previously mentioned, more false negatives are an undesirable eÔ¨Äect of bias mit-
igation, while more false positives are less problematic. We brieÔ¨Çy consider the changes of each model
and mitigation technique compared to the original confusion matrix to conclude on the overall eÔ¨Äects
that mitigation of bias has had on the classiÔ¨Åcations, with a primary focus on false negatives.
Dropping Gender
We Ô¨Ånd fewer true positives and more false negatives. This is an undesired result. The eÔ¨Äect on false
positives and true negatives are ambiguous.
Gender Swap
We see more true positives and fewer false negatives, which is desired. However, we also identify more
false positives and fewer true negatives.
Disparate Impact Removal
We Ô¨Ånd fewer true positives and more false negatives. This is undesired.
Learning Fair Representations
We observe fewer false positives and more true negatives, which is desired. However, we see fewer true
positives and more false negatives, which is not desired.
EÔ¨Äect on false negatives
Increasing the number of false negatives is an undesired eÔ¨Äect of bias mitigation. When assessing the
magnitude of the changes in percentage points, we see that the changes in false-negative classiÔ¨Åcations
are highest for LFR, followed by DI-removal. Furthermore, the percentage point changes are smallest for
dropping gender, while the changes are acceptable for gender swap, in that the number of false negatives
decreases.
In this light, LFR achieves unbiased classiÔ¨Åcations in the most problematic way. We Ô¨Ånd a change
in percentage points of 1.1% when comparing the false negative classiÔ¨Åcations of XGBoost trained on
LFR data to XGBoost trained on original data (see table 29). To put the eÔ¨Äect into perspective, this
would mean an increase in false negative classiÔ¨Åcations of approximately 29 citizens if the LFR-version
of XGBoost were to classify all 2600 citizens currently under the care of the Referral Unit of Aalborg
Municipality. We have chosen to comment on the results for XGBoost since Aalborg Municipality is
expected to use this algorithm.
11.6 Sub-conclusion: Mitigation of bias in AIR
Based on section 11, we have answered research question 2. We have tested techniques for mitigation of
bias in the AIR project in the following way:
Dropping gender
In section 11.1, we attempted to mitigate bias by dropping the protected variable, gender, from the data
set. From the results, we conclude that the technique successfully mitigated the identiÔ¨Åed bias. Further-
more, mitigation entailed undesired but small changes in the classiÔ¨Åcations and predicted probabilities.
Gender swapping
In section 11.2, we attempted to mitigate bias by creating a gender swapped version of the data set. From
the results, we conclude that the technique successfully mitigated the identiÔ¨Åed bias. Furthermore, the
changesinclassiÔ¨Åcationswerenotproblematic, andthechangesinpredictedprobabilitieswereasdesired.
6411 MITIGATION OF BIAS
Disparate impact removal
In section 11.3, we attempted to mitigate bias by running the disparate impact removal algorithm on the
numerical features of the AIR data set. From the results, we conclude that the technique successfully
mitigated the identiÔ¨Åed bias. However, the changes in classiÔ¨Åcations and predicted probabilities were
somewhat problematic.
Learning fair representations
In section 11.4, we attempted to mitigate bias by learning a fair representation of the data set. From the
results, we conclude that the technique successfully mitigated the identiÔ¨Åed bias. However, the changes
in classiÔ¨Åcations and predicted probabilities were somewhat problematic.
6512 DISCUSSION
12 Discussion
In this section, we critically reÔ¨Çect on speciÔ¨Åc elements of our thesis that could have been done diÔ¨Äerently
and discuss alternative routes of action and what impact they would have had on the conclusions of the
thesis. We structure the discussion into three sections. In the Ô¨Årst section we discuss challenges regarding
the mitigation techniques, in the second section we criticize technical aspects and our modelling choices,
and in the third section we reÔ¨Çect on our theoretical and methodological approach.
12.1 Challenges regarding the mitigation techniques
12.1.1 Dropping gender
The approach of dropping gender can be problematic since other features can be correlated with the
protected variable. These features can act as proxiesfor gender, which would enable the model to be
biased even though the variable has been dropped [15][19]. The problem of proxies is related to what
Calmon et al. call indirect discrimination . We do not explore the data set to see if any of the variables
could act as potential proxies for gender. For example, one could imagine that some speciÔ¨Åc aids are
only given to either women or men.
12.1.2 Gender Swap
Gender swap might only work when the subgroups of interest do not come from very dissimilar distri-
butions. One could imagine, that the performance of a classiÔ¨Åer could be negatively impacted, if it were
trained on a data set with "swapped" observations, that are very far from the true distribution of the
subgroup. This could worsen the performance of the classiÔ¨Åer, since the model could extrapolate in a way
that might not be correct. This could particularly be the case, if the mapping from an observed variable
to an outcome variable runs through an unobserved characteristic that is unevenly distributed between
the subgroups. As an example, consider the modelling of hours in the gym (x) onabsolute change in
muscle mass (y), where the mapping between x and y might go through the unobserved level of testos-
terone(z), which would be unevenly distributed between females and males. Using gender swap in this
example, could possibly lead to underestimating the change in muscle mass for males and overestimating
the change in muscle mass for females.
12.1.3 Disparate impact removal
In the AIF360 library, we have noticed that the algorithm used in the disparate impact implementation
does not Ô¨Ånd a median distribution as described in the literature review (section 5). Instead, it moves the
values of the group with the numerically highest distribution towards the distribution of the group with
the lowest values. For example, since females are older than males in the AIR data set, the distribution
of age for females is moved to resemble the distribution of age for males. We have not attempted to
implement a version where the median distribution is found. Removing disparate impact using the
median distribution could perhaps have changed the results of the thesis.
12.1.4 Learning fair representations
In the AIF360 implementation of learning fair representations (LFR), several hyperparameters can be
tuned - such as the number of prototype vectors representing the original data. In section 11, results
showed that the mitigation technique leads to problematic changes in the classiÔ¨Åcations and predicted
probabilities. The results could potentially have been diÔ¨Äerent if the hyperparameters were changed. For
example, changing the fairness constraint term weight, Az, in equation (19), could have enforced a more
dissimilar mapping of individuals in/not in the protected group. Furthermore, the mitigation technique
is based on several mathematical data transformations. Since the AIR model is intended to be used as a
decision support tool in the public sector, it could be challenging to explain LFR in a non-technical way.
12.2 Technical aspects and modelling choices
12.2.1 Choice of 95% as the level for conÔ¨Ådence intervals
When identifying bias between genders, we estimate the gender-speciÔ¨Åc classiÔ¨Åcation rates of a given
model and calculate the 95% conÔ¨Ådence intervals. If the conÔ¨Ådence intervals overlap, we deem it improb-
6612 DISCUSSION
able that the estimates are diÔ¨Äerent from one another. This comparison of 95% conÔ¨Ådence intervals can
be seen as a hypothesis test using an 0.05 alpha level. Interpreted as such, our method would test the
null hypothesis that an estimate of a given classiÔ¨Åcation rate is the same for females and males. If the
95% conÔ¨Ådence intervals do not overlap, we reject the null hypothesis and state that we are 95% certain
that the actual classiÔ¨Åcation rate for females and males are diÔ¨Äerent from each other.
We compare the conÔ¨Ådence intervals of men and women across 4 classiÔ¨Åcation rates x 5 models x 5
states (4 mitigation techniques and 1 original), which gives 100 comparisons or, if interpreted as above,
100 hypothesis tests. This is quite a large number. When performing multiple hypothesis tests, the risk
of false discovery is substantial [40, p. 686]. In other words, rejecting the null hypothesis would be to
conclude that the true level of a classiÔ¨Åcation rate is diÔ¨Äerent between genders even though they are the
same. We calculate the family-wise error rate (FWER) by: FWER1 (1 )M, whereMis the
number of hypotheses and is the alpha-level [40, p. 686]. Using M= 100and= 0:05, one can see
that we have a very high probability of at least one type 1 error:
FWER1 (1 0:05)100= 0:994 (39)
We deliberately do not comment on whether or not the estimates are signiÔ¨Åcantly diÔ¨Äerent from one
another. However, our method is still at risk of falsely concluding that there could be issues with bias
from the fact that the conÔ¨Ådence intervals do not overlap in the same way as would have been the case
if we did multiple hypothesis tests with 0.05 alpha levels. This being said, we do Ô¨Ånd consistent results
across all Ô¨Åve models (SVM, LR, RF, FFNN, and XGBoost), where we Ô¨Ånd conÔ¨Ådence intervals which do
not overlap for the classiÔ¨Åcation rates estimated on the original data, while all four mitigation techniques
result in mostly overlapping conÔ¨Ådence intervals for the relevant classiÔ¨Åcation rates. Furthermore, many
of our comparisons of classiÔ¨Åcation rates are correlated, in that we assess the diÔ¨Äerence between females
and males on the same classiÔ¨Åcation rates generated by diÔ¨Äerent models on augmented data sets. Finally,
the problem is limited to type 1 errors, which would be to falsely conclude that the classiÔ¨Åcation rates are
diÔ¨Äerent, which could lead to a false discovery of bias. However, most of the 100 comparisons of classiÔ¨Åca-
tion rates are made on results after mitigating, where we typically conclude that bias cannot be identiÔ¨Åed.
We could have used a more strict conÔ¨Ådence interval (for example, 99.9% conÔ¨Ådence intervals), which
would have reduced the probability of type 1 errors. This strategy would have the same eÔ¨Äect as, for
example, Bonferroni correction. However, this would increase the probability of type 2 errors, in our
case, incorrectly concluding that bias cannot be identiÔ¨Åed. This is perhaps a more undesired type of
error for our thesis, since it could leads us to falsely conclude that: 1) bias cannot be identiÔ¨Åed in the
original models and 2) a mitigation technique eÔ¨Äectively removed bias.
12.2.2 Robustness of the metric relations
For evaluating the diÔ¨Äerence in the classiÔ¨Åcation rates, the relation between females‚Äô and males‚Äô metrics
are compared. The metrics for the models built on the original data are plotted in section 10.2, table
22. The metrics are based on binary predictions, where citizens are predicted to be either falling or not.
The relations of the metrics could potentially diÔ¨Äer if the thresholds of the binary classiÔ¨Åer are changed,
which might lead to diÔ¨Äerent conclusion regarding the eÔ¨Äect of the mitigation techniques. Therefore, we
assess the robustness of the relations by testing out diÔ¨Äerent thresholds for the binary classiÔ¨Åcations of
the original models. Plots can be found in appendix, section B.
As a robustness check, we have evaluated the relations in the original models when using a classiÔ¨Å-
cation threshold between 0.4 and 0.6 to see how it could change the conclusion of the identiÔ¨Åed gender
bias. If the threshold is above 0.5, the TPR relation of the LR moves outside the 80%, in favor of males.
Furthermore, if the threshold is above 0.5, the FPR relation of the LR and XGBoost moves below the
lower bound, meaning that the classiÔ¨Åer is advantageous for males. Finally, if the thresholds move toward
0.4, the FNR relation for XGBoost moves above the upper bound of the 80% region. This means that
the relation becomes disadvantageous for females. The robustness check gives the indication that there
is some instability for the above mentioned relations. However, when assessing the relation for the FPR
and FNR with respect to the binary classiÔ¨Åcation threshold, the relation between the genders are still
biased as concluded in the bias mitigation section.
6712 DISCUSSION
12.2.3 Model performance
For assessing and comparing model performance when testing mitigation techniques, we use the accuracy
of the models as e.g. shown in table 23. As shown in table 4, around 78% of the citizens in the data
set have not fallen. A naive classiÔ¨Åer that only predicts the dominant "No fall" class, would reach an
accuracy of 78%. In this unbalanced setting, it can be diÔ¨Écult to interpret the actual performance of
the model from the accuracy, since the high performance on the actual negative class might overshadow
problems regarding lower performance the actual positive class. Instead of using accuracy, we could
have used the receiver operating characteristic curves (ROC curves) to assess model performance. Since
ROC uses the TPR and FPR, we would have avoided the issues mentioned above. In appendix C we
have provided ROC curves including the area under curve (AUC). When assessing the changes in AUC
for the diÔ¨Äerent mitigation strategies compared to the original models, we Ô¨Ånd moderate drops in AUC
of at most 3%, which is very much in line with the changes found in accuracy. Because of this, our
conclusions in the analysis would not have been impacted by using AUC instead of accuracy. However,
a disadvantage of AUC is that false negatives are not a part of the metric, and as mentioned frequently,
this is the most costly misclassiÔ¨Åcation in the AIR context.
12.2.4 Choosing features for mitigation - feature importance
Disparate impact removal and learning fair representations are only applied to the numerical features
of the data set. This is somewhat problematic since the numerical features might not aÔ¨Äect the model
output in a way that could enforce gender bias, and since only 3 out of 136 variables are numerical, while
the remaining are one-hot-encoded. The initial descriptive analysis in section 9 only gives an overall
understanding of the numerical features‚Äô distributions, but the thesis does not examine how the features
in each of the Ô¨Åve models aÔ¨Äect whether a citizen is classiÔ¨Åed as "Fall" or "Not Fall". However, if the
numerical variables impact the classiÔ¨Åcations of the models to a high degree, it can be argued that only
changing the values of 3 out of 136 variables could still have an impact on the classiÔ¨Åcations of a model.
To assess the feature importance of the original models, we have produced plots of the Ô¨Åve models‚Äô SHAP
values in the appendix, section D. SHAP is used for explaining how much each feature contributes to the
prediction [56]. In table 47, the top 15 features with the highest absolute mean SHAP value are plotted.
For four of the models, the numerical features Number of aids ,Loan period andAgeare among the top
Ô¨Åve features with highest mean SHAP values. The features‚Äô contributions to the output can both be
positive or negative, which is visualized in Ô¨Ågure 48 and 49. Overall, it can be seen that the models do
not show a clear picture of how the numerical features contribute to the output in terms of positive and
negative contributions. Based on the SHAP values, the numerical features chosen for bias mitigation are
among the features which contribute the most to the output when assessing the absolute SHAP. This
may be an argument as to why the numerical values make sense to use for bias mitigation in disparate
impact removal and learning fair representations, even though we only transform 3 out of 136 features.
We could have assessed the feature importance before implementing the disparate impact and learning
fair representations techniques, but we considered it to be out of the scope of the analysis.
12.2.5 SVM probabilities
For identifying bias in the AIR models, we assessed the predicted probabilities of the models. The SVM
is diÔ¨Äerent than the rest of the models, in that it does not use probabilities when classifying. The
SVM Ô¨Ånds an optimal separating hyperplane for classifying if a citizen falls or not. Therefore, binary
classiÔ¨Åcation is not based on a probability threshold. The probability distribution of the SVM output
lies approximately between 0.0 and 0.7. One should take this diÔ¨Äerence between the SVM and the rest of
the models into account when comparing the probability outputs of the SVM to the rest of the models.
Since the probabilities from an SVM model are somewhat unreliable, we would not recommend that the
AIR project team use the SVM probabilities to calculate a risk score.
12.2.6 Tuning of hyperparameters
DuringthedevelopmentoftheÔ¨Åvemodels, severalhyperparametersarechosenforthemodels. According
to Hellstr√∂m et al. [15], bias can occur when hyperparameters are manually set. A critique of the model
development in this thesis is that we have not tried diÔ¨Äerent parameter settings for the models and
not tuned using cross validation, for example, to see which parameters most eÔ¨Äectively mitigate bias.
This could have been explored. However, we did test a few alternative hyperparameter settings for
6812 DISCUSSION
some of the models during development but did not Ô¨Ånd any substantial changes in classiÔ¨Åcation rates.
Finally, we chose to focus on testing mitigation strategies rather than changing the hyperparameters of
the algorithms. Furthermore, since the data set is unbalanced with respect to the output variable (Fall),
we could have used stratiÔ¨Åed cross validation to test if it could improve the models.
12.3 Theoretical and methodological approach
12.3.1 Unobserved sources of bias
This section brieÔ¨Çy explores some unobserved sources of bias that we do not take into account when
identifying and mitigating bias and how these might introduce bias into the AIR project context. To this
end, we use two theoretical concepts from Suresh & Guttag [57]: measurement bias andrepresentation
bias.
Measurement bias and representation bias
The concept of measurement bias considers group-dependent diÔ¨Äerences in choosing, collecting, and com-
puting the variables that are to be used in the machine learning models [57]. Suresh & Guttag argue
that there can be group-dependent diÔ¨Äerences in the quality of data, which can lead to biased outcomes.
For example, one could image group dependent diÔ¨Äerences between females and males when it comes to
self-reporting fall incidents. Suresh & Guttag state that "women are more likely to be misdiagnosed or
not diagnosed for conditions where self-reported pain is a symptom" [57], in the sense that "professionals
hold stereotypic views of women as emotionally labile and more apt to exaggerate complaints of pain than
men"[58]. Suppose a similar dynamic is present in the AIR case, where physiotherapists and caseworkers
hold this stereotypical view of women. In that case, it might result in group-dependent noise, where the
fall incidents of women are structurally under-registered. Our thesis does not attempt to gather data
regarding this potential bias, nor take it into account during our analysis.
The concept of representation bias covers the fact that the sampling method might only reach a portion
of the population [57]. For example, one could imagine that a higher proportion of the most resourceful
citizens in Aalborg Municipality might use private alternatives to public health care, while the least re-
sourceful to a lower degree contact the municipality in due time to receive the aids they need when they
need them. If this is the case, then both the fall prediction model and the bias mitigation techniques will
potentially perform worse for the most resourceful and the least resourceful citizens in the population
since they are under-represented in the data set.
By not exploring these two avenues of bias, we could risk recommending a course of action for bias
mitigation that does not solve the issue. For example, if fall incidents for females are under-registered,
a more sensible course of action would be to put eÔ¨Äort into rectifying this diÔ¨Äerence between men and
women rather than attempting to mitigate a bias that is identiÔ¨Åed on incorrect data.
Creating new bias?
Finally, by changing the predictions of the algorithm with a bias mitigation technique, hereby possibly
creating more false negatives for males, we insert a new source of bias to the situation. Maybe the men,
who are now incorrectly classiÔ¨Åed, have some characteristic that we now are creating new bias against.
This could have been explored.
12.3.2 Social context of the algorithm
Our approach to bias identiÔ¨Åcation and mitigation is exclusively data-oriented. However, we could have
supported this approach by gathering qualitative empirical evidence, which would have given us an
understanding of the social context of the AIR algorithm. By not doing so we step into two "traps"
highlighted by Selbst et al., namely the formalism trap and theripple eÔ¨Äect trap [59].
Theformalism trap states that machine learning projects run the risk of failing to account for the entire
meaning of fairness by reducing it to mathematical formulations [59]. By reducing the identiÔ¨Åcation of
bias to non-overlapping conÔ¨Ådence intervals and relations outside the 80% region for classiÔ¨Åcation rates,
we might over-simplify bias in the AIR context. One could easily imagine that the 80% region is either
too relaxed or too strict when evaluating bias in the AIR context. Furthermore, it could be the case that
our focus on the overall classiÔ¨Åcation rates is a too broad notion of bias or that we should have chosen
6912 DISCUSSION
a diÔ¨Äerent way to identify bias altogether. A more detailed investigation of the view on bias could have
been performed by interviewing stakeholders of the AIR project. This could potentially give a more valid
foundation for deÔ¨Åning bias relation thresholds.
Theripple eÔ¨Äect trap states that some machine learning projects fail to understand how the imple-
mentation of technology might impact a pre-existing system, hereby overlooking possible unintended
consequences. By not gathering and analysing qualitative data, we fail to fully understand the social
and organizational context of the AIR algorithm. Therefore, we run the risk of recommending bias
mitigation techniques that are unÔ¨Åt for implementation in the AIR project because of an unintended
consequence we did not consider.
12.3.3 Should we even be doing this?
We end the discussion on an ethical note, considering a question that Rachel Thomas from San Fran-
cisco University recommends practitioners to ask themselves at the beginning of an AI project, which
is:"should we even be doing this?" [60]. She presents two examples where her question is particularly
relevant: an algorithm that uses facial recognition to classify faces as Chinese Uyghur, Tibetan, and Ko-
rean and another facial recognition algorithm that classiÔ¨Åes faces as homosexual or heterosexual. In the
wrong hands, it is easy to see how these algorithms could be used to oppress certain vulnerable groups.
Clearly, the AIR project is very far from being problematic in the same sense as the two examples.
However, it is still valuable to consider if one should use machine learning algorithms to predict the fall
probability for citizens in Aalborg Municipality. When the algorithm is implemented, it is anticipated
that it will be used to support caseworkers in Ô¨Ånding additional citizens that would beneÔ¨Åt from fall
prevention training, hereby providing more welfare to the citizens of Aalborg. However, it cannot be
ruled out that the algorithm could be used for diÔ¨Äerent purposes. If the algorithm were to be used in a
scenario with more limited resources and therefore should support caseworkers in deciding who does and
who does not have a high enough probability of falling to be allocated training, the issues with bias in the
algorithm would be more severe and one could argue that using an algorithmic approach is inappropriate.
However, simply refraining from training an algorithm or creating bias mitigation techniques will not
solve the issues with bias. Humans make biased decisions every day, even without the assistance of
algorithms [57] [15]. These biases are, for the most part, implicit and diÔ¨Écult to measure. In contrast,
when algorithms are used for decision support, one can attempt to formalize and make explicit the biases
of the models and attempt to mitigate them. If the goal is to move towards less biased decisions, one
could argue that even insuÔ¨Écient attempts to make bias explicit and mitigate it is a better option than
only relying on human judgment.
7013 CONCLUSION
13 Conclusion
The purpose of the thesis was to identify and mitigate bias in machine learning models built on the
data set used in the AIR project. We present the results from the analysis, and use them to provide the
AIR project team with actionable recommendations regarding bias identiÔ¨Åcation and mitigation. The
research questions are:
‚Ä¢RQ1: How can bias be identiÔ¨Åed in the AIR project?
‚Ä¢RQ2: How can bias be mitigated in the AIR project?
13.1 Results
Research question 1 is answered using a two-step strategy. First, we observed that the estimates of
diÔ¨Äerences in classiÔ¨Åcation rates (TPR, FPR, TNR, and FNR) between females and males have non-
overlapping conÔ¨Ådence intervals for all Ô¨Åve models. Second, the relation plot in Ô¨Ågure 22 shows that the
diÔ¨Äerences between females and males in terms of FPR and FNR are problematic. This means that we
identify bias in the AIR project.
Furthermore, the bias is disadvantageous for females in the sense that males who do not fall are more
likely to be provided fall prevention training than females who do not fall (FPR), while females who fall
are less likely to be provided fall prevention training than males who fall (FNR). We emphasize that the
FNR bias is particularly problematic since falling can have severe consequences for elderly citizens.
Research question 2 is answered by showing that all four bias mitigation techniques ( dropping gen-
der,gender swap ,disparate impact removal andlearning fair representations ) successfully mitigated
bias. However, the techniques did exhibit diÔ¨Äerences regarding the eÔ¨Äect on the classiÔ¨Åers‚Äô predicted
probabilities, false negative and false positive classiÔ¨Åcations:
‚Ä¢Dropping gender had undesired but small changes in the classiÔ¨Åcations and predicted probabilities.
‚Ä¢Gender swapping exhibited changes in the classiÔ¨Åcations and predicted probabilities that were as
desired.
‚Ä¢Disparate impact removal entailed changes in the classiÔ¨Åcations and predicted probabilities that
were somewhat problematic.
‚Ä¢Learning fair representations resulted in problematic changes in classiÔ¨Åcations and predicted prob-
abilities.
13.2 Recommendations
Short-term recommendations
We recommend that the AIR project team use the thesis‚Äôs bias identiÔ¨Åcation method when building the
AIR classiÔ¨Åcation model. If the thesis‚Äôs identiÔ¨Åcation method is applied, we recommend that the project
team examine whether the 80% is applicable in the AIR context. Furthermore, the team should inves-
tigate if other variables than gender are considered protected in the organizational setting. Moreover,
the AIR project team could evaluate whether the 95% conÔ¨Ådence intervals for estimates of classiÔ¨Åcation
rates are appropriate when considering the balance between type 1 and type 2 errors.
We recommend that the AIR project team consider using the two mitigation techniques: Dropping
gender and gender swapping.
Of the four tested mitigation techniques, dropping gender andgender swapping had the most desir-
able results in terms of classiÔ¨Åcations and probabilities. Furthermore, the techniques are simple and
easy to explain, which we deem advantageous for the AIR project. Gender may seem to be an irrelevant
feature to use when predicting the probability of falling. Therefore, dropping gender from the data set is
an obvious and easily explained avenue for bias mitigation. However, we also recommend that the team
investigates if any features act as proxies for gender since the gender-speciÔ¨Åc bias could run through the
proxies instead. Gender swapping aÔ¨Äected the classiÔ¨Åcations and probabilities in the most desired way.
7113 CONCLUSION
However, the technique could potentially lead to a decrease in performance if the protected subgroups
come from very diÔ¨Äerent distributions.
Nevertheless, the two techniques successfully mitigated the identiÔ¨Åed gender bias in the AIR project.
Long-term recommendations
In section 8 we describe a two-step system for bias mitigation, where one party attempts to mitigate
bias while another party attempts to maximize their utility when building a classiÔ¨Åer. If the AIR project
team and Aalborg Municipality wish to work with bias mitigation in the future we recommend this
approach. By structuring the eÔ¨Äorts to mitigate bias in this way, knowledge and practical know-how
regarding diÔ¨Äerent bias pre-processing techniques can be collected in one place. This could make it
easier to cooperate with other units in Aalborg Municipality and align bias mitigation eÔ¨Äorts across the
entire organization. The two-step system would enable Aalborg Municipality to cooperate with external
organizations or private companies, which might have proprietary algorithms. One could imagine that
Aalborg Municipality would be the owners of the data sets that are to be used to build models related
to future eÔ¨Äorts of bias mitigation. Therefore, it would be sensible to focus on pre-processing mitigation
techniques since the municipality could be expected to be responsible for processing and collecting the
data and naturally spend resources on pre-processing activities. Furthermore, there could be possibilities
in pre-processing mitigation techniques that work across application Ô¨Åelds, for example, by mitigating
bias in personal data for all citizens in Aalborg.
When focusing on the AIR project, we consider the 80% region to be an appropriate guideline for
bias identiÔ¨Åcation. However, other cases might entail that the 80% region is not acceptable as a guiding
threshold. For example, suppose Aalborg Municipality was to build an AI to support decisions regarding
more consequential interventions in people‚Äôs lives. In that case, decisions related to the work of social
services, one could argue that the acceptable region should be narrower, for instance, 90% or 95%. It
depends on the speciÔ¨Åc scenario, but we recommend that the 80% region is revised and reconsidered for
every new area where bias identiÔ¨Åcation and mitigation eÔ¨Äorts are made.
As a Ô¨Ånal remark, we believe that the identiÔ¨Åcation and mitigation techniques presented in the the-
sis could be useful for future eÔ¨Äorts to build less biased machine learning models.
72References
[1] Finansministeriet and Erhvervsministeriet, ‚ÄúNational strategi for kunstig intelligens,‚Äù 2019, available
at https://www.regeringen.dk/media/6537/ai-strategi_web.pdf, retrieved 09.03.2021.
[2] C. F. Smed. Interview with project owner. Conducted 08.02.2021.
[3] J.Zhao,T.Wang,M.Yatskar,R.Cotterell,V.Ordonez,andK.-W.Chang,‚ÄúGenderbiasincontextu-
alized word embeddings,‚Äù 2019, available at https://arxiv.org/abs/1904.03310, retrieved 10.02.2021.
[4] L. Sweeney, ‚ÄúDiscrimination in online ad delivery,‚Äù 2013, available at https://arxiv.org/pdf/1301.
6822.pdf, retrieved 18.06.2021.
[5] J. Buolamwini and T. Gebru, ‚ÄúGender Shades results,‚Äù 2018, available at http://gendershades.org/
overview.html, retrieved 11.05.2021.
[6] K. Hao. (2019) Ai is sending people to jail‚Äîand getting it wrong. Available at https://www.
technologyreview.com/2019/01/21/137783/algorithms-criminal-justice-ai/, retrieved 10.02.2021.
[7] J. Angwin, S. Mattu, J. Larson, and L. Kirchner, ‚ÄúMachine bias: there‚Äôs software used across the
country to predict future crimi- nals. and it‚Äôs biased against blacks,‚Äù 2016, available at https://www.
propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing, retrieved 10.02.2021.
[8] M. Rovatsos, B. Mittelstadt, and A. Koene, ‚ÄúLandscape summary: Bias in algorithmic
decision-making,‚Äù 2019, available at https://assets.publishing.service.gov.uk/government/uploads/
system/uploads/attachment_data/Ô¨Åle/819055/Landscape_Summary_-_Bias_in_Algorithmic_
Decision-Making.pdf, retrieved 08.03.2021.
[9] G. Shobha and S. Rangaswamy, ‚ÄúComputational analysis and understanding of natural languages:
Principles, methods and applications,‚Äù ser. Handbook of Statistics, V. N. Gudivada and C. Rao,
Eds. Elsevier, 2018, vol. 38, ch. Chapter 8 - Machine Learning, pp. 197‚Äì228, available at https:
//www.sciencedirect.com/science/article/pii/S0169716118300191, retrieved 02.05.2021.
[10] A. Joshi, Machine learning and artiÔ¨Åcial intelligence . Springer, 2020, ch. Chapter 1: Introduction
toAIandML,availableathttps://doi-org.proxy.Ô¨Åndit.dtu.dk/10.1007/978-3-030-26622-6, retrieved
02.05.2021.
[11] S. Verma and J. Rubin, ‚ÄúFairness deÔ¨Ånitions explained,‚Äù 2018, pp. 1‚Äì7, available at https://www.
ece.ubc.ca/~mjulia/publications/Fairness_DeÔ¨Ånitions_Explained_2018.pdf, retrieved 02.05.2021.
[12] T. Fawcett, ‚ÄúAn introduction to roc analysis,‚Äù Pattern Recognition Letters , vol. 27, no. 8, pp. 861‚Äì
874, 2006, available at https://www.sciencedirect.com/science/article/pii/S016786550500303X, re-
trieved 02.05.2021.
[13] D. Powers, ‚ÄúEvaluation: From precision, recall and f-factor to roc, informedness, markedness
correlation,‚Äù 2008, available at https://www.researchgate.net/publication/228529307_Evaluation_
From_Precision_Recall_and_F-Factor_to_ROC_Informedness_Markedness_Correlation,
retrieved 25.06.2021.
[14] E. M. Bender and B. Friedman, ‚ÄúData statements for natural language processing: Toward mit-
igating system bias and enabling better science,‚Äù Transactions of the Association for Computa-
tional Linguistics , vol. 6, 2018, available at https://www.aclweb.org/anthology/Q18-1041, retrieved
15.02.2021.
[15] T. Hellstr√∂m, V. Dignum, and S. Bensch, ‚ÄúBias in machine learning ‚Äì what is it good for?‚Äù 2020,
available at https://arxiv.org/abs/2004.00686, retrieved 22.02.2021.
[16] C. Dwork, M. Hardt, T. Pitassi, O. Reingold, and R. Zemel, ‚ÄúFairness through awareness,‚Äù 2011,
available at https://arxiv.org/abs/1104.3913, retrieved 21.02.2021.
[17] M. B. Zafar, I. Valera, M. Gomez Rodriguez, and K. P. Gummadi, ‚ÄúFairness beyond disparate treat-
ment & disparate impact,‚Äù 2017, available at http://dx.doi.org/10.1145/3038912.3052660, retrieved
21.02.2021.
73REFERENCES
[18] B. Jann, ‚ÄúThe blinder‚Äìoaxaca decomposition for linear regression models,‚Äù 2008, available at https:
//doi.org/10.1177/1536867X0800800401, retrieved 03.03.2021.
[19] F. Calmon, D. Wei, B. Vinzamuri, K. Natesan Ramamurthy, and K. R. Varshney,
‚ÄúOptimized pre-processing for discrimination prevention,‚Äù in Advances in Neural Informa-
tion Processing Systems , 2017, available at https://proceedings.neurips.cc/paper/2017/Ô¨Åle/
9a49a25d845a483fae4be7e341368e36-Paper.pdf, retrieved 03.03.2021.
[20] R. Zemel, Y. Wu, K. Swersky, T. Pitassi, and C. Dwork, ‚ÄúLearning fair representations,‚Äù 2013,
available at http://proceedings.mlr.press/v28/zemel13.html, retrieved 16.03.2021.
[21] M. Feldman, S. Friedler, J. Moeller, C. Scheidegger, and S. Venkatasubramanian, ‚ÄúCertifying
and removing disparate impact,‚Äù 2015, available at https://arxiv.org/pdf/1412.3756.pdf, retrieved
09.05.2021.
[22] California-State-Personnel-Board, ‚ÄúSummary of the uniform guidelines on employee selection proce-
dures,‚Äù 2003, available at https://spb.ca.gov/content/laws/selection_manual_appendixd.pdf , re-
trieved 19.06.2021.
[23] M. Hardt, E. Price, and N. Srebro, ‚ÄúEquality of opportunity in supervised learning,‚Äù 2016, available
at https://arxiv.org/abs/1610.02413, retrieved 09.03.2021.
[24] T. Bolukbasi, K.-W. Chang, J. Zou, V. Saligrama, and A. Kalai, ‚ÄúMan is to computer programmer
as woman is to homemaker? debiasing word embeddings,‚Äù 2016, available at https://arxiv.org/abs/
1607.06520, retrieved 10.02.2021.
[25] Aarhus.Universitet.(2020)AIRairehabilitering.Availableathttps://projekter.au.dk/air/, retrieved
10.02.2021.
[26] Agency.for.digitisation. (2020) Digitisation on the agenda in the annual bud-
get agreements. Available at https://en.digst.dk/news/news-archive/2020/august/
digitisation-on-the-agenda-in-the-annual-budget-agreements/, retrieved 12.02.2021.
[27] Aalborg.Kommune. Myndighedsafdelingen. Available at https://www.aalborg.dk/om-kommunen/
organisation/%C3%A6ldre-og-handicapforvaltningen/myndighedsafdelingen, retrieved 12.02.2021.
[28] T. M. of Social AÔ¨Äairs and the Interior. Consolidation act on social services. Available at http:
//english.sm.dk/media/14900/consolidation-act-on-social-services.pdf, retrieved 12.02.2021.
[29] We have held several meetings with Christian Marius Lillelund regarding development of the AIR
machine learning model. The meetings have not been recorded. Read the method section for further
details.
[30] Retsinformation. Bekendtg√∏relse af forvaltningsloven. Available at https://www.retsinformation.dk/
eli/lta/2014/433, retrieved 23.06.2021.
[31] C. M. Lillelund, Internal AIR report , retrieved 24.05.2021.
[32] Gyldendal. Matematisk modellering. Available at https://kolorit.gyldendal.dk/indhold/kolorit/
laerer/pdf/K9%20LV%20light%20Matematisk%20modellering.pdf, retrieved 04.06.2021.
[33] J. C. L. Nielsen, K. Holleufer, and H. H. B√ºlow, MetodeNU - introduktion til samfundsfaglige
metoder, 2021, ch. Chapter 2.2: Kvantitativ metode, available at https://metodenu.systime.dk/
?id=131, retrieved 04.06.2021.
[34] A. Universitet. Semistruktureret interview. Available at https://metodeguiden.au.dk/
semistruktureret-interview/, retrieved 04.06.2021.
[35] ‚Äî‚Äî. Kvalitativ metode. Available at https://metodeguiden.au.dk/kvalitativ-metode/, retrieved
04.06.2021.
[36] J. Larson, S. Mattu, L. Kirchner, and J. Angwin, ‚ÄúHow we analyzed the com-
pas recidivism algorithm,‚Äù 2016, available at https://www.propublica.org/article/
how-we-analyzed-the-compas-recidivism-algorithm, retrieved 05.04.2021.
74REFERENCES
[37] T. A. F. . A. Authors. aif360.algorithms.preprocessing.disparateimpactremover. Available at
https://aif360.readthedocs.io/en/latest/modules/generated/aif360.algorithms.preprocessing.
DisparateImpactRemover.html, retrieved 11.06.2021.
[38] ‚Äî‚Äî. aif360.algorithms.preprocessing.lfr. Available at https://aif360.readthedocs.io/en/latest/
modules/generated/aif360.algorithms.preprocessing.LFR.html, retrieved 22.06.2021.
[39] J. Brownlee, ‚ÄúA tour of machine learning algorithms,‚Äù 2019, available at https://
machinelearningmastery.com/a-tour-of-machine-learning-algorithms/, retrieved 03.05.2021.
[40] T. Hastie, R. Tibshirani, and J. Friedman, ‚ÄúThe elements of statistical learning,‚Äù 2008.
[41] scikit learn, ‚Äúsklearn. linear model. logisticregression,‚Äù 2021, available at https://scikit-learn.org/
stable/modules/generated/sklearn.linear_model.LogisticRegression.html, retrieved 07.06.2021.
[42] ‚Äî‚Äî, ‚Äúsklearn.svm.svc,‚Äù 2021, available at https://scikit-learn.org/stable/modules/generated/
sklearn.svm.SVC.html, retrieved 07.06.2021.
[43] ‚Äî‚Äî, ‚Äúsklearn.ensemble.randomforestclassiÔ¨Åer,‚Äù 2021, available at https://scikit-learn.org/stable/
modules/generated/sklearn.ensemble.RandomForestClassiÔ¨Åer.html, retrieved 07.06.2021.
[44] I. Goodfellow, Y. Bengio, and A. Courville, Deep Learning . MIT Press, 2016, ch. Chapter 6: Deep
Feedforward Networks, available at https://www.deeplearningbook.org/, retrieved 01.05.2021.
[45] M. A. Nielsen, Neural networks and deep learning . Determination press, 2015, ch. Chapter 1: Using
neural nets to recognize handwritten digits, Chapter 2: How the backpropagation algorithm works,
available at http://neuralnetworksanddeeplearning.com/chap1.html, retrieved 01.05.2021.
[46] xgboost developers. About xgboost. Available at https://xgboost.ai/about, retrieved 24.05.2021.
[47] B. Boehmke and B. Greenwell, Hands-On Machine Learning with R , 2020, ch. Chapter 12: Gradient
Boosting, available at https://bradleyboehmke.github.io/HOML/gbm.html, retrieved 25.05.2021.
[48] T. Chen and C. Guestrin, ‚ÄúXgboost: A scalable tree boosting system,‚Äù 2016, available at http:
//arxiv.org/abs/1603.02754, retrieved 24.06.2021.
[49] Y. Jin. Tree boosting with xgboost ‚Äì why does xgboost win ‚Äúevery‚Äù ma-
chine learning competition? Available at https://syncedreview.com/2017/10/22/
tree-boosting-with-xgboost-why-does-xgboost-win-every-machine-learning-competition/, retrieved
24.06.2021.
[50] xgboost developers. Introduction to boosted trees. Available at https://xgboost.readthedocs.io/en/
latest/tutorials/model.html, retrieved 24.06.2021.
[51] J. Brownlee. Tune xgboost performance with learning curves. Available at https:
//machinelearningmastery.com/tune-xgboost-performance-with-learning-curves/, retrieved
24.06.2021.
[52] C. F. Pedersen. Air (gitlab). Available at https://gitlab.au.dk/cfp/air, retrieved 24.06.2021.
[53] scikit learn, ‚Äú1.4 support vector machines,‚Äù 2021, available at https://scikit-learn.org/stable/
modules/svm.html#id11, retrieved 07.06.2021.
[54] J. C. Platt. Probabilistic outputs for svms and comparisons to regularized likelihood meth-
ods. Available at https://home.cs.colorado.edu/~mozer/Teaching/syllabi/6622/papers/Platt1999.
pdf, retrieved 07.06.2021.
[55] D. Statistik. Middellevetiden stiger fortsat. Available at https://www.dst.dk/da/Statistik/nyt/
NytHtml?cid=30217, retrieved 07.06.2021.
[56] C. Molnar, Interpretable Machine Learning , 2019, ch. Chapter 5.9-5.10, available at https://
christophm.github.io/interpretable-ml-book/, retrieved 04.06.2021.
[57] H. Suresh and J. V. Guttag, ‚ÄúA framework for understanding unintended consequences of machine
learning,‚Äù 2020, available at https://arxiv.org/abs/1901.10002, retrieved 10.02.2021.
75REFERENCES
[58] K. L. Calderon, ‚ÄúThe inÔ¨Çuence of gender on the frequency of pain and sedative medication ad-
ministered to postoperative patients,‚Äù 1990, available at https://link.springer.com/article/10.1007/
BF00289259 , retrieved 18.06.2021.
[59] A. Selbst, D. Boyd, S. Friedler, S. Venkatasubramanian, and J. Vertesi, ‚ÄúFairness and abstraction in
sociotechnical systems,‚Äù pp. 59‚Äì68, 01 2019, available at https://dl.acm.org/doi/10.1145/3287560.
3287598, retrieved 01.03.2021.
[60] R. Thomas, ‚ÄúGetting speciÔ¨Åc about algorithmic bias - rachel thomas,‚Äù 2019, available at https:
//www.youtube.com/watch?v=S-6YGPrmtYc&t=152s, retrieved 18.06.2021.
[61] K. Crawford, ‚ÄúThe trouble with bias - nips 2017 keynote - kate crawford,‚Äù 2017, youtube, Available
at https://www.youtube.com/watch?v=fMym_BKWQzk, retrieved 16.03.2021.
[62] N. Lee, P. Resnick, and G. Barton, ‚ÄúAlgorithmic bias detection and mitigation: Best practices
and policies to reduce consumer harms,‚Äù 2019, available at https://www.brookings.edu/research/
algorithmic-bias-detection-and-mitigation-best-practices-and-policies-to-reduce-consumer-harms/,
retrieved 01.03.2021.
[63] K. Kirkpatrick, ‚ÄúIt‚Äôs not the algorithm, it‚Äôs the data,‚Äù 2017, available at https://doi.org/10.1145/
3022181, retrieved 06.04.2021.
[64] ‚Äúcompas-analysis,‚Äù 2016, available at https://raw.githubusercontent.com/propublica/
compas-analysis/master/compas-scores-two-years.csv, retrieved 06.04.2021.
[65] FindLaw.com, ‚ÄúWhat‚ÄôsthediÔ¨Äerencebetweenamisdemeanorvs.felony?‚Äù 2019, availableathttps://
www.Ô¨Åndlaw.com/criminal/criminal-law-basics/what-distinguishes-a-misdemeanor-from-a-felony.
html, retrieved 13.04.2021.
76A BIAS METRICS FOR THE FIVE MODELS BUILT ON AIR DATA SET
A Bias metrics for the Ô¨Åve models built on AIR data set
Gender TPR FPR TNR FNR
Support Vector Machine
Female 63.6 25.7 74.3 36.4
(61.8-65.3) (24.9-26.6) (73.4-75.1) (34.7-38.2)
Male 78.0 38.2 61.8 22.0
(75.4-80.7) (36.8-39.6) (60.4-63.2) (19.3-24.6)
Logistic Regression
Female 53.9 33.8 66.2 46.1
(52.1-55.7) (32.9-34.6) (65.4-67.1) (44.3-47.9)
Male 68.7 44.4 55.6 31.3
(66.6-70.9) (43.1-45.7) (54.3-56.9) (29.1-33.4)
Random Forest
Female 55.0 4.5 95.5 45.0
(53.3-56.8) (4.2-4.9) (95.1-95.8) (43.2-46.7)
Male 63.0 5.7 94.3 37.0
(60.6-65.3) (5.0-6.4) (93.6-95.0) (34.7-39.4)
FFNN
Female 33.5 6.6 93.4 66.5
(31.0-36.1) (5.8-7.3) (92.7-94.2) (63.9-69.0)
Male 39.0 8.8 91.2 61.0
(35.4-42.6) (7.7-9.9) (90.1-92.3) (57.4-64.6)
XGBoost
Female 59.8 13.7 86.3 40.2
(57.7-62.0) (12.4-15.0) (85.0-87.6) (38.0-42.3)
Male 68.3 17.4 82.6 31.7
(65.9-70.6) (15.7-19.2) (80.8-84.3) (29.4-34.1)
Table 15: ClassiÔ¨Åcation metrics of algorithms grouped by gender. The original models.
77A BIAS METRICS FOR THE FIVE MODELS BUILT ON AIR DATA SET
Gender Accuracy
Support Vector Machine
Female: 72.0
(71.3-72.7)
Male: 65.7
(64.8-66.7)
Total: 69.7
(69.2-70.3)
Logistic Regression
Female: 63.7
(63.0-64.4)
Male: 58.9
(57.7-60.0)
Total: 61.9
(61.4-62.4)
Random Forest
Female: 87.1
(86.6-87.5)
Male: 86.4
(85.7-87.1)
Total: 86.8
(86.4-87.2)
FFNN
Female 81.0
(80.3-81.6)
Male 78.1
(77.2-79.1)
Total 79.9
(79.3-80.5)
XGBoost
Female 80.5
(79.9-81.1)
Male 79.5
(78.4-80.5)
Total 80.1
(79.5-80.7)
Table 16: Accuracy of algorithms grouped by gender. Original models.
78A BIAS METRICS FOR THE FIVE MODELS BUILT ON AIR DATA SET
Gender TPR FPR TNR FNR
Support Vector Machine
Female 65.4 27.2 72.8 34.6
(63.7-67.2) (26.3-28.1) (71.9-73.7) (32.8-36.3)
Male 68.5 30.0 70.0 31.5
(65.7-71.3) (28.8-31.2) (68.8-71.2) (28.7-34.3)
Logistic Regression
Female 57.2 37.2 62.8 42.8
(55.4-59.0) (36.3-38.0) (62.0-63.7) (41.0-44.6)
Male 64.1 39.4 60.6 35.9
(62.0-66.2) (38.1-40.7) (59.3-61.9) (33.8-38.0)
Random Forest
Female 55.6 5.0 95.0 44.4
(53.9-57.4) (4.6-5.4) (94.6-95.4) (42.6-46.1)
Male 59.2 4.6 95.4 40.8
(56.8-61.7) (4.0-5.2) (94.8-96.0) (38.3-43.2)
FFNN
Female 33.2 6.7 93.3 66.8
(30.1-36.3) (5.9-7.5) (92.5-94.1) (63.7-69.9)
Male 35.1 8.0 92.0 64.9
(32.2-38.1) (6.9-9.0) (91.0-93.1) (61.9-67.8)
XGBoost
Female 61.5 17.0 83.0 38.5
(59.5-63.5) (15.9-18.1) (81.9-84.1) (36.5-40.5)
Male 66.8 16.2 83.8 33.2
(64.3-69.3) (15.1-17.3) (82.7-84.9) (30.7-35.7)
Table 17: ClassiÔ¨Åcation metrics of algorithms grouped by gender - Dropping the protected variable
79A BIAS METRICS FOR THE FIVE MODELS BUILT ON AIR DATA SET
Gender Accuracy
Support Vector Machine
Females: 71.3
(70.6-72.0)
Males: 69.6
(68.6-70.5)
Total: 70.7
(70.1-71.2)
Logistic Regression
Females: 61.7
(60.9-62.4)
Males: 61.5
(60.4-62.6)
Total: 61.6
(61.1-62.1)
Random Forest
Females: 86.82
(86.34-87.31)
Males: 86.29
(85.55-87.02)
Total: 86.63
(86.2-87.06)
FFNN
Female 80.8
(80.2-81.4)
Male 77.8
(76.9-78.8)
Total 79.7
(79.2-80.3)
XGBoost
Female 78.5
(77.6-79.4)
Male 79.4
(78.3-80.5)
Total 78.8
(78.1-79.6)
Table 18: Accuracy of algorithms grouped by gender when dropping gender variable.
80A BIAS METRICS FOR THE FIVE MODELS BUILT ON AIR DATA SET
Model Females (Fall) Males (Fall) Females (No Fall) Males (No Fall)
SVM 31.6 32.7 19.4 20.1
(31.1-32.1) (32.0-33.3) (19.2-19.6) (19.8-20.5)
LR 52.0 54.1 44.0 44.4
(51.4-52.6) (53.4-54.8) (43.7-44.4) (43.9-44.9)
RF 50.2 53.3 15.0 14.5
(49.2-51.2) (52.0-54.5) (14.7-15.3) (14.0-14.9)
FFNN 35.5 36.4 18.0 19.0
(34.7-36.4) (35.3-37.4) (17.7-18.3) (18.6-19.4)
XGBoost 55.8 58.0 23.1 23.9
(54.8-56.9) (56.8-59.2) (22.7-23.6) (23.3-24.5)
Table 19: Mean and 95% conÔ¨Ådence interval for predicted probabilities conditioned by actual fall and
gender. From models trained on data where gender is dropped.
Gender TPR FPR TNR FNR
Support Vector Machine
Female: 67.0 24.1 75.9 33.0
(65.4-68.6) (23.3-24.8) (75.2-0.8) (31.4-34.6)
Male: 69.5 27.8 72.19 30.5
(67.0-72.1) (26.8-28.8) (71.2-73.2) (27.9-33.0)
Logistic Regression
Female: 57.3 37.0 63.0 42.7
(55.5-59.2) (36.2-37.9) (62.1-0.6) (40.8-44.5)
Male: 64.7 38.8 61.16 35.3
(62.6-66.8) (37.5-40.2) (59.8-62.5) (33.2-37.4)
Random Forest
Female: 57.5 5.4 94.6 42.5
(55.8-59.3) (5.0-5.8) (94.2-1.0) (40.7-44.2)
Male: 60.5 4.9 95.15 39.5
(57.9-63.1) (4.2-5.5) (94.5-95.8) (36.9-42.1)
FFNN
Female 35.8 7.3 92.7 64.2
(33.1-38.6) (6.5-8.1) (91.9-93.5) (61.4-66.9)
Male 38.6 8.5 91.5 61.4
(34.8-42.4) (7.5-9.5) (90.5-92.5) (57.6-65.2)
XGBoost
Female 61.2 15.3 84.7 38.8
(59.1-63.3) (14.2-16.5) (83.5-85.8) (36.7-40.9)
Male 65.6 14.8 85.2 34.4
(63.5-67.8) (13.3-16.3) (83.7-86.7) (32.2-36.5)
Table 20: Performance metrics of algorithms grouped by gender - Gender Swap
81A BIAS METRICS FOR THE FIVE MODELS BUILT ON AIR DATA SET
Gender Accuracy
Support Vector Machine
Female: 74.1
(73.4-74.8)
Male: 71.5
(70.6-72.3)
Total: 73.1
(72.6-73.7)
Logistic Regression
Female: 61.8
(61.0-62.5)
Male: 62.1
(60.9-63.2)
Total: 61.9
(61.4-62.4)
Random Forest
Female: 86.9
(86.4-87.3)
Male: 86.4
(85.6-87.2)
Total: 86.7
(86.3-87.1)
FFNN
Female 80.8
(80.2-81.4)
Male 78.3
(77.2-79.3)
Total 79.9
(79.3-80.5)
XGBoost
Female 79.8
(78.9-80.6)
Male 80.3
(79.2-81.5)
Total 80.0
(79.1-80.9)
Table 21: Accuracy of algorithms grouped by gender when swapping gender.
82A BIAS METRICS FOR THE FIVE MODELS BUILT ON AIR DATA SET
Model Females (Fall) Males (Fall) Females (No Fall) Males (No Fall)
SVM 38.4 39.7 16.9 18.0
(37.6-39.3) (38.7-40.7) (16.6-17.3) (17.6-18.5)
LR 52.1 54.4 43.6 43.9
(51.5-52.7) (53.7-55.1) (43.3-44.0) (43.3-44.4)
RF 55.1 57.4 13.7 13.1
(54.0-56.3) (56.0-58.8) (13.3-14.1) (12.6-13.5)
FFNN 35.1 36.7 17.8 18.6
(34.2-35.9) (35.7-37.7) (17.5-18.0) (18.2-19.0)
XGBoost 55.9 57.9 21.4 21.8
(54.7-57.0) (56.6-59.2) (21.0-21.9) (21.2-22.4)
Table 22: Mean and 95% conÔ¨Ådence interval for predicted probabilities conditioned by actual fall and
gender. From models trained on gender swapped data.
Gender TPR FPR TNR FNR
Support Vector Machine
Female: 64.8 27.5 72.5 35.2
(63.0-66.6) (26.5-28.4) (71.6-0.7) (33.4-37.0)
Male: 68.9 29.9 70.1 31.1
(66.4-71.5) (28.9-30.9) (69.1-71.1) (28.5-33.6)
Logistic Regression
Female: 57.8 37.9 62.1 42.2
(56.1-59.5) (36.9-38.9) (61.1-0.6) (40.5-43.9)
Male: 63.9 38.8 61.24 36.1
(61.8-66.0) (37.5-40.0) (60.0-62.5) (34.0-38.2)
Random Forest
Female: 48.0 5.0 95.0 52.0
(46.1-49.9) (4.5-5.4) (94.6-1.0) (50.1-53.9)
Male: 56.1 4.6 95.39 43.9
(53.7-58.6) (4.0-5.2) (94.8-96.0) (41.4-46.3)
FFNN
Female 32.2 6.5 93.5 67.8
(28.9-35.6) (5.5-7.5) (92.5-94.5) (64.4-71.1)
Male 34.4 9.3 90.7 65.6
(30.6-38.2) (8.0-10.7) (89.3-92.0) (61.8-69.4)
XGBoost
Female 55.0 16.2 83.8 45.0
(52.4-57.5) (14.7-17.8) (82.2-85.3) (42.5-47.6)
Male 60.5 16.5 83.5 39.5
(58.1-62.9) (14.7-18.2) (81.8-85.3) (37.1-41.9)
Table 23: ClassiÔ¨Åcation metrics of algorithms grouped by gender - Disparate Impact Removal
83A BIAS METRICS FOR THE FIVE MODELS BUILT ON AIR DATA SET
Gender Accuracy
Support Vector Machine
Female: 70.9
(70.2-71.7)
Male: 69.8
(69.0-70.6)
Total: 70.5
(70.0-71.1)
Logistic Regression
Female: 61.2
(60.4-62.0)
Male: 62.0
(60.8-63.1)
Total: 61.5
(60.9-62.0)
Random Forest
Female: 85.2
(84.7-85.8)
Male: 85.5
(84.8-86.2)
Total: 85.3
(84.9-85.8)
FFNN
Female 80.7
(80.1-81.3)
Male 76.6
(75.8-77.4)
Total 79.2
(78.7-79.7
XGBoost
Female 77.8
(76.7-78.9)
Male 77.8
(76.5-79.0)
Total 77.8
(76.8-78.9)
Table 24: Accuracy of algorithms grouped by gender - Disparate Impact Removal.
84A BIAS METRICS FOR THE FIVE MODELS BUILT ON AIR DATA SET
Model Females (Fall) Males (Fall) Females (No Fall) Males (No Fall)
SVM 31.5 33.0 19.4 20.2
(31.0-32.1) (32.4-33.7) (19.1-19.6) (19.8-20.5)
LR 52.3 54.1 44.4 44.0
(51.7-52.9) (53.4-54.8) (44.0-44.7) (43.5-44.5)
RF 45.0 49.7 15.8 15.2
(44.1-45.9) (48.5-50.9) (15.5-16.2) (14.8-15.7)
FFNN 35.4 35.7 18.6 19.0
(34.6-36.2) (34.7-36.7) (18.3-18.9) (18.6-19.4)
XGBoost 50.6 54.2 23.9 24.0
(49.6-51.7) (52.9-55.5) (23.5-24.4) (23.4-24.7)
Table 25: Mean and 95% conÔ¨Ådence interval for predicted probabilities conditioned by actual fall and
gender. From models trained on DI removal data.
Figure 36: Distributions of LoanPeriod for females (blue) and males (green) using the disparate impact
removal method at diÔ¨Äerent repairing levels
85A BIAS METRICS FOR THE FIVE MODELS BUILT ON AIR DATA SET
Gender TPR FPR TNR FNR
Support Vector Machine
Female: 64.4 27.5 72.5 35.6
(62.6-66.2) (26.7-28.2) (71.8-0.7) (33.8-37.4)
Male: 66.1 31.7 68.25 33.9
(63.5-68.7) (30.5-33.0) (67.0-69.5) (31.3-36.5)
Logistic Regression
Female: 60.5 37.9 62.1 39.5
(58.7-62.4) (37.2-38.7) (61.3-0.6) (37.6-41.3)
Male: 62.0 39.0 61.05 38.0
(59.8-64.3) (37.6-40.3) (59.7-62.4) (35.7-40.2)
Random Forest
Female: 52.0 4.2 95.8 48.0
(50.3-53.7) (3.9-4.6) (95.4-1.0) (46.3-49.7)
Male: 58.1 5.5 94.5 41.9
(55.9-60.2) (4.9-6.1) (93.9-95.1) (39.8-44.1)
FFNN
Female 22.9 5.6 94.4 77.1
(20.0-25.8) (4.8-6.3) (93.7-95.2) (74.2-80.0)
Male 24.5 6.6 93.4 75.5
(21.1-27.9) (5.6-7.6) (92.4-94.4) (72.1-78.9)
XGBoost
Female 55.8 14.3 85.7 44.2
(53.6-57.9) (12.9-15.6) (84.4-87.1) (42.1-46.4)
Male 63.7 16.2 83.8 36.3
(61.2-66.2) (14.8-17.6) (82.4-85.2) (33.8-38.8)
Table 26: ClassiÔ¨Åcation metrics of algorithms grouped by gender - Learning Fair Representations
86A BIAS METRICS FOR THE FIVE MODELS BUILT ON AIR DATA SET
Gender Accuracy
Support Vector Machine
Female: 70.8
(70.2-71.4)
Male: 67.7
(66.8-68.5)
Total: 69.7
(69.2-70.1)
Logistic Regression
Female: 61.7
(61.0-62.4)
Male: 61.4
(60.2-62.5)
Total: 61.6
(61.1-62.1)
Random Forest
Female: 86.7
(86.2-87.1)
Male: 85.3
(84.6-86.0)
Total: 86.2
(85.8-86.6)
FFNN
Female 79.5
(78.8-80.1)
Male 76.2
(75.3-77.0)
Total 78.3
(77.7-78.9)
XGBoost
Female 79.5
(78.4-80.5)
Male 78.7
(77.7-79.7)
Total 79.2
(78.3-80.1)
Table 27: Accuracy of algorithms grouped by gender - Learning Fair Representations.
87A BIAS METRICS FOR THE FIVE MODELS BUILT ON AIR DATA SET
Model Females (Fall) Males (Fall) Females (No Fall) Males (No Fall)
SVM 31.2 31.8 19.6 20.4
(30.7-31.7) (31.2-32.4) (19.4-19.8) (20.0-20.7)
LR 52.4 52.9 44.6 44.4
(51.8-52.9) (52.3-53.6) (44.3-44.9) (43.9-44.8)
RF 47.2 52.6 15.0 15.9
(46.2-48.1) (51.4-53.7) (14.7-15.3) (15.4-16.4)
FFNN 32.8 33.9 19.3 20.2
(32.1-33.5) (33.1-34.8) (19.0-19.5) (19.9-20.6)
XGBoost 52.1 56.6 22.6 24.5
(51.0-53.2) (55.3-57.8) (22.1-23.0) (23.9-25.1)
Table 28: Mean and 95% conÔ¨Ådence interval for predicted probabilities conditioned by actual fall and
gender. From models trained on LFR data.
Figure 37: Distribution of numerical features for females and males with LFR.
88A BIAS METRICS FOR THE FIVE MODELS BUILT ON AIR DATA SET
Model TP FP TN FN
No mitigation
SVM 15.5 23.4 54.3 6.8
LR 13.4 29.2 48.5 8.9
RF 13.0 3.8 73.9 9.3
FFNN 7.9 5.7 72.0 14.3
XGBoost 14.2 11.8 65.9 8.1
Dropping gender
SVM -0.6 -1.5 +1.5 +0.6
LR 0 +0.3 -0.3 0
RF -0.3 0 0 +0.3
FFNN -0.3 -0.2 +0.2 +0.4
XGBoost -0.1 +1.2 -1.2 +0.1
Gender swap
SVM -0.3 -3.7 +3.7 +0.3
LR +0.1 +0.1 -0.1 -0.1
RF +0.1 +0.3 -0.3 -0.1
FFNN +0.3 +0.3 -0.3 -0.2
XGBoost -0.2 -0.1 +0.1 +0.2
Disparate Impact Removal
SVM -0.7 -1.4 +1.4 +0.7
LR +0.1 +0.5 -0.5 -0.1
RF -1.6 0 0 +1.6
FFNN -0.7 0 0 +0.8
XGBoost -1.5 +0.8 -0.8 +1.5
Learning Fair Representations
SVM -1.0 -0.8 +0.8 +1.0
LR +0.2 +0.5 -0.5 -0.2
RF -1.1 -0.1 +0.1 +1.0
FFNN -2.7 -1.1 +1.1 +2.8
XGBoost -1.1 -0.2 +0.2 +1.1
Table 29: Normalised confusion matrix for all models and mitigation methods - with changes in per-
centages points from the original. Colors indicate: desired (green) and not-desired (red) result.
89A BIAS METRICS FOR THE FIVE MODELS BUILT ON AIR DATA SET
Model TP FP TN FN
No mitigation
SVM 15.5 23.4 54.3 6.8
(13.6-17.4) (20.9-26.0) (50.5-58.0) (6.1-7.5)
LR 13.4 29.2 48.5 8.9
(11.5-15.2) (27.2-31.2) (45.3-51.8) (8.3-9.5)
RF 13.0 3.8 73.9 9.3
(11.7-14.2) (3.6-4.1) (72.4-75.4) (9.1-9.5)
FFNN 7.9 5.7 72.0 14.3
(7.5-8.4) (5.1-6.4) (71.3-72.6) (13.9-14.8)
XGBoost 14.2 11.8 65.9 8.1
(13.9-14.5) (11.4-12.2) (65.5-66.3) (7.8-8.4)
Dropping gender
SVM 14.9 21.9 55.8 7.4
(13.7-16.0) (21.3-22.5) (54.1-57.4) (7.1-7.8)
LR 13.4 29.5 48.2 8.9
(12.1-14.7) (29.2-29.8) (46.8-49.5) (8.7-9.1)
RF 12.7 3.8 73.9 9.6
(11.7-13.7) (3.6-4.0) (72.8-75.1) (9.2-10.0)
FFNN 7.6 5.5 72.2 14.7
(7.0-8.2) (5.0-6.1) (71.6-72.7) (14.1-15.3)
XGBoost 14.1 13.0 64.7 8.2
(13.8-14.5) (12.2-13.8) (63.9-65.5) (7.8-8.5)
Gender swap
SVM 15.2 19.7 58.0 7.1
(14.0-16.3) (19.1-20.3) (56.2-59.8) (6.7-7.6)
LR 13.5 29.3 48.4 8.8
(12.2-14.8) (28.9-29.6) (47.1-49.7) (8.7-9.0)
RF 13.1 4.1 73.6 9.2
(12.1-14.0) (3.8-4.3) (72.5-74.7) (8.8-9.7)
FFNN 8.2 6.0 71.7 14.1
(7.8-8.7) (5.5-6.5) (71.2-72.2) (13.6-14.5)
XGBoost 14.0 11.7 66.0 8.3
(13.7-14.4) (10.9-12.6) (65.1-66.8) (7.9-8.6)
Disparate Impact Removal
SVM 14.8 22.0 55.7 7.5
(13.6-16.0) (21.4-22.6) (54.1-57.3) (7.2-7.7)
LR 13.5 29.7 48.0 8.8
(12.2-14.7) (29.1-30.3) (47.0-49.0) (8.7-9.0)
RF 11.4 3.8 73.9 10.9
(10.2-12.6) (3.5-4.0) (72.8-75.1) (10.6-11.2)
FFNN 7.2 5.7 72.0 15.1
(6.8-7.7) (5.3-6.2) (71.5-72.4) (14.6-15.5)
XGBoost 12.7 12.6 65.1 9.6
(12.2-13.3) (11.9-13.4) (64.3-65.8) (9.0-10.1)
Learning Fair Representations
SVM 14.5 22.6 55.1 7.8
(13.5-15.5) (21.8-23.3) (53.2-57.1) (7.4-8.2)
LR 13.6 29.7 48.0 8.7
(12.7-14.5) (29.2-30.2) (47.0-49.0) (8.3-9.1)
RF 11.9 3.7 74.0 10.3
(10.7-13.2) (3.4-4.0) (72.5-75.6) (10.2-10.5)
FFNN 5.2 4.6 73.1 17.1
(4.6-5.8) (4.2-5.1) (72.6-73.5) (16.5-17.7)
XGBoost 13.1 11.6 66.1 9.2
(12.9-13.3) (10.7-12.5) (65.2-67.0) (9.0-9.4)
Table 30: Normalised confusion matrix for all models and mitigation methods. Colors indicate: desired
(green) and not-desired (red) result.90B ASSESSMENT OF CLASSIFICATION THRESHOLDS
B Assessment of classiÔ¨Åcation thresholds
Figure 38: Relation of TPR between female and male vs. threshold for binary classiÔ¨Åcation. Models
built on original dataset.
91B ASSESSMENT OF CLASSIFICATION THRESHOLDS
Figure 39: Relation of TNR between female and male vs. threshold for binary classiÔ¨Åcation. Models
built on original dataset.
92B ASSESSMENT OF CLASSIFICATION THRESHOLDS
Figure 40: Relation of FPR between female and male vs. threshold for binary classiÔ¨Åcation. Models
built on original dataset.
93B ASSESSMENT OF CLASSIFICATION THRESHOLDS
Figure 41: Relation of FNR between female and male vs. threshold for binary classiÔ¨Åcation. Models
built on original dataset.
94C ROC CURVES
C ROC curves
Figure 42: ROC curve and AUC for the models build on the original data.
95C ROC CURVES
Figure 43: ROC curve and AUC for the models build on the data without gender.
96C ROC CURVES
Figure 44: ROC curve and AUC for the models build on the data with gender swap.
97C ROC CURVES
Figure 45: ROC curve and AUC for the models build on the data with disparate impact removal.
98C ROC CURVES
Figure 46: ROC curve and AUC for the models build on the data with LFR.
99D SHAP VALUES
D SHAP values
Figure 47: Absolute shap values.
100D SHAP VALUES
Figure 48: Top 10 negative and positive features based on SHAP value
101D SHAP VALUES
Figure 49: Shap values of the numerical features
102E COMPARING PROBABILITIES ACROSS THE MODELS
E Comparing probabilities across the models
The diÔ¨Äerences in the distributions of probabilities mean that a somewhat arbitrary choice of model has
a substantial impact on the distribution of the predicted probabilities that will be used to predict the risk
scores of citizens when the AIR model is implemented. This could easily be overlooked since the intuition
behind predicted probabilities is that they are comparable between models - which for these Ô¨Åve models
on the AIR data set does not seem to be the case. This point can be further explored by showing the
range of predictions for a number of randomly drawn citizens from the AIR data set. Table 31 shows the
average of the cross validates predicted probabilities and binary classiÔ¨Åcations for six randomly drawn
citizens.
Citizen Fall SVM LR RF FFNN XGBoost
1 Yes 40.1 57.8 78.2 47.7 87.6
2 No 13.6 38.5 7.9 10.3 29.1
3 Yes 45.7 51.4 78.6 28.0 80.1
4 No 14.3 64.7 12.9 16.4 33.7
5 Yes 47.8 71.2 70.8 39.2 79.4
6 No 9.0 30.2 7.2 11.2 2.1
Table 31: Six citizens‚Äô respectively predicted probabilities across all Ô¨Åve models.
In table 31 we identify a large span of probabilities across the models for the same citizen. For example,
citizen 1, who has fallen, has a range of predicted probabilities from 40.1% to 87.6%. If a caseworker
assesses whether citizen 1 should be provided fall training, one could argue that there is a substantial
diÔ¨Äerence between getting a probability score of 40.1% and 87.6%. If the AIR project intends to use the
probabilities of the AIR model, it could be relevant to keep this in mind. This is particularly true since
it seems that XGBoost has the highest predicted probabilities for those who fall and the second-highest
predicted probabilities for those who do not fall, which can be seen in table 10.
103F LINEAR REGRESSION ANALYSIS
F Linear regression analysis
To further nuance our understanding of how the variables are correlated with the probability of falling
and how the covariates aÔ¨Äect each other, we will build four nested linear regression models, where we
iteratively include more features to the covariates used in each model and observe what happens with
the coeÔ¨Écients and p-values. We will show the results of all models Ô¨Årst and then comment on what we
learn about the relations between the variables subsequently. The models are Ô¨Åt to the standardised co-
variates. We will only comment on the direction of the coeÔ¨Écient and change in p-values of the variables,
not on the magnitudes of the coeÔ¨Écient. Furthermore, we do not wish to conclude or generalise results
from the linear regression analysis, the purpose is simply to obtain a more nuanced understanding of the
domain.
We start with the simplest model, that only predicts the fall probability using: age,loan period and
number of aids . The coeÔ¨Écients and p-values can be seen in table 32.
Feature CoeÔ¨Écient P-value
Age 0.0211 0.021
Loan Period -0.0251 0.006
Number of aids -0.0038 0.683
Adj. R-squared = 0.050
Table 32: Regression model 1: Only age, loan period and number of aids.
For the second model, the one-hot-encoded aids and clusters are added to the covariates. The result can
be assessed in table 33. The coeÔ¨Écients and p-values of the OHE covariate are not shown in the table
below for practical reasons (large table) and since the focus of the analysis is the numerical features.
Feature CoeÔ¨Écient P-value
Age 0.0169 0.123
Loan Period -0.0317 0.008
Number of aids 0.0614 0.079
Adj. R-squared = 0.060
Table 33: Regression model 2: OHE aids and clusters added.
For the third model, gender is also added to the regression analysis. The result can be seen in table 34.
Feature CoeÔ¨Écient P-value
Gender (1: male) 0.0518 0.016
Age 0.0175 0.111
Loan Period -0.0276 0.022
Number of aids 0.0618 0.076
Adj. R-squared = 0.063
Table 34: Regression model 3: Gender added
In the Ô¨Ånal model, interaction terms between gender and age, loan period and number of aids are added
to the model. The results is presented in table 35
104F LINEAR REGRESSION ANALYSIS
Feature CoeÔ¨Écient P-value
Gender (1: male) 0.0325 0.159
Age 0.0015 0.907
Loan Period -0.0160 0.222
Number of aids 0.0732 0.039
Gender (1: male) * Age 0.0493 0.030
Gender (1: male) * Loan Period -0.0541 0.035
Gender (1: male) * Number of aids -0.0230 0.385
Adj. R-squared = 0.066
Table 35: Regression model 4: Interaction terms added
The p-value of ageincreases when adding the OHE aids and clusters in model 2 and remains high when
adding gender in model 3 and the interaction terms in model 4. This indicates that the citizen‚Äôs age
does not impact the probability of falling when the aids that the citizen uses are taken into account.
However, in model 4, the interaction term between gender and age is positively correlated with falling
and has a low p-value. This means that the coeÔ¨Écient describing men‚Äôs ‚Äôreturn‚Äô on the age variable is
higher than women‚Äôs. In other words, compared to women, men seem to be impacted to a higher degree
when becoming older in relation to the risk of falling. This is in line with the pattern that could be seen
from the box plot in Ô¨Ågure 18.
The p-value of loan period is low and negatively correlated with the probability of falling for model 1-3,
meaning that the longer a citizen has loaned aids on average, the lower the probability of falling. This is
contrary to our initial expectation, where we imagined a positive relation between loan period and the
probability of falling. On this causal relation, we note that there might be a selection problem, in that
those citizens who live long into their senior years are perhaps also more physically and mentally Ô¨Åt.
Therefore, atsome point, whentheless Ô¨Åt elderly -unfortunately- die, those who remain are impacted by
their age to a lesser degree. In other words, the mapping between age, loan period and a general notion
of physical and mental Ô¨Åtness is not the same across the entire age span. This selection problem might
be the cause of the somewhat surprising negative correlation between loan period and the probability of
falling. In model 4, where the interaction terms are included, the interaction term between loan period
and gender has a low p-value. Here, men have more negatively correlated returns on loan period than
women, meaning that loaning aids for a longer period of time diminishes men‚Äôs risk of falling at a higher
rate than women‚Äôs risk of falling. This result is in line with Ô¨Ågure 19, where we saw that those who did
not fall had longer loan periods than those who did fall. Again, the interpretation of the causal eÔ¨Äect
of loan period is not straightforward. Having a long loan period may indicate a long history of physical
impairment (high likelihood of falling) or reaching an old age with the help of aids while retaining ones
physical abilities (low likelihood of falling). The fact that age and loan period exhibit opposite patterns
is surprising - since they both reÔ¨Çect the passing of time in some sense. This could be due to the selection
problem mentioned earlier.
The number of aids has a high p-value in model 1 but adding the OHE aid and cluster covari-
ates in model 2 lowers the p-value, and it remains low and positively correlated with the probability
of falling in model 3 and model 4. This implies that many aids by themselves are not correlated with
falling. However, when considering which speciÔ¨Åc aids are used by adding the OHE aids, then more aids
result in a higher probability of falling. This clariÔ¨Åes the eÔ¨Äect of number of aids, which did not have a
clear interpretation when assessing the box plots in Ô¨Ågure 20. When including the interaction term in
the Ô¨Ånal model, it can be seen that p-value of the interaction term between gender and the number of
aids is high. This implies that there are no gender-related diÔ¨Äerences regarding the returns of number
of aids on the probability of falling.
105G QUALITATIVE THEORY OF BIAS IN MACHINE LEARNING
G Qualitative theory of bias in machine learning
This section reviews qualitative approaches for identifying bias and understanding how to asses bias-
related challenges in machine learning projects. Since bias has diÔ¨Äerent meanings [61], it can also be
assessed in diÔ¨Äerent ways. Furthermore, as the thesis focus on examining machine learning models used
for decision-support in the public sector, we Ô¨Ånd it necessary to remember that "(...) humans are deeply
involved in all parts of the machine learning process" [15, p. 7].
We expect that the qualitative approaches can create a foundation for a critical reÔ¨Çection of bias evalua-
tion - by not only focus on the technical machine learning aspects - but also assess problems related to the
context of the algorithms. The motivation for incorporating qualitative approaches for bias evaluation
is greatly expressed by Selbst et. al:
"Achieving fairness in machine learning systems requires embracing a socio-technical view. That is,
technical actors must shift from seeking a solution to grappling with diÔ¨Äerent frameworks that provide
guidance in identifying, articulating, and responding to fundamental tensions, uncertainties, and con-
Ô¨Çicts inherent in sociotechnical systems‚Äù [59, p. 63].
Selbst et al. emphasize that machine learning systems become a part of socio-technical systems. Achiev-
ing fairness in ML requires embracing a socio-technical view on machine learning projects. The thesis
evaluates bias in ML algorithms, but since these algorithms potentially become a part of socio-technical
systems - grappling frameworks for assessing uncertainties can help support the elucidation of bias in
machine learning. Therefore, using qualitative frameworks is anticipated to help to examine both bias
in relation to machine learning projects.
Understanding Unintended Consequences of Machine Learning
Intheir2020researchpaper, SureshandGuttagidentifysixdiÔ¨Äerenttypesofbiasinthemachinelearning
pipeline [57]. These can be seen in Ô¨Ågure 50 below.
106G QUALITATIVE THEORY OF BIAS IN MACHINE LEARNING
Figure 50: Six types of bias (red) in the machine learning pipeline [57].
Historical bias
Historical bias arises when there is a misalignment between the properties reÔ¨Çected in the world as-it-is
and the normative objectives of an ML model, such as historical data reÔ¨Çecting structural racism and an
ML-model that tries to avoid reproducing structural racism. This type of bias can exist despite perfect
sampling and feature selection. Evaluation of historical bias often involves the examination of represen-
tational harm to a particular identity group, such as reinforcing stereotypes [57].
Representation bias
Representation bias relates to the process of deÔ¨Åning and sampling the development population . The bias
arises when the development population under-represents some part of the use population , and it can
lead to low performance for the group that is under-represented. Selection bias is when the probability
distribution for the development population is not equal to the distribution of the use population [57].
Representation bias can also occur without selection bias when for example, if a minority group makes
up only a small proportion of the true distribution. In this case, even sampling in a representative way
from true distribution will likely lead to a less robust model for the minority group [57].
As an example, representation bias occurs in the facial recognition case in section 2, where the high
misclassiÔ¨Åcation rates of the dark-skinned females could indicate representation bias - here, the data
used by the companies had too few dark-skinned females represented.
Measurement bias
Measurement bias has its source in the choosing,collection andcomputing of features and labels to be
used in ML-models. Suresh and Guttag [57] argue that the available variables are typically noisy proxies
for the actual features and labels of interest.For example, to measure the label "crime" or a diÔ¨Äuse notion
of "riskiness" the proxy variable "arrest" is often used. When choosing, collecting, and computing these
proxy variables - which are, in essence, modeling choices - there is a risk of introducing bias into the
model. Suresh and Guttag [57] further argue, that measurement bias arises through diÔ¨Äerences across
107G QUALITATIVE THEORY OF BIAS IN MACHINE LEARNING
groups in measurement process andquality of data , and Ô¨Ånally through oversimpliÔ¨Åcations in deÔ¨Åning
model tasks.
In the process of choosing variables, bias can occur if the choice is inherently associated with noise
that is dependent on other variables - for instance, group aÔ¨Éliation.
In the process of collecting variables, the measurement process can lead to bias.Suresh and Guttag [57]
explain how there is a diÔ¨Äerent mapping from crime to arrest in minority communities since they are
often more highly policed, which leads to a higher arrest rate. They argue, that this diÔ¨Äerence in the
measurement process led to higher false-positive rates for African American vs. White American defen-
dants in the case of the COMPAS algorithm. Bias can also arise due to diÔ¨Äerences in data quality , where
the data quality is dependent on group or individual characteristics.
Finally, measurement bias can arise in the process of computing . This could occur through oversimpliÔ¨Å-
cationsduring model development, where modeling choices lead to changes in model performances that
are dependent on group aÔ¨Éliation, for example, better performance for one group and lower performance
for another group.
Evaluation bias
Evaluation bias can arise from using benchmarking/evaluation datasets that are not representative of
the intended target population of a model. In the Ô¨Åeld of data science, models are often compared to one
another based on their performance on a commonly known and easily accessible benchmarking dataset,
for example, UCI Adult or ImageNet. If these datasets are misrepresentative, benchmarking encourages
the development of models that only perform well on a subset of the population. The more popular and
widely used a benchmarking dataset becomes, the more severe the issue of evaluation bias will become.
To ease the ranking of models, single measure aggregate performance metrics, such as accuracy, are
typically used [57].
Aggregation bias
Suresh and Guttag [57] describe aggregation bias as bias occurring when using a one-size-Ô¨Åts-all model
on groups with diÔ¨Äerent conditional distributions. In other words, where relationships between features
and labels of interest have diÔ¨Äerent group-dependent dynamics. As an example, Suresh and Guttag [57]
explains how dynamics regarding the measurement of blood sugar to diagnose and monitor diabetes are
known to diÔ¨Äer considerably in complex ways across ethnicities and genders. Using a one-size-Ô¨Åts-all
approach for modeling can lead to either under-performance for all groups or a model that is Ô¨Åtted
to the dominant group. In this sense, aggregation bias is highly intertwined with representation bias.
Although, even without representation bias, aggregation bias can still lead to harmful outcomes.
Deployment bias
Deployment bias relates to a mismatch between what the model is intended to solve and how the model
is actually used. The bias often occurs when a model built for fully autonomous tasks becomes part of a
socio-technical system inÔ¨Çuenced by human decision-makers. This inÔ¨Çuence can be seen when inappro-
priate interpretation or usage of model predictions arise, for example, when human decision-makers use
algorithms for oÔ¨Ä-label purpose - i.e., another purpose than originally intended. [57].
Relevance to the thesis
The six types of bias identiÔ¨Åed in the machine learning pipeline could be a useful framework to structure
critical reÔ¨Çection regarding the entire process of the AIR project. Suresh and Guttag show how bias can
arise from many diÔ¨Äerent sources and in many diÔ¨Äerent ways and that not all of these are quantiÔ¨Åable.
This is the case for historical bias, where the data itself reÔ¨Çects an unwanted bias. Addressing the bias
types listed in the paper [57] could be fruitful to think about bias in a more holistic way than it is possible
with just statistical approaches.
Traps in fair-ML
Selbst et al. [59] outline Ô¨Åve "traps" regarding fairness that one can fall into when working with ma-
chine learning projects. The paper argues that fairness is a property of social and legal systems, like
employment and criminal justice, and fairness is not a property of the technical tools. To treat fairness
the paper identiÔ¨Åes the following Ô¨Åve traps:
108G QUALITATIVE THEORY OF BIAS IN MACHINE LEARNING
‚Ä¢Framing trap
‚Ä¢Portability trap
‚Ä¢Formalism trap
‚Ä¢Ripple eÔ¨Äect trap
‚Ä¢Solutionism trap
These are traps that result from failing to properly account for or understand the interactions between
technical systems and the social world. In the following, there is a brief overview of the traps.
The framing trap
The most common abstraction of machine learning consists of choosing representations (data) and la-
beling (of outcomes). The choices made constitute the algorithmic frame . In this frame, the eÔ¨Écacy of a
model is evaluated based on the relation between the input and the output. The authors state that in the
algorithmic frame, notions of fairness cannot be deÔ¨Åned. Therefore, to investigate fair machine learning,
it requires one to also encompass the data frame - which are the input and output of an algorithm - and
how do these aÔ¨Äect the model. Finally, the socio-technical frame recognizes that the model is a part of a
socio-technical system. Selbst. et al. distinguish between the output of the ML model and the output of
the socio-technical system the ML model is a part of. It is therefore important also to assess the human
outcomes (decisions) for determining fairness [59].
The framing trap illustrates how projects can fail to model the entire system that covers all relevant
aspects of a particular phenomenon of interest. More speciÔ¨Åcally, when modeling any given subject, re-
searchers will use a frame to capture and measure relevant aspects and deÔ¨Åne the scope of the problem.
When deÔ¨Åning the scope and limits of the model, some relevant aspects could be left out. Depending on
the speciÔ¨Åc aspects left out, failure to model the entire system might lead to bias or inadequate evaluation
of fairness.
The portability trap
This trap emphasizes how using an algorithm designed for one social context maybe be misleading or
inaccurate when applied in another context. A reason to fall into this trap is that computer science
culture prizes and often demands portability. Transferable code, purposefully designed to be as abstract
as possible, is considered more useful (because it is reusable). Portability allows the same "solution"
(for example, a better algorithm for binary classiÔ¨Åcation) to apply to problems in various social settings.
Certain assumptions will hold in some social contexts but not others. The assumptions should reÔ¨Çect
the anticipated application. [59]
The formalism trap
The formalism trap emphasizes that machine learning projects can fail to account for the whole meaning
of fairness - therefore, this concept cannot be solved through mathematical formalisms. Since algorithms
"speak math", there has been a tendency to mathematically deÔ¨Åne aspects or notions of fairness in
society to incorporate fairness ideals into machine learning. DeÔ¨Ånitions in the literature of mathemati-
cal fairness are simpliÔ¨Åcations that cannot capture the full range of similar and overlapping notions of
fairness and discrimination in philosophical, legal, and sociological contexts. The paper refers to legal
scholar and philosopher Deborah Hellman and states that ‚Äúwe make distinctions all the time, but only
cultural context can determine when the basis for discrimination is morally wrong‚Äù [59].
The ripple eÔ¨Äect trap
If machine learning projects do not understand how the new technology changes the behavior of a pre-
existing system, the projects end up in the ripple eÔ¨Äect trap. Implementation of technology in a social
context both has intended and unintended consequences. Unintended consequences could be how people
or organizations in a system will respond to the new technology. For example, when using a risk assess-
ment machine learning tool that produces a score for the probability of a criminal to do more crime: How
does it aÔ¨Äect the judge? Will the judge suddenly only use the score and neglect other factors? When
seeing the scores frequently, could that change how the scores are interpreted through time? [59]
109G QUALITATIVE THEORY OF BIAS IN MACHINE LEARNING
The solutionism trap
In the best case, machine learning developers can improve a model, which encompasses more and more
social context, so the model can approximate the social environment. However, as [59] states, by starting
from technology and working outwards, there is never an opportunity to evaluate whether the technology
should be built in the Ô¨Årst place [59]. The solutionism trap happens when researchers or practitioners
fail to recognize that a solution to solve a problem may not involve technology.
Selbst et al. [59] state two broads situations where the solutionism trap appears: 1) when fairness deÔ¨Åni-
tions are politically contested and 2) when modeling is too complex to be computationally tractable. In
the area of predicting politics, the authors emphasize how diÔ¨Écult a task it is since human preferences
are not rational and human psychology is not conclusively measurable.
Addressing the traps
The main takeaway from the paper is the process of determining and applying technical solutions. The
main take-aways will here be represented as Ô¨Åve questions that could be asked when building fair ML
solutions [59]:
‚Ä¢Solutionism trap: Is the technical solution appropriate in the Ô¨Årst place?
‚Ä¢Ripple eÔ¨Äect trap: Does the technical solution aÔ¨Äect the social context in a predictable way such
that the social context, that is intended to stay unchanged, remains unchanged after the introduc-
tion of the solution?
‚Ä¢Formalism trap: Can the technical solution appropriately handle robust understandings of social
requirements such as fairness?
‚Ä¢Portability trap: Has the technical solution appropriately modeled the social and technical require-
ments of the actual context in which it will be deployed?
‚Ä¢Framing trap: Is the technical solution heterogeneously framed so as to include the data and social
actors relevant to the localized questions of fairness?
Relevance to the thesis
The traps provide a list of typical mistakes researchers and practitioners make when implementing
machine learning models or evaluating bias and fairness in relation to these models. One could easily
imagine that the AIR project is challenged in similar ways, and the traps described in [59] therefore
enables our evaluation of the AIR project to focus on aspects of the implementation that are more likely
to be problematic in relation to bias and fairness.
Algorithm hygiene
Lee et al. [62] suggest a framework for algorithm hygiene - which identiÔ¨Åes some speciÔ¨Åc causes of biases
and employs best practices to identify and mitigate them. The paper draws upon the insight of 40
thought leaders from across academic disciplines, industry sectors, and civil society organizations who
participated in one of two roundtables.
The paper focuses on two causes of bias: historical human bias and incomplete/unrepresentative data.
Overall, the paper emphasizes that pervasive and often deeply embedded prejudices shape historical
human biases against certain groups, which can lead to their reproduction in computer models. Incom-
plete/unrepresentative data causes algorithmic bias, for example, if the training data over-represents a
group, the model may systematically predict worse for the under-represented group(s). [62]
For mitigating bias, the paper proposes that operators of algorithms must develop a bias impact state-
ment. The impact statement can help probe and avert any potential biases that are baked into or are
resultant from the algorithmic decision. In order to develop the statement, the paper oÔ¨Äers a template
of questions. These questions can guide, for example, developers through the design, implementation,
and monitoring phases.
They propose that operators apply the bias impact statement to assess the algorithm‚Äôs purpose, process,
and production, where appropriate. The proposed bias impact statement starts with a framework that
identiÔ¨Åes which automated decisions should be subjected to, such as scrutiny, operator incentives, and
110G QUALITATIVE THEORY OF BIAS IN MACHINE LEARNING
stakeholder engagement. Then, the user incentives should be addressed. Finally, the stakeholders should
be engaged.
This forms three elements or questions:
‚Ä¢Which automated decisions?
‚Ä¢What are the user incentives?
‚Ä¢How are stakeholders engaged?
The elements are reÔ¨Çected in a set of in-depth questions that operators should answer during the design
phase to Ô¨Ålter out potential biases - it is called the bias impact statement (see table 36).
In a Ô¨Ånal remark about the paper, the roundtable participants emphasized the importance of cross-
functional and interdisciplinary teams to create and implement the bias impact statement. Operators of
algorithms should seek to have a diverse workforce team. Employing diversity in the design of algorithms
upfront will trigger and potentially avoid harmful discriminatory eÔ¨Äects on certain protected groups. [62]
What will the automated decision do?
Who is the audience for the algorithm and who will be most aÔ¨Äected by it?
Do we have training data to make the correct predictions about the decision?
Is the training data suÔ¨Éciently diverse and reliable? What is the data lifecycle of the algorithm?
Which groups are we worried about when it comes to training data errors, disparate treatment,
and impact?
How will potential bias be detected?
How and when will the algorithm be tested? Who will be the targets for testing?
What will be the threshold for measuring and correcting for bias in the algorithm, especially as it
relates to protected groups?
What are the operator incentives?
What will we gain in the development of the algorithm?
What are the potential bad outcomes and how will we know?
How open (e.g., in code or intent) will we make the design process of the algorithm to internal
partners, clients, and customers?
What intervention will be taken if we predict that there might be bad outcomes associated with
the development or deployment of the algorithm?
How are other stakeholders being engaged?
What‚Äôs the feedback loop for the algorithm for developers, internal partners and customers?
Is there a role for civil society organizations in the design of the algorithm?
Has diversity been considered in the design and execution?
WillthealgorithmhaveimplicationsforculturalgroupsandplayoutdiÔ¨Äerentlyinculturalcontexts
Is the design team representative enough to capture these nuances and predict the application of
the algorithm within diÔ¨Äerent cultural contexts?
If not, what steps are being taken to make these scenarios more salient and understandable to
designers?
Given the algorithm‚Äôs purpose, is the training data suÔ¨Éciently diverse?
Are there statutory guardrails that companies should be reviewing
to ensure that the algorithm is both legal and ethical?
Table 36: Design questions template for bias impact statement from [62].
Relevance to the thesis
In terms of sources of bias, the paper by Lee et al. [62] resembles the notions of historical bias and
representation bias from the Suresh & Guttag paper [57]. Lee et al., however, also propose a bias impact
statement that speciÔ¨Åcally targets sources of bias that practitioners can address and take action on before
creating algorithms to make automated decisions, or in the AIR case, be used for decision support. In
the AIR case, the questions in the bias impact statement can be used as a guide for questions that lead
to actionable insights, for example, if an analysis of the incentives of users, for example, the caseworkers,
presents reasons to change certain aspects of the algorithm in relation to bias.
111G QUALITATIVE THEORY OF BIAS IN MACHINE LEARNING
The trouble with bias
In 2017 Kate Crawford spoke at the NeurlIPS conference about "Trouble in bias" [61]. Crawford dis-
tinguishes between two types of harms related to bias: harms of allocation andharms of representation .
These harms have the following characteristics:
Allocation Representation
Immediate Long term
Easily quantiÔ¨Åable DiÔ¨Écult to formalize
Discrete DiÔ¨Äuse
Transactional Cultural
Table 37: Characteristics of allocation and representation [61]
Allocation can be characterised as being immediate - since it is a time-bound moment of decision mak-
ing. Furthermore, it is often quantiÔ¨Åable. This raises questions about fairness and justice in discrete
and speciÔ¨Åc transactions. Representation is a long-term process that aÔ¨Äects attitudes and beliefs. Thus,
it can be more diÔ¨Écult to formalize. Representation is about a more diÔ¨Äuse depiction of humans and
society - thus, representation is cultural.
Harms of allocation
Harms of allocation is when a system allocates or withholds certain groups an opportunity or resource.
These types of harms are often economically oriented and centered on harms related to decisions regard-
ing, for example, who gets loans or gets insurance.
Harms of representation
Harms of representation refer to machine learning as being harmful regarding the representation of hu-
man identity - not only a system for contributing to decision making. The harms occur when systems
reinforce the subordination of some groups along the line of identities such as race and gender. This sort
of harm can occur regardless of whether resources are being withheld from members of a protected class.
Relevance to the thesis
To better understand the harms of potential bias in the AIR project, Crawfords distinctions can help
assess whether an ML algorithm is a part of a decision-making process where the public authority, for
example, has to allocate resources. If so, the characteristics of allocation harm can help uncover, for
example, the time perspective of the harms. Also, by understanding how allocation harm are related to
speciÔ¨Åc transactions, it will help question how fairness relates to the transactions - which in the AIR case
are whether a citizen should be provided fall-training. The characteristics of harms of representation
could be beneÔ¨Åcial in a critical reÔ¨Çection of how the AIR machine learning model and related data can,
in the long term, for example, reinforce stereotypes.
Crawford‚Äôs [61] two types of harms can be associated with some of the elements of Suresh and Guttag‚Äôs
framework [57]. For example, the harms of allocation can be related to evaluation bias since the fairness
of resource allocation can be viewed diÔ¨Äerently depending on the evaluation measure. Representational
harm can, for example, be related to representation bias, since under-representation in the development
population can lead to low performance of a group of subjects - for example, if facial recognition under-
performs and do denigration harm towards the under-represented group (as seen with the darker females
in section 2). Harms related to algorithms, like the two types Crawford addresses, are also examined by
Lee et al. [62] since the bias impact statement also helps outline, for example, who can be harmed by
the system and which potential bad outcomes could occur.
Data Statements for Natural Language Processing
Bender & Friedman [14] propose data statements as a way of bringing about change in the Ô¨Åeld of nat-
ural language processing (NLP) towards more ethically responsible use of NLP technology. Bender &
Friedman have designed the data statement framework explicitly as a tool for mitigating bias in systems
that use language data for training and testing.
112G QUALITATIVE THEORY OF BIAS IN MACHINE LEARNING
Bender & Friedmans proposed data statement framework consists of two parts: a long-form schema
and a short form. The long-form statement consists of several categories with relevant information that
should be created for any new dataset containing natural language data. In contrast, the short form
statement should be included in any publication using a dataset. The short-form statement is envisioned
as a 60-100 word summary of the long-form statement. The long-form statement consists of the following
elements:
‚Ä¢Curation rationale: Which texts were included, and what was the goal of obtaining them?
‚Ä¢Language variety: Standardised language tags, along with prose descriptions of relevant varieties,
for example, English as spoken in Palo Alto, California.
‚Ä¢Speaker demographic: Speaker is deÔ¨Åned as the source of the natural language data and should
include information about their age, gender, race/ethnicity, native language, socioeconomic status,
number of speakers, and presence of disordered speech.
‚Ä¢Annotator demographic: Annotators are the people who have marked the data with relevant
tags/labels,andinformationabouttheirage,gender,race/ethnicity,nativelanguage,socioeconomic
status, and training in linguistics should be included.
‚Ä¢Speech situation: This includes time/space, modality (spoken, written), edited or spontaneous,
synchronous or asynchronous interaction, intended audience.
‚Ä¢Text characteristics: Genre and topics.
‚Ä¢Recording quality: If audiovisual recordings are present, the quality can be commented.
These elements are essential when considering whether the data used is appropriate for NLP work being
undertaken and to ensure that the reported results are properly contextualised [14].
Relevance to the thesis
In the context of this master thesis, that does not have en exclusive focus on NLP, the underlying
notions behind the data statement framework are still relevant. Here, a data statement that describes
general characteristics in categories inspired by the speciÔ¨Åc NLP-related categories of the Bender &
Friedman paper could be a way to explicitly describe data set characteristics that could lead to bias
when using algorithms trained or tested on speciÔ¨Åc data sets. Since the paper presents a set of elements
that practitioners should consider when thinking about bias, there are similarities with the bias impact
statement shown in [62] section G, although the Bender & Friedman data statement focuses more on the
characteristics on the data and the curation process of the data set, rather than the general context of
the algorithm.
Bias in Machine Learning - What is it Good for?
In [15] Hellstr√∂m et al. survey the current literature on bias in machine learning, and analyse and discuss
how diÔ¨Äerent types of bias are connected and depend on each other. Figure 51 is an illustration of the
diÔ¨Äerent categories in the taxonomy and where they are placed in the machine learning pipeline.
113G QUALITATIVE THEORY OF BIAS IN MACHINE LEARNING
Figure 51: Taxonomy of bias in machine learning in [15]
Hellstr√∂m et al. underline a distinction between model bias and the rest of the bias types from Ô¨Ågure
51. While all other bias types describe the sources of potential bias in machine learning systems, model
bias refers to the detected bias in the outcome of these systems and how they are deÔ¨Åned (for example,
false-positive rates or group fairness). The four categories: biased world ,data generation ,learning and
evaluation , are brieÔ¨Çy covered in the following:
‚Ä¢A biased world: Refers to sources of bias that come from the world as it is/was , meaning that
these biases would be propagated into the model outcome even without biased procedures from
data generation or learning. This historical bias comes from human actions and historical outcomes
that manifest themselves in observations that have unwanted properties and can be expected to
be learned by any machine learning model since most machine learning techniques mimic the
observations of the world.
‚Ä¢Data generation: Refers to acquiring and processing observation and turning it into data for
a machine learning model. Here bias can occur from, for example, choosing input and output
variables of the learning task, measurement errors, or sampling.
‚Ä¢Learning: Bias in the learning step comes from choices regarding how the model learns and outputs,
forexamplethroughtestingalimitedsetofhyper-parametersorbysettingthresholdsforacceptable
levels of uncertainty.
‚Ä¢Evaluation: Referstomodelbiasdetectedintheoutcomeofthemachinelearningsystem. However,
this bias can be deÔ¨Åned in many diÔ¨Äerent ways, many of which are related and some of which are
conÔ¨Çicting and cannot be avoided simultaneously. In most assessments of model bias, the bias
metrics are used to assess diÔ¨Äerences in classiÔ¨Åcation between protected attributes, such as gender,
race, ethnicity, religion, etc.
Hellstr√∂m et al. [15] note that to be able to assess the true extent of the bias for any machine learning
system, one must deÔ¨Åne a notion of how the world should be , as opposed to how the world is or was.
This notion is, as an example, built into the group fairness view on the bias, where the model output
should be independent of a protected attribute. In other views, for example, individual fairness, where
the model‚Äôs outputs for two similar individuals should be the same, there is a diÔ¨Äerent notion of how the
world should be. These diÔ¨Äerence are at the heart of many discussion regarding bias and fairness, for
example regarding the COMPAS algorithm as presented earlier in 2.
Relevance to the thesis
Overall, Hellstr√∂m makes a distinction between the term "model bias", which relates to the outcome of
114G QUALITATIVE THEORY OF BIAS IN MACHINE LEARNING
the system, and all other types of bias. The division of the machine learning pipeline can, to some extent,
be related to Suresh and Guttag [57] framework. "A biased world" relates to "historical bias," which
together represent how framing and history inÔ¨Çuence (bias) the input data. "Data generation" relates
to "representation bias" / "measurement bias" and focuses on the data sampling and collection process.
The "learning" category is related to "aggregation bias" - both categories assess how the model is de-
Ô¨Åned or optimized. Finally, Hellstr√∂m‚Äôs "evaluation" category relates to Suresh and Guttag‚Äôs "evaluation
bias", which both assess that bias occurs in evaluating the model performance and the associated metrics.
Furthermore, Hellstr√∂m‚Äôs "learning" category can help put focus on the role of the ML developer. When
building models that reproduce the COMPAS algorithm bias, diÔ¨Äerent hyperparameters have been cho-
sen. Investigating or at least discussing what inÔ¨Çuences the learning category‚Äôs bias could have on an ML
project (or, for example, on the model bias) could be interesting to examine. Also, Hellstr√∂m emphasizes
that in order to assess bias, one must deÔ¨Åne how the world should be.
115H THE COMPAS ALGORITHM
H The COMPAS algorithm
Method considerations
The COMPAS dataset is included in the thesis since the COMPAS case is often used in the machine
learning literature as an example of an algorithm where the classiÔ¨Åcations exhibit racial bias. Since there
is a well-documented expectation of Ô¨Ånding bias in the COMPAS case [7], it is a meaningful data set to
develop and implement methods for bias identiÔ¨Åcation and mitigation. Furthermore, the COMPAS case
is similar to the AIR case in the following important ways.
The COMPAS algorithm is used to predict whether a defendant re-oÔ¨Äends or not - therefore, it is a
binary classiÔ¨Åcation problem. This corresponds to the AIR case which is also a binary classiÔ¨Åcation
problem, predicting if a citizens will fall or not. Furthermore, the COMPAS algorithm is used by judges
in the US, while the AIR algorithm is anticipated to be used by caseworkers in Aalborg Municipality
[2], making them both decision support tools used in the public sector. Finally, the two cases are similar
in the sense that they are used to assess if a citizen should be allocated or withheld an opportunity or
resource, respectively pretrial release and fall-prevention training.
Reproducing the COMPAS bias
The COMPAS algorithm is closed-sourced [7], and therefore, it is not possible to test bias identiÔ¨Åcation
and mitigation techniques on the COMPAS algorithm itself. Because of this, we implement machine
learning models that attempt to reproduce the COMPAS algorithm‚Äôs bias, on which we can test the
bias identiÔ¨Åcation and mitigation techniques. If the techniques mitigate bias in our implementations, it
cannot be guaranteed that it will also mitigate bias for the COMPAS algorithm, but it can nonetheless
be used as an experimental result that strengthens the relevance and application of the techniques.
We have chosen to built the same four machine learning models as we intend to built on the AIR data
set. See section 8.3 for details of why we choose the models.
Background
The COMPAS algorithm is a commercial software tool for decision support used in the United States
justice system, for example, in the State of New York and the State of California, among others [63].
The software assesses a criminal defendant‚Äôs likelihood of becoming a recidivist - which is a term that
describes whether a criminal re-oÔ¨Äends [36]. Judges use the COMPAS score as a decision support tool
when determining pretrial release, and sentencing [6]. In 2016 ProPublica published the article "Machine
Bias", where the authors investigated racial disparities in predictions of the COMPAS software [7].
ProPublica analysed data from more than 7000 criminal defendants in Broward County, Florida and
compared the predicted recidivism scores with the actual recidivism rates over a two-year period, the
main Ô¨Åndings were the following:
‚Ä¢African-American defendants were often predicted to be at a higher risk of recidivism than they
actually were (nearly twice as likely compared to Caucasian defendants)
‚Ä¢Caucasian defendants were often predicted to be less risky than they were (nearly twice as often
as African-American defendants)
The analysis showed that the algorithm correctly predicted an oÔ¨Äender‚Äôs recidivism 61 % of the time [7].
COMPAS data overview
TheCOMPASdatasethas7214recordsofdefendants‚ÄôCOMPASscoresandcriminalhistory. Thedataset
both contains information about defendants‚Äô criminal history before and after they got a COMPAS
score. The original dataset has 52 columns, including information about the charge that resulted in
the COMPASS scoring and the charge committed after (if any). The charges (before and after) had
associated data describing the date of the crime, the date when the defendant was jailed and got out.
116H THE COMPAS ALGORITHM
Figure 52: COMPAS scores and risk labels. Illustration by Daniel Juh√°sz Vigild and Lau Johansson
Each pretrial defendant received a COMPAS scores: ‚ÄúRisk of Recidivism‚Äù. As illustrated in Figure 52
COMPAS scores for each defendant ranged from 1 to 10, with ten being the highest risk. Scores 1 to
4 were labeled by COMPAS as ‚ÄúLow‚Äù; 5 to 7 were labeled ‚ÄúMedium‚Äù; and 8 to 10 were labeled ‚ÄúHigh‚Äù
[36]. As shown in the illustration, "Low" lead to the prediction "Will not reoÔ¨Äend" while "Medium" and
"High" leads to the prediction "Will reoÔ¨Äend".
The confusion matrix tables below show the classiÔ¨Åcations made by the COMPAS algorithm and the
recidivism ground truth, both in absolute and relative numbers:
True Class
Re-oÔ¨Äend Not re-oÔ¨Äend
Re-oÔ¨Äend 2035 (28.21%) 1282 (17.77%)COMPAS predictionsNot re-oÔ¨Äend 1216 (16.86%) 2681 (37.16%)
Table 38: The Confusion matrix for both races by Probublica [36], percentages sum to 100 pct. and
represent total population.
True Class
Re-oÔ¨Äend Not re-oÔ¨Äend
Re-oÔ¨Äend 1369 (37.04%) 805 (21.78%)COMPAS predictionsNot re-oÔ¨Äend 532 (14.39%) 990 (26.79%)
Table 39: The Confusion matrix for African-American by Probublica [36], percentages sum to 100 pct.
and represent total African-American population.
True Class
Re-oÔ¨Äend Not re-oÔ¨Äend
Re-oÔ¨Äend 505 (20.58%) 349 (14.22%)COMPAS predictionsNot re-oÔ¨Äend 461 (18.79%) 1139 (46.41%)
Table 40: The Confusion matrix for Caucasian by Probublica [36], percentages sum to 100 pct. and
represent total Caucasian population.
As can be seen in the tables, the classiÔ¨Åcations of the COMPAS algorithm have more false positives for
African-Americans (21.78%) than for Caucasians (14.22%), and that there are more false negatives for
Caucasians (18.79%) than for African-Americans (14.39%). This shows a racially dependent bias.
The data can be found at ProPublica‚Äôs github [64].
117I COMPAS - DESCRIPTIVE ANALYSIS
I COMPAS - Descriptive Analysis
In the following section, an initial descriptive data analysis is performed on the COMPAS data set to
better understand the data used for building the COMPAS algorithm and the models that should repro-
duce the COMPAS algorithm.
ProPublica tested racial disparities by creating a logistic regression model. This model considered race,
age, criminal history, future recidivism, charge degree, gender, and age. [36]. The descriptions are known
before the scoring-process, and could potentially improve the performance of the model - therefore, the
charge descriptions are also included in the following initial descriptive data analysis.
The following columns are included:
Variable Description Type
sex The sex of the defendant: female or male String
age The age of the defendant Integer
age_cat Three age categories (intervals) String
race Five diÔ¨Äerent race categories and a category
"other"String
juv_fel_count The number of juvenile felonies String
juv_misd_count The number of juvenile misdemeanors Integer
juv_other_count The number of prior juvenile convictions that
are not counted in the two variables aboveInteger
priors_count The number of prior crimes comitted Integer
c_charge_desc A short (one line) description of the charge String
c_charge_degree Degree of the charge (felony or misdemeanor) String
decile_score The decile of the COMPAS score Integer
is_recid 1, if the defendant did any criminal oÔ¨Äense
that resulted in a jail booking after the crime
for which the person was COMPAS scored,
else 0Integer / Binary
Table 41: Description of selected COMPAS data used for classiÔ¨Åcation
Sex
A defendant is categorized as either a female or male. Most of the defendants are males which constitues
approx. 80% of the defendants in the dataset, where females constitutes the remaining 20%. This means
that the majority of the defendants are males.
count percentage
sex
Female 1395 19.34
Male 5819 80.66
Table 42: Count of records grouped by sex
Race
There are Ô¨Åve unique race-categories and a category called "other" for those who were not in the Ô¨Åve
categories. This makes up six categories: ‚ÄôAfrican-American‚Äô, ‚ÄôAsian‚Äô, ‚ÄôCaucasian‚Äô, ‚ÄôHispanic‚Äô, ‚ÄôNative
American‚Äô and ‚ÄôOther‚Äô. The number of records grouped by race are:
118I COMPAS - DESCRIPTIVE ANALYSIS
count percentage
race
African-American 3696 51.23
Caucasian 2454 34.02
Hispanic 637 8.83
Other 377 5.23
Asian 32 0.44
Native American 18 0.25
Table 43: Count of records grouped by race
From table 43 the majority of the defendants are "African-American", 51%, and the subsequently largest
category is "Caucasian" with 34%. Both "Hispanic" and "Other" represent respectively under 10% of
the data set. "Asian" and "Native American" both represent under 1% respectively. This means that
African-Americans are over-represented compared to the other races.
Looking across sex and race in table 44 the Ô¨Åve most represented groups are male African-Americans
(42%), male Caucasians ( 26%), female African-Americans ( 9%), female Caucasians ( 8%) and
male Hispanics (7%).
count percentage
sex race
Female African-American 652 9.04
Asian 2 0.03
Caucasian 567 7.86
Hispanic 103 1.43
Native American 4 0.06
Other 67 0.93
Male African-American 3044 42.20
Asian 30 0.42
Caucasian 1887 26.16
Hispanic 534 7.40
Native American 14 0.19
Other 310 4.30
Table 44: Count of records grouped by sex and race
Age and age categories
Figure 53 shows a right-skewed age distribution implying that the defendants tend to younger. The
same conclusion is made when investigating the age distribution across both sex and race. The youngest
defendant is 18, and the oldest is 96.
119I COMPAS - DESCRIPTIVE ANALYSIS
Figure 53: Histogram of the defendants‚Äô ages
The histogram in Ô¨Ågure 53 is reÔ¨Çected in table 45 where almost 57 % of the defendants are between 25
and 45 years old.
count percentage
age_cat
Less than 25 1529 21.19
25 - 45 4109 56.96
Greater than 45 1576 21.85
Table 45: Count of records grouped by age category
One should keep in mind, that the ages of the defendants range from 18 to 96, also, the interval sizes in
age_cat are not consistent. The category "less than 25" spans over seven years, "25-45" spans over 20
years and "greater than 45" spans over 50 years. If the categories‚Äô span were divided into equal large
sizes, as shown in Ô¨Ågure 46, the results implies that most of the defendants are under 45 years old.
count percentage
Updated age categories
Less than 45 5527 76.61
45-70 1652 22.90
Greater than 70 35 0.49
Table 46: New age categories where the age range are divided into three equal sized intervals
120I COMPAS - DESCRIPTIVE ANALYSIS
Figure 54: Histogram of the defendants‚Äô ages grouped by race
Juvenile convictions and prior crimes
97%ofthedefendantshavedonenojuvenilefelonies. Oftheremaining3%, 282defendants, 189havedone
a single juvenile crime. The defendant with the most juvenile felonies has committed 20 felonies. 95% of
the defendants have no juvenile misdemeanors, and the defendants with the most juvenile misdemeanors
have 13. 92% of the defendants have no other juvenile convictions, and the defendant with the highest
numberofotherconvictionsiscounting17. 86.51%ofthedefendantshavenojuvenilefelonies, nojuvenile
misdemeanors, and no other juvenile convictions.
Figure 55: Boxplot of juvenile conviction variables and prior crimes.
Figure 55 shows that 50% of the defandants have less than 2 prior crimes. 25% have done 5 prior crimes
121I COMPAS - DESCRIPTIVE ANALYSIS
or more. The defendant with most prior crimes counts 38 crimes.
Charge degree and description
The charge degree is either "F" (felony) or "M" (misdemeanor). A misdemeanor is a less serious crime
than a felony. Felonies are the most serious crimes one can commit, and they are associated with long
jail or prison sentences. Misdemeanors often involve jail time, smaller Ô¨Ånes and temporary punishments
[65]. 4666 (65%) of the records are felonies, and 2548 ( 35%) are misdemeanors.
There are 437 unique charge descriptions each description is between one and eight words long. 50% of
the descriptions are only used once. 16 ( 4%) of the descriptions are each used for 1% of the records or
more. The top Ô¨Åve most frequent charge descriptions are shown in table 47 and the respective fraction
(in percentage) of the total number of records in the data set.
count percentage
c_charge_desc
Battery 1156 16.09
arrest case no charge 1137 15.82
Possession of Cocaine 474 6.60
Grand Theft in the 3rd Degree 425 5.92
Driving While License Revoked 200 2.78
Table 47: Top Ô¨Åve most frequent charge description types.
Recidivism and decile scores
In Ô¨Ågure 56 a histogram of the binary recidivism variable is plotted. 3743 (51.89%) defendants in the
data set did not re-oÔ¨Äend, and 3471 (48.11%) did re-oÔ¨Äend.
Figure 56: Histogram of recidivism
COMPAS decile scores are risk scores (integer type) ranged between 1 and 10, where 1 is the lowest risk
score, and 10 is the highest risk score. 1-4 is labeled as "LOW", 5-7 as "MEDIUM" and 8-10 as "HIGH"
[36]. Figure 57 shows a right-skewed distribution of COMPASS scores across all records in the data set.
122I COMPAS - DESCRIPTIVE ANALYSIS
Figure 57: Histogram of decile COMPAS scores
ProPublica deÔ¨Ånes a binary recidivism variable as 0 if the decile score is under 5, else 1. Each of the
COMPAS binary recidivism scores is shown as a histogram in Ô¨Ågure 58, where the actual is_recid count
is visualized as the red horizontal lines.
Figure 58: Histogram of predicted recidivism converted from COMPAS scores. The red lines show the
counts from Ô¨Ågure 56
Figure 58 shows a distribution that visually mimic the actual recidivism distribution from Ô¨Ågure 56. The
COMPAS predictions increase the distance between the two bars, which means that more defendants
are predicted as not re-oÔ¨Äending, even though they are, and vice versa with defendants that do re-
oÔ¨Äend. Since the classiÔ¨Åcation problem is a supervised learning problem, one can better understand the
performance of the COMPAS algorithm by investigating the relation between the predicted class and
the true class. The elements of the confusion matrix are shown in table 48. Furthermore, the accuracy
of the COMPAS algorithm is 65.23%.
TPR (%) FPR (%) TNR (%) FNR (%)
All defendants 61.65 31.45 68.55 38.35
Table 48: TPR, FPR, TNR and FNR for COMPAS predictions of the actual recidivism
Table 48 shows that 61.65% of the times that the algorithm predicts a criminal re-oÔ¨Äending - he or she
actual re-oÔ¨Äend. It also shows that the algorithms perform better in predicting citizens that do not re-
oÔ¨Äend - 68.55% of the time that the algorithm predicts that a citizen does not re-oÔ¨Äend - the algorithm
was correct.
123I COMPAS - DESCRIPTIVE ANALYSIS
To investigate bias related to protected variables, the confusion matrix for both African-Americans and
Caucasians are speciÔ¨Åed in table 49.
Race TPR (%) FPR (%) TNR (%) FNR (%)
African-American 70.97 43.92 56.08 29.03
Caucasian 51.02 23.16 76.84 48.98
Table 49: TPR, FPR, TNR and FNR COMPAS predictions of the actual recidivism for respectively
African-American and Caucasian
What is interesting to notice is that their metric-pairs FPR/FNR and TPR/TNR are skewed between the
tworaces. TheaccuracyforAfrican-AmericanandCaucasiansarerespectively64.29%and66.05%. Ifthe
algorithm was only to be evaluated on accuracy as a performance measure, the algorithm performance
for each class compared to each other seems immediately equal. The diÔ¨Äerence in FPR between the
two race groups are shown in table 49. This relates to defendants that are predicted to re-oÔ¨Äend but
actually did not. The diÔ¨Äerence in FPR is approximately 20 pct. points. The diÔ¨Äerence in FNR is also
approximately 20 pct. points. This metric relates to defendants that are predicted to not re-oÔ¨Äend but
actually did.
Figure 59: FPR/FNR plot for African-American and Caucasian
Figure 59 visualize the relation between FPR and FNR. Notice that "Caucasian" is located closest to
the upper left corner of the triangle, which implies both lower FPR and higher FNR.
In Ô¨Ågure 60 the decile score histogram for "Caucasian" shows a right skewed distribution - which means,
that the distribution skews towards lower risk scores. The histogram for "African-American" shows
a more uniform distribution. The caucasian distribution shows an especially high number of decile
scores with the value 1. The appearance of the two groups‚Äô distributions is very diÔ¨Äerent. Why the
distributions form as they do can not be concluded based on the plots. However, potential reasons
could be that, for example, the COMPAS algorithm is biased against African-Americans or, for example,
African-Americans re-oÔ¨Äend more.
124I COMPAS - DESCRIPTIVE ANALYSIS
Figure 60: Decile score histogram for African-American and Caucasian.
Another potential protected variable in the COMPAS data set is the sex of the defendant. From ta-
ble 42 it is shown that the majority of the defendants are males. Under-representation of a group can
lead to performance issues, and since females are less represented than males, it would be interesting to
investigate if there are any performance diÔ¨Äerences between the groups. The accuracy are 65.38% and
65.20% for females and males respectively - which shows that the algorithms performance equally "well"
for both groups.
sex TPR FPR TNR FNR
Female 60.19 31.45 68.55 39.81
Male 61.92 31.45 68.55 38.08
Table 50: Performance of COMPAS predictions of the actual recidivism for respectively females and
males
Figure 61: FPR/FNR plot for females and males.
Table 50 shows that across four metrics of the confusion matrix lies very close to each other. No obvious
bias in terms of TPR, TNR, FPR and FNR. Also notice how close the two groups lie in Ô¨Ågure 61,
which implies that the COMPAS algortithm has almost no bias when comparing the scoring between
125I COMPAS - DESCRIPTIVE ANALYSIS
females and males. Yet, to better understand the distributions of the decile scores for each group the
distributions are shown in Ô¨Ågure 62.
Figure 62: Decile score histogram for males and females
Both distributions are right-skewed, which means that both groups skew towards lower risk scores. The
males have a high number of defendants with a decile score of 1, and the female distribution is a bit
more Ô¨Çat (uniform).
As identiÔ¨Åed from table 44 there are diÔ¨Äerences in how much each of the sex-race groups makes up
the total defendants in the data set. Since the amount of data for each sex-race group also could inÔ¨Çu-
ence the performance of the COMPAS algorithm, this is further investigated.
sex and race TPR FPR TNR FNR
Male - African-American 71.15 45.33 54.67 28.85
Female - African-American 69.81 39.28 60.72 30.19
Male - Caucasian 49.88 20.82 79.18 50.12
Female - Caucasian 55.50 30.17 69.83 44.50
Table 51: Performance metrics of the COMPASS algorithm grouped by sex and race
Table 51 shows that regardless of the gender, FPR is higher for African-Americans, and FNR is lower for
African-Americans - compared to Caucasians. In FPR, there are approximately ten percentage points
in diÔ¨Äerence between caucasian males and females, where Caucasian males have the lowest FPR of all of
the groups on 20.82 %. If one had only evaluated the COMPAS algorithm on accuracy, all four groups
accuracy lies within the range between 64% and 67%.
126I COMPAS - DESCRIPTIVE ANALYSIS
Figure 63: FPR/FNR plot grouped by sex and race
In Ô¨Ågure 63 the caucasian groups both have lower FPR and higher FNR than the African-American
groups. The Caucasian males are closest to the upper left corner of the triangle, which shows that this
group has the most defendants with the lowest FPR and highest FNR. This means that male Caucasians
are the group where defendants that do re-oÔ¨Äend are not jailed - also, the group where the fewest number
of defendants are jailed even though they will not recid.
127I COMPAS - DESCRIPTIVE ANALYSIS
Figure 64: Density histogram grouped by sex and race
In Ô¨Ågure 64 the density histograms show how both female and male Caucasians distributions are right-
skewed. For female African-Americans there is a right-skew in the range between 5-10 in decile score, but
it could also be argued to be a kind of bimodal with peaks at decile score 2 and 5 (and 1). The distribu-
tion for male African-Americans looks more uniform-like. For the groups with male African-Americans,
a defendant is equally likely to get a score in the range between 1 and 10.
.
128J COMPAS - REPRODUCTION
J COMPAS - Reproduction
Reproducing the COMPAS algorithm‚Äôs bias
TheCOMPASalgorithmisaclosedsourcecommercialalgorithm[7]. Therefore, assessinghowtechniques
to identify and/or mitigate bias will aÔ¨Äect the predictions is impossible since we do not have access to
the model itself. We, therefore, attempt to create a model that reproduces the biases that have been
highlighted as problematic in relation to the COMPAS algorithm. In their critique of the algorithm,
ProPublica shows how "blacks are almost twice as likely as whites to be labeled a higher risk but not
actually re-oÔ¨Äend" and how the opposite is true for predictions regarding white defendants, where "they
are much more likely than blacks to be labeled lower-risk but go on to commit other crimes" [7]. In
other words, the algorithm predicts more false positives when assessing African-American defendants,
and more false negatives when assessing Caucasian defendants.
By creating a model that reproduces the bias of the COMPAS algorithm, we wish to create a model
on which we can test bias identiÔ¨Åcation and mitigation techniques. The reasoning behind this is that if
we build a model that reproduces the bias of interest found in the COMPAS algorithm, then hopefully,
techniques that mitigate bias on our implementation might also work on the COMPAS algorithm.
To assess if our implementation reproduces the bias, we compare our model to the COMPAS algo-
rithm in terms of the following classiÔ¨Åcation measures: false positive rate (FPR), false-negative rate
(FNR). These measures are included in ProPublica‚Äôs investigation [7]. If our implementation resem-
bles the COMPAS algorithm in terms of the classiÔ¨Åcation measures presented above, we assess that we
sucessfully have reproduced the COMPAS bias.
Our implementations
To reproduce the COMPAS bias in a robust fashion, we attempt to reproduce the COMPAS algorithm
using four diÔ¨Äerent models: a logistic regression, a support vector machine, a random forest, and a
feed-forward neural network.
We use the models to predict recidivism among the defendants in the COMPAS dataset. As exogenous
variables we include the following:
‚Ä¢age of the defendant
‚Ä¢race of the defendant (transformed to one-hot-encodings)
‚Ä¢sex of the defendant (transformed to one-hot-encodings)
‚Ä¢category of the charge (which led to the COMPAS screening) (transformed to dummy variables)
‚Ä¢Charge description transformed to one-hot-encodings)
‚Ä¢count of juvenile felonies
‚Ä¢count of juvenile misdemeanours
‚Ä¢count of prior oÔ¨Äenses
Our implementations (that is: the logistic regression, the support vector machine, the random forest and
the feed forward neural network) are all trained on a subset of the COMPAS dataset (training set) and
tested on a testset. These results, along with the reported bias measures for the COMPAS algorithm
found in [7], can be seen in table 52.
Test results: bias measures
Bytrainingthemodelsonalldefendants, reportingrace-speciÔ¨Åcbiasmeasures, andcomparingtheresults
to the bias measures of the COMPAS algorithm, we test whether or not we have succeeded in creating
models that reproduce the bias of the COMPAS algorithm. The results are shown in Ô¨Ågure 65.
129J COMPAS - REPRODUCTION
Figure 65: Bias measures from diÔ¨Äerent models. A total overview of all bias measures for all models
can be found in the appendix K.
From the Ô¨Ågure it is clear that the models reproduce the most fundamental bias, that are found in the
COMPAS algorithm, namely the diÔ¨Äerences in FPR and FNR between African-American and Caucasian
defendants. Notice how each of the blue bars (African-American) in the TPR and FPR are much higher
than the green bars (Caucasians). Also notice, how the opposite holds for TNR and FNR - where the
green bars (Caucasians) are much higher than the blues (African-American). The COMPAS algorithm
has a higher false-positive rate and a lower false-negative rate for African-American defendants and a
lower false-positive rate and higher false-negative rate for Caucasian defendants. This central diÔ¨Äerence
is reproduced in all of our implementations. This means, in other words, that if a judge were to follow
the algorithmic recommendations, she could make two types of mistakes systematically: deny bail to
African-American defendants who will, in fact, not re-oÔ¨Äend and release Caucasian defendants who will
re-oÔ¨Äend. The estimates is found in appendix K table 52. All in all, we are satisÔ¨Åed with the results,
which exhibits the same race-speciÔ¨Åc bias as the COMPAS algorithm.
130K COMPAS - REPRODUCTION METRICS
K COMPAS - Reproduction metrics
Race TPR FPR TNR FNR
Support Vector Machine
African-American :70.9 44.8 55.2 29.1
(70.2-71.6) (44.1-45.6) (54.4-0.6) (28.4-29.8)
Caucasian : 46.9 24.9 75.14 53.1
(45.9-47.8) (24.0-25.7) (74.3-76.0) (52.2-54.1)
Logistic Regression
African-American :76.3 46.0 54.0 23.7
(75.8-76.8) (45.3-46.8) (53.2-0.5) (23.2-24.2)
Caucasian : 53.6 27.3 72.69 46.4
(52.6-54.5) (26.5-28.1) (71.9-73.5) (45.5-47.4)
Random Forest
African-American :69.5 42.4 57.6 30.5
(68.9-70.1) (41.6-43.1) (56.9-0.6) (29.9-31.1)
Caucasian : 52.3 29.3 70.65 47.7
(51.5-53.2) (28.6-30.1) (69.9-71.4) (46.8-48.5)
FFNN
African-American 67.0 38.5 61.5 33.0
(65.0-68.9) (36.1-40.8) (59.2-63.9) (31.1-35.0)
Caucasian 41.3 19.2 80.8 58.7
(38.8-43.7) (17.5-20.9) (79.1-82.5) (56.3-61.2)
Table 52: COMPAS classiÔ¨Åcation metrics of algorithms grouped by race
131L MODEL ARCHITECTURES AND HYPERPARAMETERS
L Model architectures and hyperparameters
XGBoost
The following optimizer and hyperparameters has been chosen for AIR XGBoost:
‚Ä¢n_estimators: 400
‚Ä¢objective: "binary:logistic"
‚Ä¢scale_pos_weight : neg / pos. Where "neg" is the count of "No Falls" and "pos" is the count of
"Falls"
‚Ä¢use_label_encoder: False
‚Ä¢learning_rate: 0.1
‚Ä¢eval_metric: "logloss"
Feed forward neural network
The following optimizer and hyperparameters has been chosen for AIR FFNN:
‚Ä¢Number of epochs: 400
‚Ä¢Number of nodes: 500
‚Ä¢Batch size: 40
‚Ä¢Optimizer: Adam
‚Ä¢Learning rate: 0.001
‚Ä¢Weight decay (L2-norm): 0.05
‚Ä¢Dropout rate: 0.4
The following optimizer and hyperparameters has been chosen for COMPAS FFNN:
‚Ä¢Number of epochs: 200
‚Ä¢Number of nodes: 2000
‚Ä¢Batch size: 40
‚Ä¢Optimizer: Adam
‚Ä¢Learning rate: 0.001
‚Ä¢Weight decay (L2-norm): 0.05
‚Ä¢Dropout rate: 0.5
The PyTorch model:
classNetwork(nn.Module):
def__init__( self ):
super(Network , self ).__init__()
self . fully_connected1 = nn. Sequential(
nn. Linear(n_feat ,n_nodes) ,
nn.ReLU() ,
nn.BatchNorm1d(n_nodes) ,
nn.Dropout(p_drop)
)
self . fully_connected2 = nn. Sequential(
132L MODEL ARCHITECTURES AND HYPERPARAMETERS
nn. Linear(n_nodes ,n_nodes) ,
nn.ReLU() ,
nn.BatchNorm1d(n_nodes) ,
nn.Dropout(p_drop)
)
self . fully_connected3 = nn. Sequential(
nn. Linear(n_nodes ,n_nodes) ,
nn.ReLU() ,
nn.BatchNorm1d(n_nodes) ,
nn.Dropout(p_drop)
)
self . fully_connected4 = nn. Sequential(
nn. Linear(n_nodes ,n_nodes) ,
nn.ReLU() ,
nn.BatchNorm1d(n_nodes) ,
nn.Dropout(p_drop)
)
self . fully_connected5 = nn. Sequential(
nn. Linear(n_nodes ,n_nodes) ,
nn.ReLU() ,
nn.BatchNorm1d(n_nodes) ,
nn.Dropout(p_drop)
)
self . fully_connected6 = nn. Sequential(
nn. Linear(n_nodes ,output_dim) ,
nn.Sigmoid()
)
defforward( self , x):
x = self . fully_connected1(x)
x = self . fully_connected2(x)
x = self . fully_connected3(x)
x = self . fully_connected4(x)
x = self . fully_connected5(x)
x = self . fully_connected6(x)
return x
133M INTERNAL AIR REPORT
M Internal AIR report
Christian Marius Lillelund
Research Assistant, MSc
Dept. of Electrical and Computer Engineering Aarhus University
Email: cl@ece.au.dk
‚Äù ... alle datas√¶t er indsamlet gennem virksomheden DigiRehab, der laver fysioterapeutisk genoptr√¶ning
af borgere i Aalborg kommune, som har f√•et visiteret et hj√¶lpemiddel (n√•r en borger f√•r visiteret et
hj√¶lpemiddel er kommunen p√•lagt at tilbyde tr√¶ning jf. Serviceloven). DigiRehab har ogs√• fungeret
som dom√¶neeksperter i udforsknings- og tr√¶ningsprocessen af data. De indsamlede data inkluderer
f√∏lgende (se Ô¨Ågur 3.1):‚Äù
‚Ä¢En liste af udl√•n af personlige hj√¶lpemidler og deres ISO/HMI-numre af KMD.
‚Ä¢EnlistemedscreeningsdataafDigiRehab, somfort√¶ller,omdenenkeltesborgersbehovforhj√¶lpog
fysiske f√¶rdigheder. Disse er indsamlet ved Ô¨Åre ugers intervaller, hvis den enkelte borger harv √¶ret
i et rehabiliteringsforl√∏b. I screeningsdata optr√¶der ligeledes en liste med √∏velser, som borgerne
skal lave.
‚Ä¢En liste med tr√¶ningsdata, som fort√¶ller, hvorn√•r enkelte borgere laver deres tr√¶ning, om de laver
den og hvordan deres tr√¶ning er g√•et m√•lt ved et subjektivt kriterie - SOSU-assistentens vurdering
af intensitet/meningsfuldhed.
‚Ä¢En liste med faldobservationer registreret i Aalborg Kommune i systemet CURA. Disse fort√¶ller,
hvem der faldet, hvorn√•r faldet er sket og andre omst√¶ndigheder, der har v√¶ret ved faldet.
‚ÄùDet er blevet besluttet at tage udgangspunkt i data fra DigiRehab i alle cases og til hver case udvikle
en model, der givet en f√∏rstegangsscreening (baseline) kan forudsige, om en borger vil gennemf√∏re sit
tr√¶ningsforl√∏b eller ej, om en borger vil opn√• compliance i sit tr√¶ningsforl√∏b eller ej, eller om en borger
vil opleve et fald i sit forl√∏b eller ej. Alle cases vil i f√∏rste omgang udelukkende v√¶re baseret p√• generalia
om borgeren (alder, k√∏n) og oplysninger om borgerens hj√¶lpemidler p√• det tidspunkt, hvor f√∏rstegangss-
creeningen blev foretaget. Det er hensigten, at der i n√¶ste iteration af ML-delen skaludvikles yderligere
en model, som givet en screening kan estimere en borgers faldrisiko i de n√¶ste tre m√•neder.‚Äù
Her en liste over features, vi bruger for cases 1-3:
‚Ä¢Gender. En bin√¶r v√¶rdi, som indikerer borgerens k√∏n.
‚Ä¢BirthYear. De sidste to cifre i borgerens f√∏dselsdato.
‚Ä¢Cluster. En v√¶rdi mellem 0 og 32, som indikerer et tilh√∏rsforhold en borgers hj√¶lpemidler har med
andre borgere. Hver v√¶rdi er i dette henseende en gruppe (cluster), og borgere i samme gruppe
har udl√•nsm√∏nstre af hj√¶lpemidler, som minder om hinanden.
134M INTERNAL AIR REPORT
‚Ä¢NumberAts. Antal hj√¶lpemidler, som borgeren har f√•et visiteret.
‚Ä¢LoanPeriod. Det gennemsnitlige antal dage, som borgeren har har l√•nt hj√¶lpemidler af kommunen.
‚Ä¢1Ats til 50Ats (50 kolonner). Lister med unikke ISO/HMI-numre p√• de hj√¶lpemidler, som borgeren
har f√•et visiteret sorteret efter visitationsdato. Medtaget er alle hj√¶lpemidler (maksimalt 50).
135